[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "",
    "text": "1 Introduction\nOne value of science derives from its production of replicable, and thus reliable, results. When we repeat a study using the original methods we should be able to expect a similar result. However, perfect replicability is not a reasonable goal. Effect sizes will vary, and even reverse in sign, by chance alone (Gelman and Weakliem 2009). Observed patterns can differ for other reasons as well. It could be that we do not sufficiently understand the conditions that led to the original result so when we seek to replicate it, the conditions differ due to some ‘hidden moderator’. This hidden moderator hypothesis is described by meta-analysts in ecology and evolutionary biology as ‘true biological heterogeneity’ (Senior et al. 2016). This idea of true heterogeneity is popular in ecology and evolutionary biology, and there are good reasons to expect it in the complex systems in which we work (Shavit and Ellison 2017). However, despite similar expectations in psychology, recent evidence in that discipline contradicts the hypothesis that moderators are common obstacles to replicability, as variability in results in a large ‘many labs’ collaboration was mostly unrelated to commonly hypothesized moderators such as the conditions under which the studies were administered (Klein et al. 2018). Another possible explanation for variation in effect sizes is that researchers often present biased samples of results, thus reducing the likelihood that later studies will produce similar effect sizes (Open Science Collaboration 2015; Parker et al. 2016; Forstmeier, Wagenmakers, and Parker 2017; Fraser et al. 2018; Parker and Yang 2023). It also may be that although researchers did successfully replicate the conditions, the experiment, and measured variables, analytical decisions differed sufficiently among studies to create divergent results (Simonsohn, Simmons, and Nelson 2015; Silberzahn et al. 2018).\nAnalytical decisions vary among studies because researchers have many options. Researchers need to decide how to exclude possibly anomalous or unreliable data, how to construct variables, which variables to include in their models, and which statistical methods to use. Depending on the dataset, this short list of choices could encompass thousands or millions of possible alternative specifications (Simonsohn, Simmons, and Nelson 2015). However, researchers making these decisions presumably do so with the goal of doing the best possible analysis, or at least the best analysis within their current skill set. Thus it seems likely that some specification options are more probable than others, possibly because they have previously been shown (or claimed) to be better, or because they are more well known. Of course, some of these different analyses (maybe many of them) may be equally valid alternatives. Regardless, on probably any topic in ecology and evolutionary biology, we can encounter differences in choices of data analysis. The extent of these differences in analyses and the degree to which these differences influence the outcomes of analyses and therefore studies’ conclusions are important empirical questions. These questions are especially important given that many papers draw conclusions after applying a single method, or even a single statistical model, to analyze a dataset.\nThe possibility that different analytical choices could lead to different outcomes has long been recognized (Gelman and Loken 2013), and various efforts to address this possibility have been pursued in the literature. For instance, one common method in ecology and evolutionary biology involves creating a set of candidate models, each consisting of a different (though often similar) set of predictor variables, and then, for the predictor variable of interest, averaging the slope across all models (i.e. model averaging) (Burnham and Anderson 2002; Grueber et al. 2011). This method reduces the chance that a conclusion is contingent upon a single model specification, though use and interpretation of this method is not without challenges (Grueber et al. 2011). Further, the models compared to each other typically differ only in the inclusion or exclusion of certain predictor variables and not in other important ways, such as methods of parameter estimation. More explicit examination of outcomes of differences in model structure, model type, data exclusion, or other analytical choices can be implemented through sensitivity analyses (e.g., Noble et al. 2017). Sensitivity analyses, however, are typically rather narrow in scope, and are designed to assess the sensitivity of analytical outcomes to a particular analytical choice rather than to a large universe of choices. Recently, however, analysts in the social sciences have proposed extremely thorough sensitivity analysis, including ‘multiverse analysis’ (Steegen et al. 2016) and the ‘specification curve’ (Simonsohn, Simmons, and Nelson 2015), as a means of increasing the reliability of results. With these methods, researchers identify relevant decision points encountered during analysis and conduct the analysis many times to incorporate many plausible decisions made at each of these points. The study’s conclusions are then based on a broad set of the possible analyses and so allow the analyst to distinguish between robust conclusions and those that are highly contingent on particular model specifications. These are useful outcomes, but specifying a universe of possible modelling decisions is not a trivial undertaking. Further, the analyst’s knowledge and biases will influence decisions about the boundaries of that universe, and so there will always be room for disagreement among analysts about what to include. Including more specifications is not necessarily better. Some analytical decisions are better justified than others, and including biologically implausible specifications may undermine this process. Regardless, these powerful methods have yet to be adopted, and even more limited forms of sensitivity analyses are not particularly widespread. Most studies publish a small set of analyses and so the existing literature does not provide much insight into the degree to which published results are contingent on analytical decisions.\nDespite the potential major impacts of analytical decisions on variance in results, the outcomes of different individuals’ data analysis choices have received limited empirical attention. The only formal exploration of this that we were aware of when we submitted our Stage 1 manuscript were (1) an analysis in social science that asked whether male professional football (soccer) players with darker skin tone were more likely to be issued red cards (ejection from the game for rule violation) than players with lighter skin tone (Silberzahn et al. 2018) and (2) an analysis in neuroimaging which evaluated nine separate hypotheses involving the neurological responses detected with fMRI in 108 participants divided between two treatments in a decision making task (Botvinik-Nezer et al. 2020). Several others have been published since (e.g., Huntington-Klein et al. 2021; Schweinsberg et al. 2021; Breznau et al. 2022; Coretta et al. 2023). In the red card study, twenty-nine teams designed and implemented analyses of a dataset provided by the study coordinators (Silberzahn et al. 2018). Analyses were peer reviewed (results blind) by at least two other participating analysts; a level of scrutiny consistent with standard pre-publication peer review. Among the final 29 analyses, odds-ratios varied from 0.89 to 2.93, meaning point estimates varied from having players with lighter skin tones receive more red cards (odds ratio &lt; 1) to a strong effect of players with darker skin tones receiving more red cards (odds ratio &gt; 1). Twenty of the 29 teams found a statistically-significant effect in the predicted direction of players with darker skin tones being issued more red cards. This degree of variation in peer-reviewed analyses from identical data is striking, but the generality of this finding has only just begun to be formally investigated.\nIn the neuroimaging study, 70 teams evaluated each of the nine different hypotheses with the available fMRI data (Botvinik-Nezer et al. 2020). These 70 teams followed a divergent set of workflows that produced a wide range of results. The rate of reporting of statistically significant support for the nine hypotheses ranged from 21% to 84%, and for each hypothesis on average, 20% of research teams observed effects that differed substantially from the majority of other teams. Some of the variability in results among studies could be explained by analytical decisions such as choice of software package, smoothing function, and parametric versus non-parametric corrections for multiple comparisons. However, substantial variability among analyses remained unexplained, and presumably emerged from the many different decisions each analyst made in their long workflows. Such variability in results among analyses from this dataset and from the very different red-card dataset suggests that sensitivity of analytical outcome to analytical choices may characterize many distinct fields, as several more recent many-analyst studies also suggest (Huntington-Klein et al. 2021; Schweinsberg et al. 2021; Breznau et al. 2022).\nTo further develop the empirical understanding of the effects of analytical decisions on study outcomes, we chose to estimate the extent to which researchers’ data analysis choices drive differences in effect sizes, model predictions, and qualitative conclusions in ecology and evolutionary biology. This is an important extension of the meta-research agenda of evaluating factors influencing replicability in ecology, evolutionary biology, and beyond (Fidler et al. 2017). To examine the effects of analytical decisions, we used two different datasets and recruited researchers to analyze one or the other of these datasets to answer a question we defined. The first question was “To what extent is the growth of nestling blue tits (Cyanistes caeruleus) influenced by competition with siblings?” To answer this question, we provided a dataset that includes brood size manipulations from 332 broods conducted over three years at Wytham Wood, UK. The second question was “How does grass cover influence Eucalyptus spp. seedling recruitment?” For this question, analysts used a dataset that includes, among other variables, number of seedlings in different size classes, percentage cover of different life forms, tree canopy cover, and distance from canopy edge from 351 quadrats spread among 18 sites in Victoria, Australia.\nWe explored the impacts of data analysts’ choices with descriptive statistics and with a series of tests to attempt to explain the variation among effect sizes and predicted values of the dependent variable produced by the different analysis teams for both datasets separately. To describe the variability, we present forest plots of the standardized effect sizes and predicted values produced by each of the analysis teams, estimate heterogeneity (both absolute, \\(\\tau^2\\), and proportional, \\(I^2\\)) in effect size and predicted values among the results produced by these different teams, and calculate a similarity index that quantifies variability among the predictor variables selected for the different statistical models constructed by the different analysis teams. These descriptive statistics provide the first estimates of the extent to which explanatory statistical models and their outcomes in ecology and evolutionary biology vary based on the decisions of different data analysts. We then quantified the degree to which the variability in effect size and predicted values could be explained by (1) variation in the quality of analyses as rated by peer reviewers and (2) the similarity of the choices of predictor variables between individual analyses.\nThis project involved a series of steps (1-6) that began with identifying datasets for analyses and continued through recruiting independent groups of scientists to analyze the data, allowing the scientists to analyze the data as they saw fit, generating peer review ratings of the analyses (based on methods, not results), evaluating the variation in effects among the different analyses, and producing the final manuscript.\nWhen a large pool of ecologists and evolutionary biologists analyzed the same two datasets to answer the corresponding two research questions, they produced substantially heterogeneous sets of answers. Although the variability in analytical outcomes was high for both datasets, the patterns of this variability differed distinctly between them. For the blue tit dataset, there was nearly continuous variability across a wide range of \\(Z_r\\) values. In contrast, for the Eucalyptus dataset, there was less variability across most of the range, but more striking outliers at the tails. Among out-of-sample predictions, there was again almost continuous variation across a wide range (2 SD) among blue tit estimates. For Eucalyptus, out-of-sample predictions were also notably variable, with about half the predicted stem count values at &lt;2 but the other half being much larger, and ranging to nearly 40 stems per 15 m x 15 m plot. We investigated several hypotheses for drivers of this variability within datasets, but found little support for any of these. Most notably, even when we excluded analyses that had received one or more poor peer reviews, the heterogeneity in results largely persisted. Regardless of what drives the variability, the existence of such dramatically heterogeneous results when ecologists and evolutionary biologists seek to answer the same questions with the same data should trigger conversations about how ecologists and evolutionary biologists analyze data and interpret the results of their own analyses and those of others in the literature (e.g., Silberzahn et al. 2018; Simonsohn, Simmons, and Nelson 2020; Auspurg and Brüderl 2021; Breznau et al. 2022).\nOur observation of substantial heterogeneity due to analytical decisions is consistent with a growing body of work, much of it from the quantitative social sciences (e.g., Silberzahn et al. 2018; Botvinik-Nezer et al. 2020; Huntington-Klein et al. 2021; Schweinsberg et al. 2021; Breznau et al. 2022; Coretta et al. 2023). In all of these studies, when volunteers from the discipline analyzed the same data, they produced a worryingly diverse set of answers to a pre-set question. This diversity always included a wide range of effect sizes, and in most cases, even involved effects in opposite directions. Thus, our result should not be viewed as an anomalous outcome from two particular datasets, but instead as evidence from additional disciplines regarding the heterogeneity that can emerge from analyses of complex datasets to answer questions in probabilistic science. Not only is our major observation consistent with other studies, it is, itself, robust because it derived primarily from simple forest plots that we produced based on a small set of decisions that were mostly registered before data gathering and which conform to widely accepted meta-analytic practices.\nUnlike the strong pattern we observed in the forest plots, our other analyses, both registered and post hoc, produced either inconsistent patterns, weak patterns, or the absence of patterns. Our registered analyses found that deviations from the meta-analytic mean by individual effect sizes (\\(Z_r\\)) or the predicted values of the dependent variable (\\(y\\)) were poorly explained by our hypothesized predictors: peer rating of each analysis team’s method section, a measurement of the distinctiveness of the set of predictor variables included in each analysis, or whether the model included random effects. However, in our post hoc analyses, we found that dropping analyses identified as unpublishable or in need of major revision by at least one reviewer modestly reduced the observed heterogeneity among the \\(Z_r\\) outcomes, but only for Eucalyptus analyses, apparently because this led to the dropping of the major outlier. This limited role for peer review in explaining the variability in our results should be interpreted cautiously because the inter-rater reliability among peer reviewers was extremely low, and at least some analyses that appeared flawed to us were not marked as flawed by reviewers. However, the hypothesis that poor quality analyses drove the heterogeneity we observed was also contradicted by our observation that analysts’ self-declared statistical expertise appeared unrelated to heterogeneity. When we retained only analyses from teams including at least one member with high self-declared levels of expertise, heterogeneity among effect sizes remained high. Thus, our results suggest lack of statistical expertise is not the primary factor responsible for the heterogeneity we observed, although further work is merited before rejecting a role for statistical expertise. Not surprisingly, simply dropping outlier values of \\(Z_r\\) for Eucalyptus analyses, which had more extreme outliers, led to less observable heterogeneity in the forest plots, and also reductions in our quantitative measures of heterogeneity. We did not observe a similar effect in the blue tit dataset because that dataset had outliers that were much less extreme and instead had more variability across the core of the distribution.\nOur major observations raise two broad questions; why was the variability among results so high, and why did the pattern of variability differ between our two datasets. One important and plausible answer to the first question is that much of the heterogeneity derives from the lack of a precise relationship between the two biological research questions we posed and the data we provided. This lack of a precise relationship between data and question creates many opportunities for different model specifications, and so may inevitably lead to varied analytical outcomes (Auspurg and Brüderl 2021). However, we believe that the research questions we posed are consistent with the kinds of research question that ecologists and evolutionary biologists typically work from. When designing the two biological research questions, we deliberately sought to represent the level of specificity we typically see in these disciplines. This level of specificity is evident when we look at the research questions posed by some recent meta-analyses in these fields:\nThere is not a single precise answer to any of these questions, nor to the questions we posed to analysts in our study. And this lack of single clear answers will obviously continue to cause uncertainty since ecologists and evolutionary biologists conceive of the different answers from the different statistical models as all being answers to the same general question. A possible response would be a call to avoid these general questions in favor of much more precise alternatives (Auspurg and Brüderl 2021). However, the research community rewards researchers who pose broad questions (Simons, Shoda, and Lindsay 2017), and so researchers are unlikely to narrow their scope without a change in incentives. Further, we suspect that even if individual studies specified narrow research questions, other scientists would group these more narrow questions into broader categories, for instance in meta-analyses, because it is these broader and more general questions that often interest the research community.\nAlthough variability in statistical outcomes among analysts may be inevitable, our results raise questions about why this variability differed between our two datasets. We are particularly interested in the differences in the distribution of \\(Z_r\\) since the distributions of out-of-sample predictions were on different scales for the two datasets, thus limiting the value of comparisons. The forest plots of \\(Z_r\\) from our two datasets showed distinct patterns, and these differences are consistent with several alternative hypotheses. The results submitted by analysts of the Eucalyptus dataset showed a small average (close to zero) with most estimates also close to zero (± 0.2), though about a third far enough above or below zero to cross the traditional threshold of statistical significance. There were a small number of striking outliers that were very far from zero. In contrast, the results submitted by analysts of the blue tit dataset showed an average much further from zero (- 0.35) and a much greater spread in the core distribution of estimates across the range of \\(Z_r\\) values (± 0.5 from the mean), with few modest outliers. So, why was there more spread in effect sizes (across the estimates that are not outliers) in the blue tit analyses relative to the Eucalyptus analyses?\nOne possible explanation for the lower heterogeneity among most Eucalyptus \\(Z_r\\) effects is that weak relationships may limit the opportunities for heterogeneity in analytical outcome. Some evidence for this idea comes from two sets of “many labs” studies in psychology (Klein et al. 2014, 2018). In these studies, many independent lab groups each replicated a large set of studies, including, for each study, the experiment, data collection, and statistical analyses. These studies showed that, when the meta-analytic mean across the replications from different labs was small, there was much less heterogeneity among the outcomes than when the mean effect sizes were large (Klein et al. 2014, 2018). Of course, a weak average effect size would not prevent divergent effects in all circumstances. As we saw with the Eucalyptus analyses, taking a radically smaller subset of the data can lead to dramatically divergent effect sizes even when the mean with the full dataset is close to zero.\nOur observation that dramatic sub-setting in the Eucalyptus dataset was associated with correspondingly dramatic divergence in effect sizes leads us towards another hypothesis to explain the differences in heterogeneity between the Eucalyptus and blue tit analysis sets. It may be that when analysts often divide a dataset into subsets, the result will be greater heterogeneity in analytical outcome for that dataset. Although we saw sub-setting associated with dramatic outliers in the Eucalyptus dataset, nearly all other analyses of Eucalyptus data used very close to the same set of 351 samples, and as we saw, these effects did not vary substantially. However, analysts often analyzed only a subset of the blue tit data, and as we observed, sample sizes were much more variable among blue tit effects, and the effects themselves were also much more variable. Important to note here is that subsets of data may differ from each other for biological reasons, but they may also differ due to sampling error. Sampling error is a function of sample size, and sub-samples are, by definition, smaller samples, and so more subject to variability in effects due to sampling error (Jennions et al. 2013).\nOther features of datasets are also plausible candidates for driving heterogeneity in analytical outcomes, including features of covariates. In particular, relationships between covariates and the response variable as well as relationships between covariates and the primary independent variable (collinearity) can strongly influence the modeled relationship between the independent variable of interest and the dependent variable (Morrissey and Ruxton 2018; Dormann et al. 2013). Therefore, inclusion or exclusion of these covariates can drive heterogeneity in effect sizes (\\(Z_r\\)). Also, as we saw with the two most extreme \\(Z_r\\) values from the blue tit analyses, in multivariate models with collinear predictors, extreme effects can emerge when estimating partial correlation coefficients due to high collinearity, and conclusions can differ dramatically depending on which relationship receives the researcher’s attention. Therefore, differences between datasets in the presence of strong and/or collinear covariates could influence the differences in heterogeneity in results among those datasets.\nAlthough it is too early in the many-analyst research program to conclude which analytical decisions or which features of datasets are the most important drivers of heterogeneity in analytical outcomes, we must still grapple with the possibility that analytical outcomes may vary substantially based on the choices we make as analysts. If we assume that, at least sometimes, different analysts will produce dramatically different statistical outcomes, what should we do as ecologists and evolutionary biologists? We review some ideas below.\nThe easiest path forward after learning about this analytical heterogeneity would be simply to continue with “business as usual”, where researchers report results from a small number of statistical models. A case could be made for this path based on our results. For instance, among the blue tit analyses, the precise values of the estimated \\(Z_r\\) effects varied substantially, but the average effect was convincingly different from zero, and a majority of individual effects (84%) were in the same direction. Arguably, many ecologists and evolutionary biologists appear primarily interested in the direction of a given effect and the corresponding p-value (Fidler et al. 2006), and so the variability we observed when analyzing the blue tit dataset may not worry these researchers. Similarly, most effects from the Eucalyptus analyses were relatively close to zero, and about two-thirds of these effects did not cross the traditional threshold of statistical significance. Therefore, a large proportion of people analyzing these data would conclude that there was no effect, and this is consistent with what we might conclude from the meta-analysis.\nHowever, we find the counter arguments to “business as usual” to be compelling. For blue tits, there were a substantial minority of calculated effects that would be interpreted by many biologists as indicating the absence of an effect (28%), and there were three traditionally ‘significant’ effects in the opposite direction to the average. The qualitative conclusions of analysts also reflected substantial variability, with fully half of teams drawing a conclusion distinct from the one we draw from the distribution as a whole. These teams with different conclusion were either uncertain about the negative relationship between competition and nestling growth, or they concluded that effects were mixed or absent. For the Eucalyptus analyses, this issue is more concerning. Around two-thirds of effects had confidence intervals overlapping zero, and of the third of analyses with confidence intervals excluding zero, almost half were positive, and the rest were negative. Accordingly, the qualitative conclusions of the Eucalyptus teams were spread across the full range of possibilities. But even these problems are optimistic.\nA potentially larger argument against “business as usual” is that it provides the raw material for biasing the literature. When different model specifications readily lead to different results, analysts may be tempted to report the result that appears most interesting, or that is most consistent with expectation (Gelman and Loken 2013; Forstmeier, Wagenmakers, and Parker 2017). There is growing evidence that researchers in ecology and evolutionary biology often report a biased subset of the results they produce (Deressa et al. 2023; Kimmel, Avolio, and Ferraro 2023), and that this bias exaggerates the average size of effects in the published literature between 30 and 150% (Yang et al. 2023; Parker and Yang 2023). The bias then accumulates in meta-analyses, apparently more than doubling the rate of conclusions of “statistical significance” in published meta-analyses above what would have been found in the absence of bias (Yang et al. 2023). Thus, “business as usual” does not just create noisy results, it helps create systematically misleading results.\nOverall, our results suggest to us that, where there is a diverse set of plausible analysis options, no single analysis should be considered a complete or reliable answer to a research question. We contend that ecologists and evolutionary biologists typically do multiple analyses (as many of our analyst teams did) however, soe of these analyses dont make it into the published manuscript. Further, because of the evidence that ecologists and evolutionary biologists often present a biased subset of the analyses they conduct (Deressa et al. 2023; Yang et al. 2023; Kimmel, Avolio, and Ferraro 2023), we do not expect that even a collection of different effect sizes from different studies will accurately represent the true distribution of effects (Yang et al. 2023). Therefore, we believe that an increased level of skepticism of the outcomes of single analyses, or even single meta-analyses, is warranted going forward. We recognize that some researchers have long maintained a healthy level of skepticism of individual studies as part of sound and practical scientific practice, and it is possible that those researchers will be neither surprised nor concerned by our results. However, we doubt that many researchers are sufficiently aware of the potential problems of analytical flexibility to be appropriately skeptical.\nIf we are skeptical of single analyses, the path forward may be multiple analyses per dataset. One possibility is the traditional robustness or sensitivity check (e.g., Pei et al. 2020; Briga and Verhulst 2021), in which the researcher presents several alternative versions of an analysis to demonstrate that the result is ‘robust’ (Lu and White 2014). Unfortunately, robustness checks are at risk of the same potential biases of reporting found in other studies (Silberzahn et al. 2018), especially given the relatively few models typically presented. However, these risks could be minimized by running more models and doing so with pre-registration or registered report. Another option is model averaging. Averages across models often perform well (e.g., Taylor and Taylor 2023), and in some forms this may be a relatively simple solution. As most often practiced in ecology and evolutionary biology, model averaging involves first identifying a small suite of candidate models (see Burnham and Anderson 2002), then using Akaike weights, based on Akaike’s Information Criterion (AIC), to calculate weighted averages for parameter estimates from those models. Again, the small number of models limits the exploration of specification space, but we can examine a larger number of models. However, there are more concerning limitations. The largest of these limitations is that averaging regression coefficients is problematic when models differ in interaction terms or collinear variables (Cade 2015). Additionally, weighting by AIC may often be inconsistent with our modelling goals. AIC balances the trade-off between model complexity and predictive ability, but penalizing models for complexity may not be suited for testing hypotheses about causation. So, AIC may often not offer the weight we want to use for an average, and we may also not wish to just generate an average. Instead, if we hope to understand an extensive universe of possible modelling outcomes, we could conduct a multiverse analysis, possibly with a specification curve (Simonsohn, Simmons, and Nelson 2015, 2020). This could mean running hundreds or thousands of models (or more!) to examine the distribution of possible effects, and to see how different specification choices map onto these effects. However, there is a trade-off between efficiently exploring large areas of specification space and limiting the analyses to biologically plausible specifications. Instead of simply identifying modelling decisions and creating all possible combinations for the multiverse, a researcher could attempt to prevent implausible combinations, though the more variables in the dataset, the more difficult this becomes. To make this easier, one could recruit many analysts to each designate one or a few plausible specifications, as with our ‘many analyst’ study (Silberzahn et al. 2018). An alternative that may be more labor intensive for the primary analyst, but which may lead to a more plausible set of models, could involve hypothesizing about causal pathways with DAGs [directed acyclic graphs; Arif and MacNeil (2023)] to constrain the model set. Devoting this effort to thoughtful multiverse specifications, possibly combined with pre-registration to hinder undisclosed data dredging, seems worthy of consideration.\nAlthough we have reviewed a variety of potential responses to the existence of variability in analytical outcomes, we certainly do not wish to imply that this is a comprehensive set of possible responses. Nor do we wish to imply that the opinions we have expressed about these options are correct. Determining how the disciplines of ecology and evolutionary biology should respond to knowledge of the variability in analytical outcome will benefit from the contribution and discussion of ideas from across these disciplines. We look forward to learning from these discussions and to seeing how these disciplines ultimately respond.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-1-select-datasets",
    "href": "index.html#step-1-select-datasets",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.1 Step 1: Select Datasets",
    "text": "2.1 Step 1: Select Datasets\nWe used two previously unpublished datasets, one from evolutionary ecology and the other from ecology and conservation.\nEvolutionary ecology\nOur evolutionary ecology dataset is relevant to a sub-discipline of life-history research which focuses on identifying costs and trade-offs associated with different phenotypic conditions. These data were derived from a brood-size manipulation experiment imposed on wild birds nesting in boxes provided by researchers in an intensively studied population. Understanding how the growth of nestlings is influenced by the numbers of siblings in the nest can give researchers insights into factors such as the evolution of clutch size, determination of provisioning rates by parents, and optimal levels of sibling competition (Vander Werf 1992; DeKogel 1997; Royle et al. 1999; Verhulst, Holveck, and Riebel 2006; Nicolaus et al. 2009). Data analysts were provided this dataset and instructed to answer the following question: “To what extent is the growth of nestling blue tits (Cyanistes caeruleus) influenced by competition with siblings?”\nResearchers conducted brood size manipulations and population monitoring of blue tits at Wytham Wood, a 380 ha woodland in Oxfordshire, U.K (1º 20’W, 51º 47’N). Researchers regularly checked approximately 1100 artificial nest boxes at the site and monitored the 330 to 450 blue tit pairs occupying those boxes in 2001-2003 during the experiment. Nearly all birds made only one breeding attempt during the April to June study period in a given year. At each blue tit nest, researchers recorded the date the first egg appeared, clutch size, and hatching date. For all chicks alive at age 14 days, researchers measured mass and tarsus length and fitted a uniquely numbered, British Trust for Ornithology (BTO) aluminium leg ring. Researchers attempted to capture all adults at their nests between day 6 and day 14 of the chick-rearing period. For these captured adults, researchers measured mass, tarsus length, and wing length and fitted a uniquely numbered BTO leg ring. During the 2001-2003 breeding seasons, researchers manipulated brood sizes using cross fostering. They matched broods for hatching date and brood size and moved chicks between these paired nests one or two days after hatching. They sought to either enlarge or reduce all manipulated broods by approximately one fourth. To control for effects of being moved, each reduced brood had a portion of its brood replaced by chicks from the paired increased brood, and vice versa. Net manipulations varied from plus or minus four chicks in broods of 12 to 16 to plus or minus one chick in broods of 4 or 5. Researchers left approximately one third of all broods unmanipulated. These unmanipulated broods were not selected systematically to match manipulated broods in clutch size or laying date. We have mass and tarsus length data from 3720 individual chicks divided among 167 experimentally enlarged broods, 165 experimentally reduced broods, and 120 unmanipulated broods. The full list of variables included in the dataset is publicly available (https://osf.io/hdv8m), along with the data (https://osf.io/qjzby).\n\n\n\n\n\n\nAdditional explanation: Shortly after beginning to recruit analysts, several analysts noted a small set of related errors in the blue tit dataset. We corrected the errors, replaced the dataset on our OSF site, and emailed the analysts on 19 April 2020 to instruct them to use the revised data. The email to analysts is available here (https://osf.io/4h53z). The errors are explained in that email.\n\n\n\nEcology and conservation\nOur ecology and conservation dataset is relevant to a sub-discipline of conservation research which focuses on investigating how best to revegetate private land in agricultural landscapes. These data were collected on private land under the Bush Returns program, an incentive system where participants entered into a contract with the Goulburn Broken Catchment Management Authority and received annual payments if they executed predetermined restoration activities. This particular dataset is based on a passive regeneration initiative, where livestock grazing was removed from the property in the hopes that the Eucalyptus spp. overstorey would regenerate without active (and expensive) planting. Analyses of some related data have been published (Miles 2008; Vesk et al. 2016) but those analyses do not address the question analysts answered in our study. Data analysts were provided this dataset and instructed to answer the following question: “How does grass cover influence Eucalyptus spp. seedling recruitment?”.\nResearchers conducted three rounds of surveys at 18 sites across the Goulburn Broken catchment in northern Victoria, Australia in winter and spring 2006 and autumn 2007. In each survey period, a different set of 15 x 15 m quadrats were randomly allocated across each site within 60 m of existing tree canopies. The number of quadrats at each site depended on the size of the site, ranging from four at smaller sites to 11 at larger sites. The total number of quadrats surveyed across all sites and seasons was 351. The number of Eucalyptus spp. seedlings was recorded in each quadrat along with information on the GPS location, aspect, tree canopy cover, distance to tree canopy, and position in the landscape. Ground layer plant species composition was recorded in three 0.5 x 0.5 m sub-quadrats within each quadrat. Subjective cover estimates of each species as well as bare ground, litter, rock and moss/lichen/soil crusts were recorded. Subsequently, this was augmented with information about the precipitation and solar radiation at each GPS location. The full list of variables included in the dataset is publicly available (https://osf.io/r5gbn), along with the data (https://osf.io/qz5cu).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-2-recruitment-and-initial-survey-of-analysts",
    "href": "index.html#step-2-recruitment-and-initial-survey-of-analysts",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.2 Step 2: Recruitment and Initial Survey of Analysts",
    "text": "2.2 Step 2: Recruitment and Initial Survey of Analysts\nThe lead team (TP, HF, SN, EG, SG, PV, FF) created a publicly available document providing a general description of the project (https://osf.io/mn5aj/). The project was advertised at conferences, via Twitter, using mailing lists for ecological societies (including Ecolog, Evoldir, and lists for the Environmental Decisions Group, and Transparency in Ecology and Evolution), and via word of mouth. The target population was active ecology, conservation, or evolutionary biology researchers with a graduate degree (or currently studying for a graduate degree) in a relevant discipline. Researchers could choose to work independently or in a small team. For the sake of simplicity, we refer to these as ‘analysis teams’ though some comprised one individual. We aimed for a minimum of 12 analysis teams independently evaluating each dataset (see sample size justification below). We simultaneously recruited volunteers to peer review the analyses conducted by the other volunteers through the same channels. Our goal was to recruit a similar number of peer reviewers and analysts, and to ask each peer reviewer to review a minimum of four analyses. If we were unable to recruit at least half the number of reviewers as analysis teams, we planned to ask analysts to serve also as reviewers (after they had completed their analyses), but this was unnecessary. All analysts and reviewers were offered the opportunity to share co-authorship on this manuscript and we planned to invite them to participate in the collaborative process of producing the final manuscript. All analysts signed [digitally] a consent (ethics) document (https://osf.io/xyp68/) approved by the Whitman College Institutional Review Board prior to being allowed to participate.\n\n\n\n\n\n\nPreregistration Deviation:\nDue to the large number of recruited analysts and reviewers and the anticipated challenges of receiving and integrating feedback from so many authors, we limited analyst and reviewer participation in the production of the final manuscript to an invitation to call attention to serious problems with the manuscript draft.\n\n\n\nWe identified our minimum number of analysts per dataset by considering the number of effects needed in a meta-analysis to generate an estimate of heterogeneity (\\(\\tau^{2}\\)) with a 95% confidence interval that does not encompass zero. This minimum sample size is invariant regardless of \\(\\tau^{2}\\). This is because the same t-statistic value will be obtained by the same sample size regardless of variance (\\(\\tau^{2}\\)). We see this by first examining the formula for the standard error, SE for variance, (\\(\\tau^{2}\\)) or SE(\\(\\tau^{2}\\)) assuming normality in an underlying distribution of effect sizes (Knight 2000):\n\\[\nSE({{τ}^2})=\\sqrt{\\frac{{t}^4}{n-1}}\n\\tag{2.1}\\]\nand then rearranging the above formula to show how the t-statistic is independent of \\(\\tau^2\\), as seen below.\n\\[\nt=\\frac{{τ}^2}{SE({{τ}^2})}=\\sqrt{\\frac{n-1}{2}}\n\\tag{2.2}\\]\nWe then find a minimum n = 12 according to this formula.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-3-primary-data-analyses",
    "href": "index.html#step-3-primary-data-analyses",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.3 Step 3: Primary Data Analyses",
    "text": "2.3 Step 3: Primary Data Analyses\nAnalysis teams registered and answered a demographic and expertise survey (https://osf.io/seqzy/). We then provided them with the dataset of their choice and requested that they answer a specific research question. For the evolutionary ecology dataset that question was “To what extent is the growth of nestling blue tits (Cyanistes caeruleus) influenced by competition with siblings?” and for the conservation ecology dataset it was “How does grass cover influence Eucalyptus spp. seedling recruitment?” Once their analysis was complete, they answered a structured survey (https://osf.io/neyc7/), providing analysis technique, explanations of their analytical choices, quantitative results, and a statement describing their conclusions. They also were asked to upload their analysis files (including the dataset as they formatted it for analysis and their analysis code [if applicable]) and a detailed journal-ready statistical methods section.\n\n\n\n\n\n\nPreregistration Deviation:\nWe originally planned to have analysts complete a single survey (https://osf.io/neyc7/), but after we evaluated the results of that survey, we realized we would need a second survey (https://osf.io/8w3v5/) to adequately collect the information we needed to evaluate heterogeneity of results (step 5). We provided a set of detailed instructions with the follow-up survey, and these instructions are publicly available and can be found within the following files (blue tit: https://osf.io/kr2g9, Eucalyptus: https://osf.io/dfvym).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-4-peer-reviews-of-analyses",
    "href": "index.html#step-4-peer-reviews-of-analyses",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.4 Step 4: Peer Reviews of Analyses",
    "text": "2.4 Step 4: Peer Reviews of Analyses\nAt minimum, each analysis was evaluated by four different reviewers, and each volunteer peer reviewer was randomly assigned methods sections from at least four analyst teams (the exact number varied). Each peer reviewer registered and answered a demographic and expertise survey identical to that asked of the analysts, except we did not ask about ‘team name’ since reviewers did not work in teams. Reviewers evaluated the methods of each of their assigned analyses one at a time in a sequence determined by the project leaders. We systematically assigned the sequence so that, if possible, each analysis was allocated to each position in the sequence for at least one reviewer. For instance, if each reviewer were assigned four analyses to review, then each analysis would be the first analysis assigned to at least one reviewer, the second analysis assigned to another reviewer, the third analysis assigned to yet another reviewer, and the fourth analysis assigned to a fourth reviewer. Balancing the order in which reviewers saw the analyses controls for order effects, e.g. a reviewer might be less critical of the first methods section they read than the last.\nThe process for a single reviewer was as follows. First, the reviewer received a description of the methods of a single analysis. This included the narrative methods section, the analysis team’s answers to our survey questions regarding their methods, including analysis code, and the dataset. The reviewer was then asked, in an online survey (https://osf.io/4t36u/), to rate that analysis on a scale of 0-100 based on this prompt: “Rate the overall appropriateness of this analysis to answer the research question (one of the two research questions inserted here) with the available data. To help you calibrate your rating, please consider the following guidelines:\n\n\nA perfect analysis with no conceivable improvements from the reviewer\n\n\nAn imperfect analysis but the needed changes are unlikely to dramatically alter outcomes\n\n\nA flawed analysis likely to produce either an unreliable estimate of the relationship or an over-precise estimate of uncertainty\n\n\nA flawed analysis likely to produce an unreliable estimate of the relationship and an over-precise estimate of uncertainty\n\n\nA dangerously misleading analysis, certain to produce both an estimate that is wrong and a substantially over-precise estimate of uncertainty that places undue confidence in the incorrect estimate.\n\n\n*Please note that these values are meant to calibrate your ratings. We welcome ratings of any number between 0 and 100.”\nAfter providing this rating, the reviewer was presented with this prompt, in multiple-choice format: “Would the analytical methods presented produce an analysis that is (a) publishable as is, (b) publishable with minor revision, (c) publishable with major revision, (d) deeply flawed and unpublishable?” The reviewer was then provided with a series of text boxes and the following prompts: “Please explain your ratings of this analysis. Please evaluate the choice of statistical analysis type. Please evaluate the process of choosing variables for and structuring the statistical model. Please evaluate the suitability of the variables included in (or excluded from) the statistical model. Please evaluate the suitability of the structure of the statistical model. Please evaluate choices to exclude or not exclude subsets of the data. Please evaluate any choices to transform data (or, if there were no transformations, but you think there should have been, please discuss that choice).” After submitting this review, a methods section from a second analysis was then made available to the reviewer. This same sequence was followed until all analyses allocated to a given reviewer were provided and reviewed. After providing the final review, the reviewer was simultaneously provided with all four (or more) methods sections the reviewer had just completed reviewing, the option to revise their original ratings, and a text box to provide an explanation. The invitation to revise the original ratings was as follows: “If, now that you have seen all the analyses you are reviewing, you wish to revise your ratings of any of these analyses, you may do so now.” The text box was prefaced with this prompt: “Please explain your choice to revise (or not to revise) your ratings.”\n\n\n\n\n\n\nAdditional explanation: Unregistered analysis.\nTo determine how consistent peer reviewers were in their ratings, we assessed inter-rater reliability among reviewers for both the categorical and quantitative ratings combining blue tit and Eucalyptus data using Krippendorff’s alpha for ordinal and continuous data respectively. This provides a value that is between -1 (total disagreement between reviewers) and 1 (total agreement between reviewers).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-5-evaluate-variation",
    "href": "index.html#step-5-evaluate-variation",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.5 Step 5: Evaluate Variation",
    "text": "2.5 Step 5: Evaluate Variation\nThe lead team conducted the analyses outlined in this section. We described the variation in model specification in several ways. We calculated summary statistics describing variation among analyses, including mean, SD, and range of number of variables per model included as fixed effects, the number of interaction terms, the number of random effects, and the mean, SD, and range of sample sizes. We also present the number of analyses in which each variable was included. We summarized the variability in standardized effect sizes and predicted values of dependent variables among the individual analyses using standard random effects meta-analytic techniques. First, we derived standardized effect sizes from each individual analysis. We did this for all linear models or generalized linear models by converting the \\(t\\) value and the degree of freedom (\\(df\\)) associated with regression coefficients (e.g. the effect of the number of siblings [predictor] on growth [response] or the effect of grass cover [predictor] on seedling recruitment [response]) to the correlation coefficient, \\(r\\), using the following:\n\\[\nr=\\sqrt{\\frac{{t}^2}{\\left({{t}^2}+df\\right) }}\n\\tag{2.3}\\]\nThis formula can only be applied if \\(t\\) and \\(df\\) values originate from linear or generalized linear models [GLMs; Shinichi Nakagawa and Cuthill (2007)]. If, instead, linear mixed-effects models (LMMs) or generalized linear mixed-effects models (GLMMs) were used by a given analysis, the exact \\(df\\) cannot be estimated. However, adjusted \\(df\\) can be estimated, for example, using the Satterthwaite approximation of \\(df\\), \\({df}_S\\), [note that SAS uses this approximation to obtain \\(df\\) for LMMs and GLMMs; Luke (2017)]. For analyses using either LMMs or GLMMs that do not produce \\({df}_S\\) we planned to obtain \\({df}_S\\) by rerunning the same (G)LMMs using the lmer() or glmer() function in the lmerTest package in R (Kuznetsova, Brockhoff, and Christensen 2017; R Core Team 2022).\n\n\n\n\n\n\nPreregistration Deviation:\nRather than re-run these analyses ourselves, we sent a follow-up survey (referenced above under “Primary data analyses”) to analysts and asked them to follow our instructions for producing this information. The instructions are publicly available and can be found within the following files (blue tit: https://osf.io/kr2g9, Eucalyptus: https://osf.io/dfvym).\n\n\n\nWe then used the \\(t\\) values and \\(df_S\\) from the models to obtain \\(r\\) as per the formula above. All \\(r\\) and accompanying \\(df\\) (or \\(df_S\\)) were converted to \\(Z_r\\) and its sampling variance; \\(1/(n – 3)\\) where \\(n = df + 1\\). Any analyses from which we could not derive a signed \\(Z_r\\), for instance one with a quadratic function in which the slope changed sign, were excluded from the analyses of Fisher’s \\(Z_r\\). We expected such analyses would be rare. In fact, most submitted analyses excluded from our meta-analysis of \\(Z_r\\) were excluded because of a lack of sufficient information provided by the analyst team rather than due to the use of effects that could not be converted to \\(Z_r\\). Regardless, as we describe below, we generated a second set of standardized effects (predicted values) that could (in principle) be derived from any explanatory model produced by these data.\nBesides \\(Z_r\\), which describes the strength of a relationship based on the amount of variation in a dependent variable explained by variation in an independent variable, we also examined differences in the shape of the relationship between the independent and dependent variables. To accomplish this, we derived a point estimate (out-of-sample predicted value) for the dependent variable of interest for each of three values of our primary independent variable. We originally described these three values as associated with the 25th percentile, median, and 75th percentile of the independent variable and any covariates.\n\n\n\n\n\n\nPreregistration Deviation: The original description of the out-of-sample specifications did not account for the facts that (a) some variables are not distributed in a way that allowed division in percentiles and that (b) variables could be either positively or negatively correlated with the dependent variable. We provide a more thorough description here: We derived three point-estimates (out-of-sample predicted values) for the dependent variable of interest; one for each of three values of our primary independent variable that we specified. We also specified values for all other variables that could have been included as independent variables in analysts’ models so that we could derive the predicted values from a fully specified version of any model produced by analysts. For all potential independent variables, we selected three values or categories. Of the three we selected, one was associated with small, one with intermediate, and one with large values of one typical dependent variable (day 14 chick weight for the blue tit data and total number of seedlings for the Eucalyptus data; analysts could select other variables as their dependent variable, but the others typically correlated with the two identified here). For continuous variables, this means we identified the 25th percentile, median, and 75th percentile and, if the slope of the linear relationship between this variable and the typical dependent variable was positive, we left the quartiles ordered as is. If, instead, the slope was negative, we reversed the order of the independent variable quartiles so that the ‘lower’ quartile value was the one associated with the lower value for the dependent variable. In the case of categorical variables, we identified categories associated with the 25th percentile, median, and 75th percentile values of the typical dependent variable after averaging the values for each category. However, for some continuous and categorical predictors, we also made selections based on the principle of internal consistency between certain related variables, and we fixed a few categorical variables as identical across all three levels where doing so would simplify the modelling process (specification tables available: blue tit: https://osf.io/86akx; Eucalyptus: https://osf.io/jh7g5).\n\n\n\nWe used the 25th and 75th percentiles rather than minimum and maximum values to reduce the chance of occupying unrealistic parameter space. We planned to derive these predicted values from the model information provided by the individual analysts. All values (predictions) were first transformed to the original scale along with their standard errors (SE); we used the delta method (Ver Hoef 2012) for the transformation of SE. We used the square of the SE associated with predicted values as the sampling variance in the meta-analyses described below, and we planned to analyze these predicted values in exactly the same ways as we analyzed \\(Z_r\\) in the following analyses.\n\n\n\n\n\n\nPreregistration Deviation: Because analysts of blue tit data chose different dependent variables on different scales, after transforming out-of-sample values to the original scales, we standardized all values as z scores (‘standard scores’) to put all dependent variables on the same scale and make them comparable. This involved taking each relevant value on the original scale (whether a predicted point estimate or a SE associated with that estimate) and subtracting the value in question from the mean value of that dependent variable derived from the full dataset and then dividing this difference by the standard deviation, SD, corresponding to the mean from the full dataset. Thus, all our out-of-sample prediction values from the blue tit data are from a distribution with the mean of 0 and SD of 1. We did not add this step for the Eucalyptus data because (a) all responses were on the same scale (counts of Eucalyptus stems) and were thus comparable and (b) these data, with many zeros and high skew, are poorly suited for z scores.\n\n\n\nWe plotted individual effect size estimates (\\(Z_r\\)) and predicted values of the dependent variable (\\(y_i\\)) and their corresponding 95% confidence / credible intervals in forest plots to allow visualization of the range and precision of effect size and predicted values. Further, we included these estimates in random effects meta-analyses (Higgins et al. 2003; Borenstein et al. 2017) using the metafor package in R (Viechtbauer 2010; R Core Team 2022):\n\\[\nZ_r \\sim 1 + \\left(1 \\vert \\text{analysisID} \\right)\n\\tag{2.4}\\]\n\\[  \ny_i \\sim 1 + \\left(1 \\vert \\text{analysisID} \\right)\n\\tag{2.5}\\]\nwhere \\(y_i\\) is the predicted value for the dependent variable at the 25th percentile, median, or 75th percentile of the independent variables. The individual \\(Z_r\\) effect sizes were weighted with the inverse of sampling variance for \\(Z_r\\). The individual predicted values for dependent variable (\\(y_i\\)) were weighted by the inverse of the associated \\(SE^2\\) (original registration omitted “inverse of the” in error). These analyses provided an average \\(Z_r\\) score or an average \\(y_i\\) with corresponding 95% confidence interval and allowed us to estimate two heterogeneity indices, \\(\\tau^2\\) and \\(I^2\\). The former, \\(\\tau^2\\), is the absolute measure of heterogeneity or the between-study variance (in our case, between-effect variance) whereas \\(I^2\\) is a relative measure of heterogeneity. We obtained the estimate of relative heterogeneity (\\(I^2\\)) by dividing the between-effect variance by the sum of between-effect and within-effect variance (sampling error variance). \\(I^2\\) is thus, in a standard meta-analysis, the proportion of variance that is due to heterogeneity as opposed to sampling error. When calculating \\(I^2\\), within-study variance is amalgamated across studies to create a “typical” within-study variance which serves as the sampling error variance (Higgins et al. 2003; Borenstein et al. 2017). Our goal here was to visualize and quantify the degree of variation among analyses in effect size estimates (Shinichi Nakagawa and Cuthill 2007). We did not test for statistical significance.\n\n\n\n\n\n\nAdditional explanation: Our use of \\(I^{2}\\) to quantify heterogeneity violates an important assumption, but this violation does not invalidate our use of \\(I^{2}\\) as a metric of how much heterogeneity can derive from analytical decisions. In standard meta-analysis, the statistic \\(I^{2}\\) quantifies the proportion of variance that is greater than we would expect if differences among estimates were due to sampling error alone (Rosenberg 2013). However, it is clear that this interpretation does not apply to our value of \\(I^{2}\\) because \\(I^{2}\\) assumes that each estimate is based on an independent sample (although these analyses can account for non-independence via hierarchical modelling), whereas all our effects were derived from largely or entirely overlapping subsets of the same dataset. Despite this, we believe that \\(I^{2}\\) remains a useful statistic for our purposes. This is because, in calculating \\(I^{2}\\), we are still setting a benchmark of expected variation due to sampling error based on the variance associated with each separate effect size estimate, and we are assessing how much (if it all) the variability among our effect sizes exceeds what would be expected had our effect sizes been based on independent data. In other words, our estimates can tell us how much proportional heterogeneity is possible from analytical decisions alone when sample sizes (and therefore meta-analytic within-estimate variance) are similar to the ones in our analyses. Among other implications, our violation of the independent sample assumption means that we (dramatically) over-estimate the variance expected due to sampling error, and because \\(I^{2}\\) is a proportional estimate, we thus underestimate the actual proportion of variance due to differences among analyses other than sampling error. However, correcting this underestimation would create a trivial value since we designed the study so that much of the variance would derive from analytic decisions as opposed to differences in sampled data. Instead, retaining the \\(I^{2}\\) value as typically calculated provides a useful comparison to \\(I^{2}\\) values from typical meta-analyses.\nInterpretation of \\(\\tau^2\\) also differs somewhat from traditional meta-analysis, and we discuss this further in the Results.\n\n\n\nFinally, we assessed the extent to which deviations from the meta-analytic mean by individual effect sizes (\\(Z_r\\)) or the predicted values of the dependent variable (\\(y_i\\)) were explained by the peer rating of each analysis team’s method section, by a measurement of the distinctiveness of the set of predictor variables included in each analysis, and by the choice of whether or not to include random effects in the model. The deviation score, which served as the dependent variable in these analyses, is the absolute value of the difference between the meta-analytic mean \\(Z_r\\) (or \\(y_i\\)) and the individual \\(Z_r\\) (or \\(y_i\\)) estimate for each analysis. We used the Box-Cox transformation on the absolute values of deviation scores to achieve an approximately normal distribution (c.f. Fanelli and Ioannidis 2013; Fanelli, Costas, and Ioannidis 2017 : supplement). We described variation in this dependent variable with both a series of univariate analyses and a multivariate analysis. All these analyses were general linear (mixed) models. These analyses were secondary to our estimation of variation in effect sizes described above. We wished to quantify relationships among variables, but we had no a priori expectation of effect size and made no dichotomous decisions about statistical significance.\nWhen examining the extent to which reviewer ratings (on a scale from 0 to 100) explained deviation from the average effect (or predicted value), each analysis had been rated by multiple peer reviewers, so for each reviewer score to be included, we include each deviation score in the analysis multiple times. To account for the non-independence of multiple ratings of the same analysis, we planned to include analysis identity as a random effect in our general linear mixed model in the lme4 package in R (Bates et al. 2015; R Core Team 2022). To account for potential differences among reviewers in their scoring of analyses, we also planned to include reviewer identity as a random effect:\n\\[\n\\begin{align}\n\\text{DeviationScore}_j = \\text{BoxCox}(abs(\\text{DeviationFromMean}_{j})) \\\\\n{\\text{DeviationScore}}_{ij} \\sim Rating_{ij} + \\text{ReviewerID}_{i} + {\\text{AnalysisID}}_{j} \\\\\n{\\text{ReviewerID}}_i \\sim \\mathcal{N}(0,\\sigma^2) \\\\\n{\\text{AnalysisID}}_j \\sim \\mathcal{N}(0,\\sigma^2)\n\\end{align}\n\\tag{2.6}\\]\nWhere \\(DeviationFromMean_{j}\\) is the deviation from the meta-analytic mean for the jth analysis, \\(ReviewerID_{i}\\) is the random intercept assigned to each i reviewer, and \\(AnalysisID_{j}\\) is the random intercept assigned to each j analysis, both of which are assumed to be normally distributed with a mean of 0 and a variance of \\(\\sigma^2\\). Absolute deviation scores were Box-Cox transformed using the step_box_cox() function from the timetk package in R (Dancho and Vaughan 2023; R Core Team 2022).\nWe conducted a similar analysis with the four categories of reviewer ratings ((1) deeply flawed and unpublishable, (2) publishable with major revision, (3) publishable with minor revision, (4) publishable as is) set as ordinal predictors numbered as shown here. As with the analyses above, we planned for these analyses to also include random effects of analysis identity and reviewer identity. Both of these analyses (1: 1-100 ratings as the fixed effect, 2: categorical ratings as the fixed effects) were planned to be conducted eight times for each dataset. Each of the four responses (\\(Z_r\\), \\(y_{25th}\\), \\(y_{50th}\\), \\(y_{75th}\\)) were to be compared once to the initial ratings provided by the peer reviewers, and again based on the revised ratings provided by the peer reviewers.\n\n\n\n\n\n\nPreregistration Deviation:\n\nWe planned to include random effects of both analysis identity and reviewer identity in these models comparing reviewer ratings with deviation scores. However, after we received the analyses, we discovered that a subset of analyst teams had either conducted multiple analyses and/or identified multiple effects per analysis as answering the target question. We therefore faced an even more complex potential set of random effects. We decided that including team ID, analysis ID, and effect ID along with reviewer ID as random effects in the same model would almost certainly lead to model fit problems, and so we started with simpler models including just effect ID and reviewer ID. However, even with this simpler structure, our dataset was sparse, with reviewers rating a small number of analyses, resulting in models with singular fit (Section C.2). Removing one of the random effects was necessary for the models to converge. The models that included the categorical quality rating converged when including reviewer ID, and the models that included the continuous quality rating converged when including effect ID.\nWe conducted analyses only with the final peer ratings after the opportunity for revision, not with the initial ratings. This was because when we recorded the final ratings, they over-wrote the initial ratings, and so we did not have access to those initial values.\n\n\n\n\nThe next set of univariate analyses sought to explain deviations from the mean effects based on a measure of the distinctiveness of the set of variables included in each analysis. As a ‘distinctiveness’ score, we used Sorensen’s Similarity Index (an index typically used to compare species composition across sites), treating variables as species and individual analyses as sites. To generate an individual Sorensen’s value for each analysis required calculating the pairwise Sorensen’s value for all pairs of analyses (of the same dataset), and then taking the average across these Sorensen’s values for each analysis. We calculated the Sorensen’s index values using the betapart package (Baselga et al. 2023) in R:\n\\[\n\\beta_{Sorensen} = \\frac{b+c}{2a+b+c}\n\\tag{2.7}\\]\nwhere \\(a\\) is the number of variables common to both analyses, \\(b\\) is the number of variables that occur in the first analysis but not in the second and \\(c\\) is the number of variables that occur in the second analysis. We then used the per-model average Sorensen’s index value as an independent variable to predict the deviation score in a general linear model, and included no random effect since each analysis is included only once, in R (R Core Team 2022):\n\\[\n\\text{DeviationScore}_{j} \\sim \\beta \\text{Sorensen}_{j}\n\\tag{2.8}\\]\n\n\n\n\n\n\nAdditional explanation:\nWhen we planned this analysis, we anticipated that analysts would identify a single primary effect from each model, so that each model would appear in the analysis only once. Our expecation was incorrect because some analysts identified &gt;1 effect per analysis, but we still chose to specify our model as registered and not use a random effect. This is because most models produced only one effect and so we expected that specifying a random effect to account for the few cases where &gt;1 effect was included for a given model would prevent model convergence.\nNote that this analysis contrasts with the analyses in which we used reviewer ratings as predictors because in the analyses with reviewer ratings, each effect appeared in the analysis approximately four times due to multiple reviews of each analysis, and so it was much more important to account for that variance through a random effect.\n\n\n\nFinally, we conducted a multivariate analysis with the five predictors described above (peer ratings 0-100 and peer ratings of publishability 1-4; both original and revised and Sorensen’s index, plus a sixth, presence /absence of random effects) with random effects of analysis identity and reviewer identity in the lme4 package in R (Bates et al. 2015; R Core Team 2022). We had stated here in the text that we would use only the revised (final) peer ratings in this analysis, so the absence of the initial ratings is not a deviation from our plan:\n\\[\n\\begin{align}\n{\\text{DeviationScore}}_{j} \\sim {\\text{RatingContinuous}}_{ij}\\space+ \\\\\n{\\text{RatingCategorical}}_{ij} \\space + \\\\\n{\\beta\\text{Sorensen}}_{j} \\space + \\\\\n{\\text{AnalysisID}}_{j} \\space + \\\\\n{\\text{ReviewerID}}_{i} \\\\\n{\\text{ReviewerID}}_i \\sim \\mathcal{N}(0,\\sigma^2) \\\\\n{\\text{AnalysisID}}_j \\sim \\mathcal{N}(0,\\sigma^2)\n\\end{align}\n\\tag{2.9}\\]\nWe conducted all the analyses described above eight times; for each of the four responses (\\(Z_r\\), \\(y_{25th}\\), \\(y_{50th}\\), \\(y_{75th}\\)) one time for each of the two datasets.\nWe have publicly archived all relevant data, code, and materials on the Open Science Framework (https://osf.io/mn5aj/). Archived data includes the original datasets distributed to all analysts, any edited versions of the data analyzed by individual groups, and the data we analyzed with our meta-analyses, which include the effect sizes derived from separate analyses, the statistics describing variation in model structure among analyst groups, and the anonymized answers to our surveys of analysts and peer reviewers. Similarly, we have archived both the analysis code used for each individual analysis (where available) and the code from our meta-analyses. We have also archived copies of our survey instruments from analysts and peer reviewers.\nOur rules for excluding data from our study were as follows. We excluded from our synthesis any individual analysis submitted after we had completed peer review or those unaccompanied by analysis files that allow us to understand what the analysts did. We also excluded any individual analysis that did not produce an outcome that could be interpreted as an answer to our primary question (as posed above) for the respective dataset. For instance, this means that in the case of the data on blue tit chick growth, we excluded any analysis that did not include something that can be interpreted as growth or size as a dependent (response) variable, and in the case of the Eucalyptus establishment data, we excluded any analysis that did not include a measure of grass cover among the independent (predictor) variables. Also, as described above, any analysis that could not produce an effect that could be converted to a signed \\(Z_r\\) was excluded from analyses of \\(Z_r\\).\n\n\n\n\n\n\nPreregistration Deviation:\nSome analysts had difficulty implementing our instructions to derive the out-of-sample predictions, and in some cases (especially for the Eucalyptus data), they submitted predictions with implausibly extreme values. We believed these values were incorrect and thus made the conservative decision to exclude out-of-sample predictions where the estimates were &gt; 3 standard deviations from the mean value from the full dataset.\n\n\n\n\n\n\n\n\n\nAdditional explanation: We conducted several unregistered analyses.\n1. Evaluating model fit.\nWe evaluated all fitted models using the performance() function from the performance package (Lüdecke et al. 2021) and the glance() function from the broom.mixed package (Bolker et al. 2022). For all models, we calculated the square root of the residual variance (Sigma) and the root mean squared error (RMSE). For GLMMs performance() calculates the marginal and conditional \\(R^2\\) values as well as the contribution of random effects (ICC), based on Nakagawa et al. (2017). The conditional \\(R^2\\) accounts for both the fixed and random effects, while the marginal \\(R^2\\) considers only the variance of the fixed effects. The contribution of random effects is obtained by subtracting the marginal \\(R^2\\) from the conditional \\(R^2\\).\n2. Exploring outliers and analysis quality.\nAfter seeing the forest plots of \\(Z_r\\) values and noticing the existence of a small number of extreme outliers, especially from the Eucalyptus analyses, we wanted to understand the degree to which our heterogeneity estimates were influenced by these outliers. To explore this question, we removed the highest two and lowest two values of \\(Z_r\\) in each dataset and re-calculated our heterogeneity estimates.\nTo help understand the possible role of the quality of analyses in driving the heterogeneity we observed among estimates of \\(Z_r\\), we recalculated our heterogeneity estimates after removing all effects from analysis teams that had received at least one rating of “deeply flawed and unpublishable” and then again after removing all effects from analysis teams with at least one rating of either “deeply flawed and unpublishable” or “publishable with major revisions”. We also used self-identified levels of statistical expertise to examine heterogeneity when we retained analyses only from analysis teams that contained at least one member who rated themselves as “highly proficient” or “expert” (rather than “novice” or “moderately proficient”) in conducting statistical analyses in their research area in our intake survey.\n3. Exploring possible impacts of lower quality estimates of degrees of freedom.\nOur meta-analyses of variation in \\(Z_r\\) required variance estimates derived from estimates of the degrees of freedom in original analyses from which \\(Z_r\\) estimates were derived. While processing the estimates of degrees of freedom submitted by analysts, we identified a subset of these estimates in which we had lower confidence because two or more effects from the same analysis were submitted with identical degrees of freedom. We therefore conducted a second set of (more conservative) meta-analyses that excluded these \\(Z_r\\) estimates with identical estimates of degrees of freedom and we present these analyses in the supplement.\n\n\n\n\n\n\n\n\n\nAdditional explanation: Best practices in many-analysts research.\nAfter we initiated our project, a paper was published outlining best practices in many-analysts studies (Aczel et al. 2021). Although we did not have access to this document when we implemented our project, our study complies with these practices nearly completely. The one exception is that although we requested analysis code from analysts, we did not require submission of code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#step-6-facilitated-discussion-and-collaborative-write-up-of-manuscript",
    "href": "index.html#step-6-facilitated-discussion-and-collaborative-write-up-of-manuscript",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.6 Step 6: Facilitated Discussion and Collaborative Write-Up of Manuscript",
    "text": "2.6 Step 6: Facilitated Discussion and Collaborative Write-Up of Manuscript\nWe planned for analysts and initiating authors to discuss the limitations, results, and implications of the study and collaborate on writing the final manuscript for review as a stage-2 Registered Report.\n\n\n\n\n\n\nPreregistration Deviation: As described above, due to the large number of recruited analysts and reviewers and the anticipated challenges of receiving and integrating feedback from so many authors, we limited analyst and reviewer participation in the production of the final manuscript to an invitation to call attention to serious problems with the manuscript draft.\n\n\n\nAll data cleaning and preparation for our analyses was conducted in R (R Core Team 2022) and is publicly archived at https://zenodo.org/doi/10.5281/zenodo.10046152. Please see Section 4.1 for the full list of packages and their citations used in our analysis pipeline.\nWe built an R package, ManyEcoEvo to conduct the analyses described in this chapter https://github.com/egouldo/ManyEcoEvo/. This same package can be used to reproduce our analyses or replicate the analyses described here using alternate datasets. The full suite of analyses we conducted is completely reproducible and can be reproduced and queried using (WE WILL INSERT LINK TO BINDR when coding complete).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#ethics-consent-and-permissions",
    "href": "index.html#ethics-consent-and-permissions",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "2.7 Ethics, consent and permissions",
    "text": "2.7 Ethics, consent and permissions\nWe obtained permission to conduct this research from the Whitman College Institutional Review Board (IRB). As part of this permission, the IRB approved the consent form (https://osf.io/xyp68/) that all participants completed prior to joining the study. The authors declare that they have no competing interests.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#summary-statistics",
    "href": "index.html#summary-statistics",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.1 Summary Statistics",
    "text": "3.1 Summary Statistics\nIn total, 173 analyst teams, comprising 246 analysts, contributed 182 usable analyses of the two datasets examined in this study which yielded 215 effects. Analysts produced 135 distinct effects that met our criteria for inclusion in at least one of our meta-analyses for the blue tit dataset. Analysts produced 81 distinct effects meeting our criteria for inclusion for the Eucalyptus dataset. Excluded analyses and effects either did not answer our specified biological questions, were submitted with insufficient information for inclusion in our meta-analyses, or were incompatible with production of our effect size(s). We expected this final scenario (incompatible analyses), for instance we cannot extract a \\(Z_r\\) from random forest models, which is why we analyzed two distinct types of effects, \\(Z_r\\) and out-of-sample. Effects included in only a subset of our meta-analyses provided sufficient information for inclusion in only that subset (see Table A.1). For both datasets, most submitted analyses incorporated mixed effects. Submitted analyses of the blue tit dataset typically specified normal error and analyses of the Eucalyptus dataset typically specified a non-normal error distribution (Table A.1).\nFor both datasets, the composition of models varied substantially in regards to the number of fixed and random effects, interaction terms, and the number of data points used, and these patterns differed somewhat between the blue tit and Eucalyptus analyses (See Table A.2). Focussing on the models included in the \\(Z_r\\) analyses (because this is the larger sample), blue tit models included a similar number of fixed effects on average (mean 5.2 \\(\\pm\\) 2.92 SD) as Eucalyptus models (mean 5.01 \\(\\pm\\) 3.83 SD), but the standard deviation in number of fixed effects was somewhat larger in the Eucalyptus models. The average number of interaction terms was much larger for the blue tit models (mean 0.44 \\(\\pm\\) 1.11 SD) than for the Eucalyptus models (mean 0.16 \\(\\pm\\) 0.65 SD), but still under 0.5 for both, indicating that most models did not contain interaction terms. Blue tit models also contained more random effects (mean 3.53 \\(\\pm\\) 2.08 SD) than Eucalyptus models (mean 1.41 \\(\\pm\\) 1.09 SD). The maximum possible sample size in the blue tit dataset (3720 nestlings) was an order of magnitude larger than the maximum possible in the Eucalyptus dataset (351 plots), and the means and standard deviations of the sample size used to derive the effects eligible for our study were also an order of magnitude greater for the blue tit dataset (mean 2622.07 \\(\\pm\\) 939.28 SD) relative to the Eucalyptus models (mean 298.43 \\(\\pm\\) 106.25 SD). However, the standard deviation in sample size from the Eucalyptus models was heavily influenced by a few cases of dramatic sub-setting (described below). Approximately three quarters of Eucalyptus models used sample sizes within 3% of the maximum. In contrast, fewer than 20% of blue tit models relied on sample sizes within 3% of the maximum, and approximately 50% of blue tit models relied on sample sizes 29% or more below the maximum.\nAnalysts provided qualitative descriptions of the conclusions of their analyses. Each analysis team provided one conclusion per dataset. These conclusions could take into account the results of any formal analyses completed by the team as well as exploratory and visual analyses of the data. Here we summarize all qualitative responses, regardless of whether we had sufficient information to use the corresponding model results in our quantitative analyses below. We classified these conclusions into the categories summarized below (Table 3.1):\nMixed: some evidence supporting a positive effect, some evidence supporting a negative effect\nConclusive negative: negative relationship described without caveat\nQualified negative: negative relationship but only in certain circumstances or where analysts express uncertainty in their result\nConclusive none: analysts interpret the results as conclusive of no effect\nNone qualified: analysts describe finding no evidence of a relationship but they describe the potential for an undetected effect\nQualified positive: positive relationship described but only in certain circumstances or where analysts express uncertainty in their result\nConclusive positive: positive relationship described without caveat\nFor the blue tit dataset, most analysts concluded that there was negative relationship between measures of sibling competition and nestling growth, though half the teams expressed qualifications or described effects as mixed or absent. For the Eucalyptus dataset, there was a broader spread of conclusions with at least one analyst team providing conclusions consistent with each conclusion category. The most common conclusion for the Eucalyptus dataset was that there was no relationship between grass cover and Eucalyptus recruitment (either conclusive or qualified description of no relationship), but more than half the teams concluded that there were effects; negative, positive, or mixed.\n\n\n\n\nTable 3.1: Tallies of analysts’ qualitative answers to the research questions addressed by their analyses.\n\n\n\n\n\n\n\n\n\nDataset\nMixed\nNegative Conclusive\nNegative Qualified\nNone Conclusive\nNone Qualified\nPositive Qualified\nPositive Conclusive\n\n\n\n\nblue tit\n5\n37\n27\n4\n1\n0\n0\n\n\nEucalyptus\n8\n6\n12\n19\n12\n4\n2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#distribution-of-effects",
    "href": "index.html#distribution-of-effects",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.2 Distribution of Effects",
    "text": "3.2 Distribution of Effects\n\nEffect Sizes (\\(Z_r\\))\nAlthough the majority (111 of 132) of the usable \\(Z_r\\) effects from the blue tit dataset found nestling growth decreased with sibling competition, and the meta-analytic mean \\(Z_r\\) (Fisher’s transformation of the correlation coefficient) was convincingly negative (-0.35, -0.35 \\(\\pm\\) 0.06, 0.08 95% CI), there was substantial variability in the strength and the direction of this effect. \\(Z_r\\) ranged from -1.55 to 0.38, and approximately continuously from (SECOND SMALLEST to SECOND LARGEST) ( Figure 3.1 (a) and Table 3.4), and of the 111 effects with negative slopes, 92 had confidence intervals exluding 0. Of the 20 with positive slopes indicating increased nestling growth in the presence of more siblings, 3 had confidence intervals excluding zero (Figure 3.1 A).\nMeta-analysis of the Eucalyptus dataset also showed substantial variability in the strength of effects as measured by \\(Z_r\\), and unlike with the blue tits, a notable lack of consistency in the direction of effects (Figure 3.1 (b), Table 3.4). \\(Z_r\\) ranged from -4.47 (Figure A.2), indicating a strong tendency for reduced Eucalyptus seedling success as grass cover increased, to 0.39, indicating the opposite. Although the range of reported effects skewed strongly negative, this was due to a small number of substantial outliers. Most values of \\(Z_r\\) were relatively small with values &lt; |0.2| and the meta-analytic mean effect size was close to zero (-0.09, -0.17 \\(\\pm\\) 0.12, 0.26 95% CI). Of the 79 effects, fifty-three had confidence intervals overlapping zero, approximately a quarter (fifteen) crossed the traditional threshold of statistical significance indicating a negative relationship between grass cover and seedling success, and eleven crossed the significance threshold indicating a positive relationship between grass cover and seedling success (Figure 3.1 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Blue tit analyses: Points where \\(Z_r\\) are less than 0 indicate analyses that found a negative relationship between sibling number and nestling growth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Eucalyptus analyses: Points where \\(Z_r\\) are less than 0 indicate a negative relationship between grass cover and Eucalyptus seedling success.\n\n\n\n\n\n\n\nFigure 3.1: Forest plots of meta-analytic estimated standardized effect sizes (\\(Z_r\\), blue circles) and their 95% confidence intervals for each effect size included in the meta-analysis model. The meta-analytic mean effect size is denoted by a black triangle and a dashed vertical line, with error bars also representing the 95% confidence interval. The solid black vertical line demarcates effect size of 0, indicating no relationship between the test variable and the response variable. Note that the Eucalyptus plot omits one extreme outlier with the value of -4.47 (Figure A.2) in order to standardize the x-axes on these two panels.\n\n\n\n\n\n\nOut-of-sample predictions \\(y_{i}\\)\nAs with the effect size \\(Z_r\\), we observed substantial variability in the size of out-of-sample predictions derived from the analysts’ models. Blue tit predictions (Figure 3.2), which were z-score-standardised to accommodate the use of different response variables, always ranged far in excess of one standard deviation. In the \\(y_{25}\\) scenario, model predictions ranged from -1.85 to 0.43 (a range of 2.68 standard deviations), in the \\(y_{50}\\) they ranged from -0.53 to 1.11 (a range of 1.63 standard deviations), and in the \\(y_{75}\\) scenario they ranged from -0.03 to 1.58 (a range of 1.9 standard deviations). As should be expected given the existence of both negative and positive \\(Z_r\\) values, all three out-of-sample scenarios produced both negative and positive predictions, although as with the \\(Z_r\\) values, there is a clear trend for scenarios with more siblings to be associated with smaller nestlings. This is supported by the meta-analytic means of these three sets of predictions which were -0.66 (95% CI -0.82–0.5) for the \\(y_{25}\\), 0.34 (95% CI 0.2-0.48) for the \\(y_{50}\\), and 0.67 (95% CI 0.57-0.77) for the \\(y_{75}\\).\nEucalyptus out-of-sample predictions also varied substantially (Figure 3.3), but because they were not z-score-standardised and are instead on the original count scale, the types of interpretations we can make differ. The predicted Eucalyptus seedling counts per 15 x 15 m plot for the \\(y_{25}\\) scenario ranged from 0.04 to 33.66, for the \\(y_{50}\\) scenario ranged from 0.03 to 13.02, and for the \\(y_{75}\\) scenario they ranged from 0.05 to 21.93. The meta-analytic mean predictions for these three scenarios were similar; 0.58 (95% CI 0.21-1.37) for the \\(y_{25}\\), 0.92 (95% CI 0.36-1.65) for the \\(y_{50}\\), and 1.67 (95% CI 0.8-2.83) for the \\(y_{75}\\) scenarios respectively.\n\n\n\n\n\n\n\n\nFigure 3.2: Forest plot of meta-analytic estimated standardized (z-score) blue tit out-of-sample predictions, \\(y_i\\). Circles represent individual estimates. Triangles represent the meta-analytic mean for each prediction scenario. Dark-blue points correspond to \\(y_{25}\\) scenario, medium-blue points correspond to the \\(y_{50}\\) scenario, while light blue points correspond to the \\(y_{75}\\) scenario. Error bars are 95% confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Forest plot of meta-analytic estimated Eucalyptus out-of-sample predictions, \\(y_{i}\\), on the response-scale (stems counts). Circles represent individual estimates. Triangles represent the meta-analytic mean for each prediction scenario. Dark-blue points correspond to \\(y_{25}\\) scenario, medium-blue points correspond to the \\(y_{50}\\) scenario, while light blue points correspond to the \\(y_{75}\\) scenario. Error bars are 95% confidence intervals. Outliers (observations more than 3SD above the mean) have been removed prior to model fitting and do not appear on this figure. x-axis is truncated to approx. 40, and thus some error bars are incomplete. See Figure B.5 for full figure.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#quantifying-heterogeneity",
    "href": "index.html#quantifying-heterogeneity",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.3 Quantifying Heterogeneity",
    "text": "3.3 Quantifying Heterogeneity\n\nEffect Sizes (\\(Z_r\\))\nWe quantified both absolute (\\(\\tau^{2}\\)) and relative (\\(I^{2}\\)) heterogeneity resulting from analytical variation. Both measures suggest that substantial variability among effect sizes was attributable to the analytical decisions of analysts.\nThe total absolute level of variance beyond what would typically be expected due to sampling error, \\(\\tau^{2}\\) (Table 3.2), among all usable blue tit effects was 0.088 and for Eucalyptus effects was 0.267. This is similar to or exceeding the median value (0.105) of \\(\\tau^{2}\\) found across 31 recent meta-analyses (calculated from the data in Yang et al. 2023). The similarity of our observed values to values from meta-analyses of different studies based on different data suggest the potential for a large portion of heterogeneity to arise from analytical decisions. For further discussion of interpretation of \\(\\tau^{2}\\) in our study, please consult discussion of post hoc analyses below.\n\n\n\n\nTable 3.2: Heterogeneity in the estimated effects \\(Z_r\\) for meta-analyses of the full dataset, as well as from post hoc analyses including the dataset with outliers removed, the dataset excluding effects from analysis teams with at least one “unpublishable” rating, the dataset excluding effects from analysis teams with at least one “major revisions” rating or worse, or the dataset including only analyses from teams in which at least one analyst rated themselves as “highly proficient” or “expert” in statistical analysis. \\({\\tau}_{Team}^{2}\\) is the absolute heterogeneity for the random effect Team, \\({\\tau}_{effectID}^{2}\\) is the absolute heterogeneity for the random effect EffectID, nested under Team, and \\({\\tau}_{total}^{2}\\) is the total absolute heterogeneity. \\({I^2}_{Total}\\) is the proportional heterogeneity; the proportion of the variance among effects not attributable to sampling error, \\({I^2}_{Team}\\) is the subset of the proportional heterogeneity due to differences among Teams and \\({I^2}_{Team,effectID}\\) is subset of the proportional heterogeneity attributable to among-EffectID differences.\n\n\n\n\n  \n  \n\n\n\nDataset\n$${\\tau}_{total}^{2}$$\n$${\\tau}_{Team}^{2}$$\n$${\\tau}_{effectID}^{2}$$\n$${I^2}_{Total}$$\n$${I^2}_{Team}$$\n$${I^2}_{Team, effectID}$$\nN.Obs\n\n\n\n\nAll analyses\n\n\nblue tit\n0.09\n0.04\n0.05\n97.732%\n40.11%\n57.63%\n131\n\n\nEucalyptus\n0.27\n0.02\n0.25\n98.589%\n6.88%\n91.71%\n79\n\n\nAll analyses, Outliers Removed\n\n\nblue tit\n0.07\n0.05\n0.02\n97.030%\n66.90%\n30.13%\n127\n\n\nEucalyptus\n0.01\n0.00\n0.01\n66.193%\n19.27%\n46.93%\n75\n\n\nAnalyses receiving at least one 'Unpublishable' rating removed\n\n\nblue tit\n0.08\n0.03\n0.05\n97.601%\n38.10%\n59.50%\n109\n\n\nEucalyptus\n0.01\n0.01\n0.01\n79.741%\n28.32%\n51.42%\n55\n\n\nAnalyses receiving at least one 'Unpublishable' and or 'Major Revisions' rating removed\n\n\nblue tit\n0.14\n0.01\n0.13\n98.718%\n5.17%\n93.55%\n32\n\n\nEucalyptus\n0.03\n0.03\n0.00\n88.911%\n88.91%\n0.00%\n13\n\n\nAnalyses from teams that include highly proficient or expert data analysts\n\n\nblue tit\n0.10\n0.04\n0.06\n98.058%\n36.27%\n61.78%\n89\n\n\nEucalyptus\n0.58\n0.02\n0.56\n99.412%\n3.49%\n95.93%\n34\n\n\n\n\n\n\n\n\n\n\nIn our analyses, \\(I^{2}\\) is a plausible index of how much more variability among effect sizes we have observed, as a proportion, than we would have observed if sampling error were driving variability. We discuss our interpretation of \\(I^{2}\\) further in the methods, but in short, it is a useful metric for comparison to values from published meta-analyses and provides a plausible value for how much heterogeneity could arise in a normal meta-analysis with similar sample sizes due to analytical variability alone. In our study, total \\(I^{2}\\) for the blue tit \\(Z_r\\) estimates was extremely large, at 97.73%, as was the Eucalyptus estimate (98.59% Table 3.2).\nAlthough the overall \\(I^{2}\\) values were similar for both Eucalyptus and blue tit analyses, the relative composition of that heterogeneity differed. For both datasets, the majority of heterogeneity in \\(Z_r\\) was driven by differences among effects as opposed to differences among teams, though this was more prominent for the Eucalyptus dataset, where nearly all of the total heterogeneity was driven by differences among effects (91.71%) as opposed to differences among teams (6.88%) (Table 3.2).\n\n\nOut-of-sample predictions (\\(y_{i}\\))\nWe observed substantial heterogeneity among out-of-sample estimates, but the pattern differed somewhat from the \\(Z_r\\) values (Table 3.3). Among the blue tit predictions, \\(I^{2}\\) ranged from medium-high for the \\(y_{25}\\) scenario (68.36) to low (27.02) for the \\(y_{75}\\) scenario. Among the Eucalyptus predictions, \\(I^{2}\\) values were uniformly high (&gt;82%). For both datasets, most of the existing heterogeneity among predicted values was attributable to among-team differences, with the exception of the \\(y_{50}\\) analysis of the Eucalyptus dataset. We are limited in our interpretation of \\(\\tau^{2}\\) for these estimates because, unlike for the \\(Z_r\\) estimates, we have no benchmark for comparison with other meta-analyses.\n\n\n\n\nTable 3.3: Heterogeneity among the out-of-sample predictions \\({y}_{i}\\) for both blue tit and Eucalyptus datasets. \\({\\tau}_{Team}^{2}\\) is the absolute heterogeneity for the random effect Team, \\({\\tau}_{effectID}^{2}\\) is the absolute heterogeneity for the random effect EffectID, nested under Team, and \\({\\tau}_{total}^{2}\\) is the total absolute heterogeneity. \\({I^2}_{Total}\\) is the proportional heterogeneity; the proportion of the variance among effects not attributable to sampling error, \\({I^2}_{Team}\\) is the subset of the proportional heterogeneity due to differences among Teams and \\({I^2}_{Team,effectID}\\) is subset of the proportional heterogeneity attributable to among-EffectID differences.\n\n\n\n\n  \n  \n\n\n\nPrediction Scenario\n$${N}_{Obs}$$\n$${\\tau}_{Total}$$\n$${\\tau}_{Team}^{2}$$\n$${\\tau}_{effectID}^{2}$$\n$${I^2}_{Total}$$\n$${I^2}_{Team}$$\n$${I^2}_{Team, effectID}$$\n\n\n\n\nblue tit\n\n\ny25\n62\n0.14\n0.11\n0.03\n68.36%\n51.82%\n16.54%\n\n\ny50\n59\n0.07\n0.06\n0.01\n50.37%\n45.66%\n4.71%\n\n\ny75\n62\n0.02\n0.02\n0.00\n27.02%\n25.57%\n1.45%\n\n\nEucalyptus\n\n\ny25\n22\n3.05\n1.95\n1.10\n88.76%\n56.76%\n32.00%\n\n\ny50\n24\n1.61\n0.53\n1.08\n83.26%\n27.52%\n55.73%\n\n\ny75\n24\n1.69\n1.41\n0.28\n79.76%\n66.52%\n13.25%",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#post-hoc-analysis-exploring-outlier-characteristics-and-the-effect-of-outlier-removal-on-heterogeneity",
    "href": "index.html#post-hoc-analysis-exploring-outlier-characteristics-and-the-effect-of-outlier-removal-on-heterogeneity",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.4 Post-hoc Analysis: Exploring outlier characteristics and the effect of outlier removal on heterogeneity",
    "text": "3.4 Post-hoc Analysis: Exploring outlier characteristics and the effect of outlier removal on heterogeneity\n\nEffect Sizes (\\(Z_r\\))\nThe outlier Eucalyptus \\(Z_r\\) values were striking and merited special examination. The three negative outliers had very low sample sizes were based on either small subsets of the dataset or, in one case, extreme aggregation of data. The outliers associated with small subsets had sample sizes (\\(n=\\) 117, 90) that were less than half of the total possible sample size of 351. The case of extreme aggregation involved averaging all values within each of the 18 sites in the dataset.\nSurprisingly, both the largest and smallest effect sizes in the blue tit analyses (Figure 3.1 (a)) come from the same analyst (anonymous ID: Adelong), with identical models in terms of the explanatory variable structure, but with different response variables. However, the radical change in effect was primarily due to collinearity with covariates. The primary predictor variable (brood count after manipulation) was accompanied by several collinear variables, including the highly collinear (correlation of approximately 0.9 (Figure D.2)) covariate (brood size at day 14) in both analyses. In the analysis of nestling weight, brood count after manipulation showed a strong positive partial correlation with weight after controlling for brood count at day 14 and treatment category (increased, decreased, unmanipulated). In that same analysis, the most collinear covariate (the day 14 count) had a negative partial correlation with weight. In the analysis with tarsus length as the response variable, these partial correlations were almost identical in absolute magnitude, but reversed in sign and so brood count after manipulation was now the collinear predictor with the negative relationship. The two models were therefore very similar, but the two collinear predictors simply switched roles, presumably because a subtle difference in the distribution of weight and tarsus length data.\nWhen we dropped the Eucalyptus outliers, \\(I^{2}\\) decreased from high (98.59%), using Higgins’ (Higgins et al. 2003) suggested benchmark, to between moderate and high (66.19%, Table 3.2). However, more notably, \\(\\tau^2\\) dropped from 0.27 to 0.01, indicating that, once outliers were excluded, the observed variation in effects was similar to what we would expect if sampling error were driving the differences among effects (since \\(\\tau^2\\) is the variance in addition to that driven by sampling error). The interpretation of this value of \\(\\tau^2\\) in the context of our many-analyst study is somewhat different than a typical meta-analysis, however, since in our study (especially for Eucalyptus, where most analyses used almost exactly the same data points), there is almost no role for sampling error in driving the observed differences among the estimates. Thus, rather than concluding that the variability we observed among estimates (after removing outliers) was due only to sampling error (because \\(\\tau^2\\) became small: 10% of the median from Yang et al. 2023), we instead conclude that the observed variability, which must be due to the divergent choices of analysts rather than sampling error, is approximately of the same magnitude as what we would have expected if, instead, sampling error, and not analytical heterogeneity, were at work. Presumably, if sampling error had actually also been at work, it would have acted as an additional source of variability and would have led total variability among estimates to be higher. With total variability higher and thus greater than expected due to sampling error alone, \\(\\tau^2\\) would have been noticeably larger. Conversely, dropping outliers from the set of blue tit effects did not meaningfully reduce \\(I^{2}\\) , and only modestly reduced \\(\\tau^2\\) (Table 3.2). Thus, effects at the extremes of the distribution were much stronger contributors to total heterogeneity for effects from analyses of the Eucalyptus than for the blue tit dataset.\n\n\n\n\nTable 3.4: Estimated mean value of the standardised correlation coefficient, \\(Z_r\\), along with its standard error and 95% confidence intervals. We re-computed the meta-analysis for different post-hoc subsets of the data: All eligible effects, removal of effects from analysis teams that received at least one peer rating of “deeply flawed and unpublishable”, removal of any effects from analysis teams that received at least one peer rating of either “deeply flawed and unpublishable” or “publishable with major revisions”, inclusion of only effects from analysis teams that included at least one member who rated themselves as “highly proficient” or “expert” at conducting statistical analyses in their research area.\n\n\n\n\n  \n  \n\n\n\nDataset\n$$\\hat\\mu$$\n$$SE[\\hat\\mu]$$\n95%CI\nstatistic\np-value\n\n\n\n\nAll analyses\n\n\nblue tit\n−0.35\n0.03\n[−0.41,−0.28]\n−10.49\n&lt;0.001\n\n\nEucalyptus\n−0.09\n0.06\n[−0.22,0.03]\n−1.47\n0.14\n\n\nAll analyses, outliers removed\n\n\nblue tit\n−0.35\n0.03\n[−0.42,−0.29]\n−10.95\n&lt;0.001\n\n\nEucalyptus\n−0.03\n0.01\n[−0.06,0.00]\n−2.23\n0.026\n\n\nAnalyses from teams with highly proficient or expert data analysts\n\n\nblue tit\n−0.35\n0.04\n[−0.44,−0.27]\n−8.31\n&lt;0.001\n\n\nEucalyptus\n−0.17\n0.13\n[−0.43,0.10]\n−1.24\n0.2\n\n\nAnalyses receiving at least one 'Unpublishable' and or 'Major Revisions' rating removed\n\n\nblue tit\n−0.37\n0.07\n[−0.51,−0.23]\n−5.34\n&lt;0.001\n\n\nEucalyptus\n−0.04\n0.05\n[−0.15,0.07]\n−0.77\n0.4\n\n\nAnalyses receiving at least one 'Unpublishable' rating removed\n\n\nblue tit\n−0.36\n0.03\n[−0.43,−0.29]\n−10.49\n&lt;0.001\n\n\nEucalyptus\n−0.02\n0.02\n[−0.07,0.02]\n−1.15\n0.3\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-sample predictions (\\(y_{i}\\))\nWe did not conduct these post hoc analyses on the out-of-sample predictions as the number of eligible effects was smaller and the pattern of outliers differed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#post-hoc-analysis-exploring-the-effect-of-removing-analyses-with-poor-peer-ratings-on-heterogeneity",
    "href": "index.html#post-hoc-analysis-exploring-the-effect-of-removing-analyses-with-poor-peer-ratings-on-heterogeneity",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.5 Post-hoc analysis: Exploring the effect of removing analyses with poor peer ratings on heterogeneity",
    "text": "3.5 Post-hoc analysis: Exploring the effect of removing analyses with poor peer ratings on heterogeneity\n\nEffect Sizes (\\(Z_r\\))\nRemoving poorly rated analyses had limited impact on the meta-analytic means (Figure B.3). For the Eucalyptus dataset, the meta-analytic mean shifted from -0.09 to -0.02 when effects from analyses rated as unpublishable were removed, and to -0.04 when effects from analyses rated, at least once, as unpublishable or requiring major revisions were removed. Further, the confidence intervals for all of these means overlapped each of the other means (Table 3.4). We saw similar patterns for the blue tit dataset, with only small shifts in the meta-analytic mean, and confidence intervals of all three means overlapping each other mean (Table 3.4). Refitting the meta-analysis with a fixed effect for categorical ratings also showed no indication of differences in group meta-analytic means due to peer ratings (Figure B.1).\nFor the blue tit dataset, removing poorly-rated analyses led to only negligible changes in \\(I^{2}_{Total}\\) and relatively minor impacts on \\(\\tau^{2}\\) . However, for the Eucalyptus dataset, removing poorly-rated analyses led to notable reductions in \\(I^{2}_{Total}\\) and substantial reductions in \\(\\tau^{2}\\). When including all analyses, the Eucalyptus \\(I^{2}_{Total}\\) was 98.59% and \\(\\tau^{2}\\) was 0.27, but eliminating analyses with ratings of “unpublishable” reduced \\(I^{2}_{Total}\\) to 79.74% and \\(\\tau^{2}\\) to 0.01, and removing also those analyses “needing major revisions” left \\(I^{2}_{Total}\\) at 88.91% and \\(\\tau^{2}\\) at 0.03 (Table 3.2). Additionally, the allocations of \\(I^{2}\\) to the team versus individual effect were altered for both blue tit and Eucalyptus meta-analyses by removing poorly rated analyses, but in different ways. For blue tit meta-analysis, between a third and two-thirds of the total \\(I^{2}\\) was attributable to among-team variance in most analyses until both analyses rated “unpublishable” and analyses rated in need of “major revision” were eliminated, in which case almost all remaining heterogeneity was attributable to among-effect differences. In contrast, for Eucalyptus meta-analysis, the among-team component of \\(I^{2}\\) was less than third until both analyses rated “unpublishable” and analyses rated in need of “major revision” were eliminated, in which case almost 90% of heterogeneity was attributable to differences among teams.\n\n\nOut-of-sample predictions \\(y_{i}\\)\nWe did not conduct these post hoc analyses on the out-of-sample predictions as the number of eligible effects was smaller and our ability to interpret heterogeneity values for these analyses was limited.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#post-hoc-analysis-exploring-the-effect-of-including-only-analyses-conducted-by-analysis-teams-with-at-least-one-member-self-rated-as-highly-proficient-or-expert-in-conducting-statistical-analyses-in-their-research-area",
    "href": "index.html#post-hoc-analysis-exploring-the-effect-of-including-only-analyses-conducted-by-analysis-teams-with-at-least-one-member-self-rated-as-highly-proficient-or-expert-in-conducting-statistical-analyses-in-their-research-area",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.6 Post-hoc analysis: Exploring the effect of including only analyses conducted by analysis teams with at least one member self-rated as “highly proficient” or “expert” in conducting statistical analyses in their research area",
    "text": "3.6 Post-hoc analysis: Exploring the effect of including only analyses conducted by analysis teams with at least one member self-rated as “highly proficient” or “expert” in conducting statistical analyses in their research area\n\nEffect Sizes (\\(Z_r\\))\nIncluding only analyses conducted by teams that contained at least one member who rated themselves as “highly proficient” or “expert” in conducting the relevant statistical methods had negligible impacts on the meta-analytic means (Table 3.4), the distribution of \\(Z_r\\) effects (Figure B.4), or heterogeneity estimates (Table 3.2), which remained extremely high.\n\n\nOut-of-sample predictions \\(y_{i}\\)\nWe did not conduct these post hoc analyses on the out-of-sample predictions as the number of eligible effects was smaller.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#post-hoc-analysis-exploring-the-effect-of-excluding-estimates-of-z_r-in-which-we-had-reduced-confidence",
    "href": "index.html#post-hoc-analysis-exploring-the-effect-of-excluding-estimates-of-z_r-in-which-we-had-reduced-confidence",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.7 Post-hoc analysis: Exploring the effect of excluding estimates of \\(Z_r\\) in which we had reduced confidence",
    "text": "3.7 Post-hoc analysis: Exploring the effect of excluding estimates of \\(Z_r\\) in which we had reduced confidence\nAs described in our addendum to the methods, we identified a subset of estimates of \\(Z_r\\) in which we had less confidence because of features of the submitted degrees of freedom. Excluding these effects in which we had lower confidence had minimal impact on the meta-analytic mean and the estimates of total \\(I^{2}\\) and \\(\\tau^{2}\\) for both blue tit and Eucalyptus meta-analyses, regardless of whether outliers were also excluded (Table B.1).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#explaining-variation-in-deviation-scores",
    "href": "index.html#explaining-variation-in-deviation-scores",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.8 Explaining Variation in Deviation Scores",
    "text": "3.8 Explaining Variation in Deviation Scores\nNone of the pre-registered predictors explained substantial variation in deviation among submitted statistical effects from the meta-analytic mean (Table 3.5, Table 3.6). Note that the extremely high \\(R^{2}_{\\text{Conditional}}\\) values from the analyses of continuous peer ratings as predictors of deviation scores are a function of the random effects, not the fixed effect of interest. These high values of the \\(R^{2}_{\\text{Conditional}}\\) result from the fact that each effect size was included in the analysis multiple times, to allow comparison with ratings from the multiple peer reviewers who reviewed each analysis, and therefore when we included effect ID as a random effect, the observations within each random effect category were identical.\n\n\n\n\nTable 3.5: Summary metrics for registered models seeking to explain deviation (Box-Cox transformed absolute deviation scores) from the mean \\(Z_r\\) as a function of Sorensen’s Index, categorical peer ratings, and continuous peer ratings for blue tit and Eucalyptus analyses, and as a function of the presence or absence of random effects (in the analyst’s models) for Eucalyptus analyses. We report coefficient of determination, \\(R^2\\), for our models including only fixed effects as predictors of deviation, and we report \\(R^{2}_{Conditional}\\), \\(R^{2}_{Marginal}\\) and the intra-class correlation (ICC) from our models that included both fixed and random effects. For all our models, we calculated the residual standard deviation \\(\\sigma\\) and root mean squared error (RMSE).\n\n\n\n\n\n\n\n\n\nDataset\n$$R^2$$\n$$R^{2}_{Conditional}$$\n$$R^{2}_{Marginal}$$\nICC\n$$\\sigma$$\nRMSE\n$$N_{Obs.}$$\n\n\n\n\nDeviation explained by categorical ratings\n\n\nblue tit\n\n0.0903\n6.67e-03\n0.0842\n6.52e-01\n6.32e-01\n473\n\n\nEucalyptus\n\n0.1319\n1.24e-02\n0.1209\n1.06e+00\n1.02e+00\n346\n\n\nDeviation explained by continuous ratings\n\n\nblue tit\n\n1.0000\n2.00e-26\n1.0000\n1.63e-05\n1.56e-12\n473\n\n\nEucalyptus\n\n0.9998\n6.57e-30\n0.9998\n7.93e-03\n7.09e-14\n346\n\n\nDeviation explained by Sorensen's index\n\n\nblue tit\n1.11e-03\n\n\n\n6.81e-01\n6.76e-01\n124\n\n\nEucalyptus\n5.06e-04\n\n\n\n1.14e+00\n1.12e+00\n72\n\n\nDeviation explained by inclusion of random effects\n\n\nblue tit\n2.68e-02\n\n\n\n6.58e-01\n6.53e-01\n131\n\n\nEucalyptus\n8.67e-08\n\n\n\n1.12e+00\n1.10e+00\n79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.6: Parameter estimates from models of Box-Cox transformed deviation scores as a function of continuous and categorical peer ratings, Sorensen scores, and the inclusion of random effects. Standard Errors (SE), 95% confidence intervals (95%CI) are reported for all estimates, while t values, degrees of freedom and p-values are presented for fixed-effects. Note that positive parameter estimates mean that as the predictor variable increases, so does the absolute value of the deviation from the meta-analytic mean.\n\n\n\n\n  \n  \n\n\n\nParameter\nEffects\nGroup\nCoefficient\nSE\n95%CI\nt\ndf\np\n\n\n\n\nDeviation explained by inclusion of random effects - Eucalyptus\n\n\n(Intercept)\n\n\n-2.53\n0.27\n[-3.06,-1.99]\n-9.31\n77\n&lt;0.001\n\n\nMixed model\n\n\n0.00\n0.31\n[-0.60, 0.60]\n0.00\n77\n&gt;0.9\n\n\nDeviation explained by Sorensen’s index - Eucalyptus\n\n\n(Intercept)\n\n\n-2.75\n1.07\n[-4.85,-0.65]\n-2.57\n70\n0.010\n\n\nMean Sorensen's index\n\n\n0.29\n1.54\n[-2.74, 3.32]\n0.19\n70\n0.9\n\n\nDeviation explained by Sorensen’s index - blue tit\n\n\n(Intercept)\n\n\n-1.56\n0.38\n[-2.30,-0.82]\n-4.12\n122\n&lt;0.001\n\n\nMean Sorensen's index\n\n\n0.23\n0.63\n[-1.00, 1.46]\n0.37\n122\n0.7\n\n\nDeviation explained by continuous ratings - Eucalyptus\n\n\n(Intercept)\nfixed\n\n-2.51\n0.06\n[-2.63,-2.40]\n-42.58\n342\n&lt;0.001\n\n\nRateAnalysis\nfixed\n\n-6e-17\n2e-10\n[-4e-10,4e-10]\n-3e-07\n342\n&gt;0.9\n\n\nSD (Intercept)\nrandom\nEffect ID\n0.53\n0.04\n[ 0.45, 0.62]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n0.01\n3e-04\n[0.01,0.01]\n\n\n\n\n\nDeviation explained by continuous ratings - blue tit\n\n\n(Intercept)\nfixed\n\n-1.41\n0.03\n[-1.47,-1.35]\n-46.41\n469\n&lt;0.001\n\n\nRateAnalysis\nfixed\n\n-3e-15\n1e-09\n[-2e-09,2e-09]\n-2e-06\n469\n&gt;0.9\n\n\nSD (Intercept)\nrandom\nEffect ID\n0.34\n0.02\n[ 0.30, 0.39]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n2e-05\n6e-07\n[ 2e-05,2e-05]\n\n\n\n\n\nDeviation explained by categorical ratings - Eucalyptus\n\n\n(Intercept)\nfixed\n\n-2.66\n0.27\n[-3.18,-2.13]\n-9.97\n340\n&lt;0.001\n\n\nPublishable with major revision\nfixed\n\n0.29\n0.29\n[-0.27, 0.85]\n1.02\n340\n0.3\n\n\nPublishable with minor revision\nfixed\n\n0.01\n0.28\n[-0.54, 0.56]\n0.04\n340\n&gt;0.9\n\n\nPublishable as is\nfixed\n\n0.05\n0.31\n[-0.55, 0.66]\n0.17\n340\n0.9\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.39\n0.09\n[ 0.25, 0.61]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n1.06\n0.04\n[0.98,1.15]\n\n\n\n\n\nDeviation explained by categorical ratings - blue tit\n\n\n(Intercept)\nfixed\n\n-1.21\n0.15\n[-1.50,-0.93]\n-8.29\n467\n&lt;0.001\n\n\nPublishable with major revision\nfixed\n\n-0.23\n0.15\n[-0.53, 0.07]\n-1.50\n467\n0.13\n\n\nPublishable with minor revision\nfixed\n\n-0.23\n0.15\n[-0.53, 0.07]\n-1.52\n467\n0.13\n\n\nPublishable as is\nfixed\n\n-0.15\n0.17\n[-0.48, 0.18]\n-0.89\n467\n0.4\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.20\n0.05\n[ 0.13, 0.31]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n0.65\n0.02\n[0.61,0.7]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#deviation-scores-as-explained-by-reviewer-ratings",
    "href": "index.html#deviation-scores-as-explained-by-reviewer-ratings",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.9 Deviation Scores as explained by Reviewer Ratings",
    "text": "3.9 Deviation Scores as explained by Reviewer Ratings\n\nEffect Sizes (\\(Z_r\\))\nWe obtained reviews from 128 reviewers who reviewed analyses for a mean of 3.27 (range 1 - 11) analysis teams. Analyses of the blue tit dataset received a total of 240 reviews, each was reviewed by a mean of 3.87 (SD 0.71, range 3-5) reviewers. Analyses of the Eucalyptus dataset received a total of 178 reviews, each was reviewed by a mean of 4.24 (SD 0.79, range 3-6) reviewers. We tested for inter-rater-reliability to examine how similarly reviewers reviewed each analysis and found approximately no agreement among reviewers. When considering continuous ratings, IRR was 0.01, and for categorical ratings, IRR was -0.14.\nMany of the models of deviance as a function of peer ratings faced issues of failure to converge or singularity due to sparse design matrices with our pre-registered random effects (Effect_Id and Reviewer_ID) (see supplementary material C.1). These issues persisted after increasing the tolerance and changing the optimizer. For both Eucalyptus and blue tit datasets, models with continuous ratings as a predictor were singular when both pre-registered random effects were included.\nWhen using only categorical ratings as predictors, models converged only when specifying reviewer ID as a random effect. That model had a \\({R}_{C}^2\\) of 0.09 and a \\({R}_{M}^2\\) of 0.01. The model using the continuous ratings converged for both random effects (in isolation), but not both. We present results for the model using study ID as a random effect because we expected it would be a more important driver of variation in deviation scores. That model had a \\({R}_{C}^2\\) of 1 and a \\({R}_{M}^2\\) of 0.01 for the blue tit dataset and a \\({R}_{C}^2\\) of 1 and a \\({R}_{M}^2\\) of 0.01 for the Eucalyptus dataset. Neither continuous or categorical reviewer ratings of the analyses meaningfully predicted deviance from the meta-analytic mean (Table 3.6, Figure 3.4). We re-ran the multi-level meta-analysis with a fixed-effect for the categorical publishability ratings and found no difference in mean standardised effect sizes among publishability ratings (Figure B.1).\n\n\n\n\n\n\n\n\nFigure 3.4: Violin plot of Box-Cox transformed deviation from meta-analytic mean \\(Z_r\\) as a function of categorical peer rating. Grey points for each rating group denote model-estimated marginal mean deviation, and error bars denote 95% CI of the estimate. A Blue tit dataset, B Eucalyptus dataset.\n\n\n\n\n\n\n\nOut-of-sample predictions (\\(y_{i}\\))\nSome models of the influence of reviewer ratings on out-of-sample predictions (\\(y_{i}\\)) had issues with convergence and singularity of fit (see Table C.2) and those models that converged and were not singular showed no strong relationship (Figure C.2, Figure C.3), as with the \\(Zr\\) analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#deviation-scores-as-explained-by-the-distinctiveness-of-variables-in-each-analysis",
    "href": "index.html#deviation-scores-as-explained-by-the-distinctiveness-of-variables-in-each-analysis",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.10 Deviation scores as explained by the distinctiveness of variables in each analysis",
    "text": "3.10 Deviation scores as explained by the distinctiveness of variables in each analysis\n\nEffect Sizes (\\(Z_r\\))\nWe employed Sorensen’s index to calculate the distinctiveness of the set of predictor variables used in each model (Figure 3.5). The mean Sorensen’s score for blue tit analyses was 0.69 (range 0.55-0.98), and for Eucalyptus analyses was 0.59 (range 0.43-0.86).\nWe found no meaningful relationship between distinctiveness of variables selected and deviation from the meta-analytic mean (Table 3.6, Figure 3.5) for either blue tit (mean 0.23, 95% CI -1,1.46) or Eucalyptus effects (mean 0.29, 95% CI -2.74,3.32).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Blue tit\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Eucalyptus\n\n\n\n\n\n\n\nFigure 3.5: Fitted model of the Box-Cox-transformed deviation score (deviation in effect size from meta-analytic mean) as a function of the mean Sorensen’s index showing distinctiveness of the set of predictor variables. Grey ribbons on predicted values are 95% CI’s.\n\n\n\n\n\n\nOut-of-sample predictions (\\(y_{i}\\))\nAs with the \\(Z_r\\) estimates, we did not observe any convincing relationships between deviation scores of out-of-sample predictions and Sorensen’s index values. Please see supplementary material C.4.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#deviation-scores-as-explained-by-the-inclusion-of-random-effects",
    "href": "index.html#deviation-scores-as-explained-by-the-inclusion-of-random-effects",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.11 Deviation scores as explained by the inclusion of random effects",
    "text": "3.11 Deviation scores as explained by the inclusion of random effects\n\nEffect Sizes (\\(Z_r\\))\nThere were only three blue tit analyses that did not include random effects, which is below the pre-registered threshold for fitting a model of the Box-Cox transformed deviation from the meta-analytic mean as a function of whether the analysis included random-effects. However, 17 Eucalyptus analyses included only fixed effects, which crossed our pre-registered threshold. Consequently, we performed this analysis for the Eucalyptus dataset only. There was no relationship between random-effect inclusion and deviation from meta-analytic mean among the Eucalyptus analyses (Table 3.6, Figure 3.6).\n\n\n\n\n\n\n\n\nFigure 3.6: Violin plot of mean Box-Cox transformed deviation from meta-analytic mean as a function of random-effects inclusion in Eucalyptus analyses. ‘1’ indicates random-effects were included in analyst’s model, while 0 indicates no random-effects were included. White points for each group of analyses denote model-estimated marginal mean deviation, and error bars denote 95% CI of the estimate.\n\n\n\n\n\n\n\nOut-of-sample predictions (\\(y_{i}\\))\nAs with the \\(Z_r\\) estimates, we did not examine the possibility of a relationship between the inclusion of random effects and the deviation scores of the blue tit out-of-sample predictions. When we examined the possibility of this relationship for the Eucalyptus effects, we found consistent evidence of somewhat higher Box-Cox-transformed deviation values for models including a random effect, meaning the models including random effects averaged slightly higher deviation from the meta-analytic means (Figure C.5).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#multivariate-analysis-effect-size-z_r-and-out-of-sample-predictions-y_i",
    "href": "index.html#multivariate-analysis-effect-size-z_r-and-out-of-sample-predictions-y_i",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "3.12 Multivariate Analysis Effect size (\\(Z_r\\)) and Out-of-sample predictions (\\(y_{i}\\))",
    "text": "3.12 Multivariate Analysis Effect size (\\(Z_r\\)) and Out-of-sample predictions (\\(y_{i}\\))\nLike the univariate models, the multivariate models did a poor job of explaining deviations from the meta-analytic mean. Because we pre-registered a multivariate model that contained collinear predictors that produce results which are not readily interpretable, we present these models in the supplement. We also had difficulty with convergence and singularity for multivariate models of out-of-sample (\\(y_i\\)) result, and had to adjust which random effects we included (Table C.7). However, no multivariate analyses of Eucalyptus out-of-sample results avoided problems of convergence or singularity, no matter which random effects we included (Table C.7). We therefore present no multivariate Eucalyptus \\(y_i\\) models. We present parameter estimates from multivariate \\(Z_r\\) models for both datasets (Table C.5, Table C.6) and from \\(y_i\\) models from the blue tit dataset (Table C.8, Table C.9). We include interpretation of the results from these models in the supplement, but the results do not change the interpretations we present above based on the univariate analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "References",
    "text": "References\n\n\nAczel, Balazs, Barnabas Szaszi, Gustav Nilsonne, Olmo R van den Akker,\nCasper J Albers, Marcel ALM van Assen, Jojanneke A Bastiaansen, et al.\n2021. “Consensus-Based Guidance for Conducting and Reporting\nMulti-Analyst Studies.” eLife 10 (November). https://doi.org/10.7554/elife.72185.\n\n\nArif, Suchinta, and M. Aaron MacNeil. 2023. “Applying the\nStructural Causal Model Framework for Observational Causal Inference in\nEcology.” Journal Article. Ecological Monographs 93 (1):\ne1554. https://doi.org/https://doi.org/10.1002/ecm.1554.\n\n\nAtkinson, Joe, Lars A. Brudvig, Max Mallen-Cooper, Shinichi Nakagawa,\nAngela T. Moles, and Stephen P. Bonser. 2022. “Terrestrial\nEcosystem Restoration Increases Biodiversity and Reduces Its\nVariability, but Not to Reference Levels: A Global\nMeta-Analysis.” Journal Article. Ecology Letters 25 (7):\n1725–37. https://doi.org/https://doi.org/10.1111/ele.14025.\n\n\nAuspurg, Katrin, and Josef Brüderl. 2021. “Has the Credibility of\nthe Social Sciences Been Credibly Destroyed? Reanalyzing the ‘Many\nAnalysts, One Data Set’ Project.” Journal Article.\nSocius 7: 23780231211024421. https://doi.org/10.1177/23780231211024421.\n\n\nBaselga, Andres, David Orme, Sebastien Villeger, Julien De Bortoli,\nFabien Leprieur, Maxime Logez, Sara Martinez-Santalla, et al. 2023.\n“Package ‘Betapart’.” https://CRAN.R-project.org/package=betapart.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using Lme4.” Journal\nArticle. 2015 67 (1): 48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, David Robinson, Dieter Menne, Jonah Gabry, Paul Buerkner,\nChrisopher Hau, William Petry, et al. 2022. “Package\n‘Broom.mixed’.” https://github.com/bbolker/broom.mixed.\n\n\nBorenstein, Michael, Julian P. T. Higgins, Larry Hedges, and Hannah\nRothstein. 2017. “Basics of Meta-Analysis: I2 Is Not an Absolute\nMeasure of Heterogeneity.” Journal Article. Research\nSynthesis Methods 8: 5–18. https://doi.org/10.1002/jrsm.1230.\n\n\nBotvinik-Nezer, Rotem, Felix Holzmeister, Colin F. Camerer, Anna Dreber,\nJuergen Huber, Magnus Johannesson, Michael Kirchler, et al. 2020.\n“Variability in the Analysis of a Single Neuroimaging Dataset by\nMany Teams.” Nature 582 (7810): 84–88.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung H. V. Nguyen,\nMuna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022.\n“Observing Many Researchers Using the Same Data and Hypothesis\nReveals a Hidden Universe of Uncertainty.” Proceedings of the\nNational Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nBriga, Michael, and Simon Verhulst. 2021. “Mosaic Metabolic\nAgeing: Basal and Standard Metabolic Rates Age in Opposite Directions\nand Independent of Environmental Quality, Sex and Life Span in a\nPasserine.” Journal Article. Functional Ecology 35 (5):\n1055–68. https://doi.org/https://doi.org/10.1111/1365-2435.13785.\n\n\nBurnham, K. P., and D. R. Anderson. 2002. Model Selection and\nMultimodel Inference: A Practical Information-Theoretical Approach.\nBook. 2nd ed. New York: Springer-Verlag. https://doi.org/10.1007/b97636.\n\n\nCade, Brian S. 2015. “Model Averaging and Muddled Multimodel\nInferences.” Journal Article. Ecology 96 (9): 2370–82.\nhttp://www.jstor.org.ezproxy.whitman.edu/stable/24702343.\n\n\nCapilla-Lasheras, Pablo, Megan J. Thompson, Alfredo Sánchez-Tójar, Yacob\nHaddou, Claire J. Branston, Denis Réale, Anne Charmantier, and Davide M.\nDominoni. 2022. “A Global Meta-Analysis Reveals Higher Variation\nin Breeding Phenology in Urban Birds Than in Their Non-Urban\nNeighbours.” Journal Article. Ecology Letters 25 (11):\n2552–70. https://doi.org/https://doi.org/10.1111/ele.14099.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke,\nByron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023.\n“Multidimensional Signals and Analytic Flexibility: Estimating\nDegrees of Freedom in Human-Speech Analyses.” Journal Article.\nAdvances in Methods and Practices in Psychological Science 6\n(3): 25152459231162567. https://doi.org/10.1177/25152459231162567.\n\n\nDancho, Matt, and Davis Vaughan. 2023. Timetk: A Tool Kit for\nWorking with Time Series. https://CRAN.R-project.org/package=timetk.\n\n\nDeKogel, C. H. 1997. “Long-Term Effects of Brood Size Manipulation\non Morphological Development and Sex-Specific Mortality of\nOffspring.” Journal Article. Journal of Animal Ecology\n66 (2): 167–78. &lt;Go\nto ISI&gt;://WOS:A1997WQ19600003.\n\n\nDeressa, Teshome, David Stern, Jaco Vangronsveld, Jan Minx, Sebastien\nLizin, Robert Malina, and Stephan Bruns. 2023. “More Than Half of\nStatistically Significant Research Findings in the Environmental\nSciences Are Actually Not.” Journal Article. EcoEvoRxiv.\nhttps://doi.org/https://doi.org/10.32942/X24G6Z.\n\n\nDormann, Carsten F., Jane Elith, Sven Bacher, Carsten Buchmann, Gudrun\nCarl, Gabriel Carré, Jaime R. García Marquéz, et al. 2013.\n“Collinearity: A Review of Methods to Deal with It and a\nSimulation Study Evaluating Their Performance.” Journal Article.\nEcography 36 (1): 27–46. https://doi.org/https://doi.org/10.1111/j.1600-0587.2012.07348.x.\n\n\nFanelli, Daniele, Rodrigo Costas, and John P. A. Ioannidis. 2017.\n“Meta-Assessment of Bias in Science.” Journal Article.\nProceedings of the National Academy of Sciences 114: 3714–19.\nhttps://doi.org/10.1073/pnas.1618569114.\n\n\nFanelli, Daniele, and John P. A. Ioannidis. 2013. “US Studies May\nOverestimate Effect Sizes in Softer Research.” Journal Article.\nProceedings of the National Academy of Sciences 110 (37):\n15031–36. https://doi.org/10.1073/pnas.1302997110.\n\n\nFidler, Fiona, Mark A. Burgman, Geoff Cumming, Robert Buttrose, and Neil\nThomason. 2006. “Impact of Criticism of Null-Hypothesis\nSignificance Testing on Statistical Reporting Practices in Conservation\nBiology.” Journal Article. Conservation Biology 20 (5):\n1539–44. https://doi.org/10.1111/j.1523-1739.2006.00525.x.\n\n\nFidler, Fiona, Yung En Chee, Bonnie C. Wintle, Mark A. Burgman, Michael\nA. McCarthy, and Ascelin Gordon. 2017. “Metaresearch for\nEvaluating Reproducibility in Ecology and Evolution.” Journal\nArticle. BioScience 67 (3): 282–89. https://doi.org/10.1093/biosci/biw159.\n\n\nForstmeier, Wolfgang, Eric-Jan Wagenmakers, and T. H. Parker. 2017.\n“Detecting and Avoiding Likely False-Positive Findings – a\nPractical Guide.” Journal Article. Biological Reviews\n92: 1941–68. https://doi.org/10.1111/brv.12315.\n\n\nFraser, Hannah, Tim Parker, Shinichi Nakagawa, Ashley Barnett, and Fiona\nFidler. 2018. “Questionable Research Practices in Ecology and\nEvolution.” Journal Article. PLOS ONE 13 (7): e0200303.\nhttps://doi.org/10.1371/journal.pone.0200303.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking\nPaths: Why Multiple Comparisons Can Be a Problem, Even When There Is No\n‘Fishing Expedition’ or ‘p-Hacking’ and the\nResearch Hypothesis Was Posited Ahead of Time.” Department of\nStatistics, Columbia University.\n\n\nGelman, Andrew, and David Weakliem. 2009. “Of Beauty, Sex, and\nPower.” Journal Article. American Scientist 97: 310–16.\n\n\nGrueber, C. E., S. Nakagawa, R. J. Laws, and I. G. Jamieson. 2011.\n“Multimodel Inference in Ecology and Evolution: Challenges and\nSolutions.” Journal Article. Journal of Evolutionary\nBiology 24 (4): 699–711. https://doi.org/doi:10.1111/j.1420-9101.2010.02210.x.\n\n\nHiggins, Julian P T, Simon G Thompson, Jonathan J Deeks, and Douglas G\nAltman. 2003. “Measuring Inconsistency in Meta-Analyses.”\nJournal Article. BMJ 327 (7414): 557–60. https://doi.org/10.1136/bmj.327.7414.557.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey R. Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.” Journal Article. Economic Inquiry 59\n(3): 944–60. https://doi.org/https://doi.org/10.1111/ecin.12992.\n\n\nJennions, M. D., C. J. Lortie, M. S. Rosenberg, and H. R. Rothstein.\n2013. “Publication and Related Biases.” Book Section. In\nHandbook of Meta-Analysis in Ecology and Evolution, edited by\nJ. Koricheva, J. Gurevitch, and K. Mengersen, 207–36. Princeton, USA:\nPrinceton University Press.\n\n\nKimmel, Kaitlin, Meghan L. Avolio, and Paul J. Ferraro. 2023.\n“Empirical Evidence of Widespread Exaggeration Bias and Selective\nReporting in Ecology.” Journal Article. Nature Ecology &\nEvolution. https://doi.org/10.1038/s41559-023-02144-3.\n\n\nKlein, Richard A., Kate A. Ratliff, Michelangelo Vianello, Reginald B.\nAdams Jr., Štěpán Bahník, Michael J. Bernstein, Konrad Bocian, et al.\n2014. “Investigating Variation in Replicability: A \"Many Labs\"\nReplication Project.” Journal Article. Social Psychology\n45 (3): 142–52. https://doi.org/10.1027/1864-9335/a000178.\n\n\nKlein, Richard A., Michelangelo Vianello, Fred Hasselman, Byron G.\nAdams, Reginald B. Adams, Sinan Alper, Mark Aveyard, et al. 2018.\n“Many Labs 2: Investigating Variation in Replicability Across\nSamples and Settings.” Journal Article. Advances in Methods\nand Practices in Psychological Science 1 (4): 443–90. https://doi.org/10.1177/2515245918810225.\n\n\nKnight, K. 2000. Mathematical Statistics. Book. New York:\nChapman; Hall.\n\n\nKou-Giesbrecht, Sian, and Duncan N. L. Menge. 2021.\n“Nitrogen-Fixing Trees Increase Soil Nitrous Oxide Emissions: A\nMeta-Analysis.” Journal Article. Ecology 102 (8):\ne03415. https://doi.org/https://doi.org/10.1002/ecy.3415.\n\n\nKuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen.\n2017. “lmerTest Package: Tests in Linear Mixed Effects\nModels.” Journal Article. Journal of Statistical\nSoftware 82 (13): 1–26. https://doi.org/10.18637/jss.v082.i13.\n\n\nLeybourne, Daniel J., Katharine F. Preedy, Tracy A. Valentine, Jorunn I.\nB. Bos, and Alison J. Karley. 2021. “Drought Has Negative\nConsequences on Aphid Fitness and Plant Vigor: Insights from a\nMeta-Analysis.” Journal Article. Ecology and Evolution\n11 (17): 11915–29. https://doi.org/https://doi.org/10.1002/ece3.7957.\n\n\nLu, Xun, and Halbert White. 2014. “Robustness Checks and\nRobustness Tests in Applied Economics.” Journal Article.\nJournal of Econometrics 178: 194–206. https://doi.org/https://doi.org/10.1016/j.jeconom.2013.08.016.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\nWaggoner, and Dominique Makowski. 2021. “Performance: An r Package\nfor Assessment, Comparison and Testing of Statistical Models.”\nJournal Article. Journal of Open Source Software 6 (60): 3139.\nhttps://doi.org/10.21105/joss.03139.\n\n\nLuke, S. G. 2017. “Evaluating Significance in Linear Mixed-Effects\nModels in r.” Journal Article. Behavior Research Methods\n49 (4): 1494–1502.\n\n\nMiles, C. 2008. “Testing Market-Based Instruments for Conservation\nin Northern Victoria.” Book Section. In Biodiversity:\nIntegrating Conservation and Production: Case Studies from Australian\nFarms, Forests and Fisheries, edited by T. Norton, T. Lefroy, K.\nBailey, and G. Unwin, 133–46. Melbourne, Australia: CSIRO Publishing.\n\n\nMorrissey, Michael B., and Graeme D. Ruxton. 2018. “Multiple\nRegression Is Not Multiple Regressions: The Meaning of Multiple\nRegression and the Non-Problem of Collinearity.” Philosophy,\nTheory, and Practice in Biology 10 (3). https://doi.org/10.3998/ptpbio.16039257.0010.003.\n\n\nNakagawa, Shinichi, and Innes C. Cuthill. 2007. “Effect Size,\nConfidence Interval and Statistical Significance: A Practical Guide for\nBiologists.” Journal Article. Biological Reviews 82 (4):\n591–605. https://doi.org/10.1111/j.1469-185X.2007.00027.x.\n\n\nNakagawa, S., D. W. Noble, A. M. Senior, and M. Lagisz. 2017.\n“Meta-Evaluation of Meta-Analysis: Ten Appraisal Questions for\nBiologists.” BMC Biology 15 (1): 18. https://doi.org/10.1186/s12915-017-0357-7.\n\n\nNicolaus, M., S. P. M. Michler, R. Ubels, M. van der Velde, J. Komdeur,\nC. Both, and J. M. Tinbergen. 2009. “Sex-Specific Effects of\nAltered Competition on Nestling Growth and Survival: An Experimental\nManipulation of Brood Size and Sex Ratio.” Journal Article.\nJournal of Animal Ecology 78 (2): 414–26. https://doi.org/10.1111/j.1365-2656.2008.01505.x.\n\n\nNoble, Daniel W. A., Malgorzata Lagisz, Rose E. O’Dea, and Shinichi\nNakagawa. 2017. “Nonindependence and Sensitivity Analyses in\nEcological and Evolutionary Meta-Analyses.” Journal Article.\nMolecular Ecology 26 (9): 2410–25. https://doi.org/10.1111/mec.14031.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Journal Article. Science 349\n(6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nParker, Timothy H., Wolfgang Forstmeier, Julia Koricheva, Fiona Fidler,\nJarrod D. Hadfield, Yung En Chee, Clint D. Kelly, Jessica Gurevitch, and\nShinichi Nakagawa. 2016. “Transparency in Ecology and Evolution:\nReal Problems, Real Solutions.” Journal Article. Trends in\nEcology & Evolution 31 (9): 711–19. https://doi.org/10.1016/j.tree.2016.07.002.\n\n\nParker, Timothy H., and Yefeng Yang. 2023. “Exaggerated Effects in\nEcology.” Journal Article. Nature Ecology &\nEvolution. https://doi.org/10.1038/s41559-023-02156-z.\n\n\nPei, Yifan, Wolfgang Forstmeier, Daiping Wang, Katrin Martin, Joanna\nRutkowska, and Bart Kempenaers. 2020. “Proximate Causes of\nInfertility and Embryo Mortality in Captive Zebra Finches.”\nJournal Article. The American Naturalist 196 (5): 577–96. https://doi.org/10.1086/710956.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRosenberg, M. S. 2013. “Moment and Least-Squares Based Approaches\nto Metaanalytic Inference.” Book Section. In Handbook of\nMeta-Analysis in Ecology and Evolution, edited by J. Koricheva, J.\nGurevitch, and K. Mengersen, 108–24. Princeton, USA: Princeton\nUniversity Press.\n\n\nRoyle, N. J., I. R. Hartley, I. P. F. Owens, and G. A. Parker. 1999.\n“Sibling Competition and the Evolution of Growth Rates in\nBirds.” Journal Article. Proceedings of the Royal Society\nB-Biological Sciences 266 (1422): 923–32. https://doi.org/10.1098/rspb.1999.0725.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2022. GGally:\nExtension to ’Ggplot2’.\n\n\nSchweinsberg, M., M. Feldman, N. Staub, O. R. van den Akker, R. C. M.\nvan Aert, Malm van Assen, Y. Liu, et al. 2021. “Same Data,\nDifferent Conclusions: Radical Dispersion in Empirical Results When\nIndependent Analysts Operationalize and Test the Same\nHypothesis.” Journal Article. Organizational Behavior and\nHuman Decision Processes 165: 228–49. https://doi.org/10.1016/j.obhdp.2021.02.003.\n\n\nSenior, Alistair M., Catherine E. Grueber, Tsukushi Kamiya, Malgorzata\nLagisz, Katie O’Dwyer, Eduardo S. A. Santos, and Shinichi Nakagawa.\n2016. “Heterogeneity in Ecological and Evolutionary Meta-Analyses:\nIts Magnitude and Implications.” Journal Article.\nEcology 97 (12): 3293–99. https://doi.org/10.1002/ecy.1591.\n\n\nShavit, A., and Aaron M. Ellison. 2017. Stepping in the Same River\nTwice: Replication in Biological Research. Edited Book. New Haven,\nConnecticut, USA: Yale University Press.\n\n\nSiegel, Kyle R., Muskanjot Kaur, A. Calvin Grigal, Rebecca A. Metzler,\nand Gary H. Dickinson. 2022. “Meta-Analysis Suggests Negative, but\npCO2-Specific, Effects of Ocean Acidification on the Structural and\nFunctional Properties of Crustacean Biomaterials.” Journal\nArticle. Ecology and Evolution 12 (6): e8922.\nhttps://doi.org/https://doi.org/10.1002/ece3.8922.\n\n\nSilberzahn, R., E. L. Uhlmann, D. P. Martin, P. Anselmi, F. Aust, E.\nAwtrey, Š. Bahník, et al. 2018. “Many Analysts, One Data Set:\nMaking Transparent How Variations in Analytic Choices Affect\nResults.” Journal Article. Advances in Methods and Practices\nin Psychological Science 1 (3): 337–56. https://doi.org/10.1177/2515245917747646.\n\n\nSimons, Daniel J., Yuichi Shoda, and D. Stephen Lindsay. 2017.\n“Constraints on Generality (COG): A Proposed Addition to All\nEmpirical Papers.” Journal Article. Perspectives on\nPsychological Science. https://doi.org/10.1177/174569161770863.\n\n\nSimonsohn, Uri, Joseph P. Simmons, and Leif D. Nelson. 2015.\n“Specification Curve: Descriptive and Inferential Statistics on\nAll Reasonable Specifications.” Manuscript. SSRN Electronic\nJournal. https://doi.org/10.2139/ssrn.2694998\n.\n\n\n———. 2020. “Specification Curve Analysis.” Nature Human\nBehaviour 4 (11): 1208–14. https://doi.org/10.1038/s41562-020-0912-z.\n\n\nSteegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel.\n2016. “Increasing Transparency Through a Multiverse\nAnalysis.” Journal Article. Perspectives on Psychological\nScience 11 (5): 702–12. https://doi.org/10.1177/1745691616658637.\n\n\nTaylor, James W., and Kathryn S. Taylor. 2023. “Combining\nProbabilistic Forecasts of COVID-19 Mortality in the United\nStates.” Journal Article. European Journal of Operational\nResearch 304 (1): 25–41. https://doi.org/https://doi.org/10.1016/j.ejor.2021.06.044.\n\n\nVander Werf, Eric. 1992. “Lack’s Clutch Size Hypothesis: An\nExamination of the Evidence Using Meta-Analysis.” Journal\nArticle. Ecology 73 (5): 1699–1705. https://doi.org/10.2307/1940021.\n\n\nVer Hoef, Jay M. 2012. “Who Invented the Delta Method?”\nJournal Article. The American Statistician 66 (2): 124–27. https://doi.org/10.1080/00031305.2012.687494.\n\n\nVerhulst, S., M. J. Holveck, and K. Riebel. 2006. “Long-Term\nEffects of Manipulated Natal Brood Size on Metabolic Rate in Zebra\nFinches.” Journal Article. Biology Letters 2 (3):\n478–80. https://doi.org/10.1098/rsbl.2006.0496.\n\n\nVesk, P. A., W. K. Morris, W. McCallum, R. Apted, and C. Miles. 2016.\n“Processes of Woodland Eucalypt Regeneration: Lessons from the\nBush Returns Trial.” Journal Article. Proceedings of the\nRoyal Society of Victoria 128: 54–63.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in r with\nthe Metafor Package.” Journal Article. 2010 36 (3): 48.\nhttps://doi.org/10.18637/jss.v036.i03.\n\n\nYang, Yefeng, Alfredo Sánchez-Tójar, Rose E. O’Dea, Daniel W. A. Noble,\nJulia Koricheva, Michael D. Jennions, Timothy H. Parker, Malgorzata\nLagisz, and Shinichi Nakagawa. 2023. “Publication Bias Impacts on\nEffect Size, Statistical Power, and Magnitude (Type m) and Sign (Type s)\nErrors in Ecology and Evolutionary Biology.” Journal Article.\nBMC Biology 21 (1): 71. https://doi.org/10.1186/s12915-022-01485-y.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "index.html#sec-sesion-info",
    "href": "index.html#sec-sesion-info",
    "title": "Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.",
    "section": "4.1 Session Info",
    "text": "4.1 Session Info\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Australia/Melbourne\n date     2023-10-27\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package          * version    date (UTC) lib source\n abind              1.4-5      2016-07-21 [2] CRAN (R 4.2.0)\n backports          1.4.1      2021-12-13 [2] CRAN (R 4.2.0)\n base64enc          0.1-3      2015-07-28 [2] CRAN (R 4.2.0)\n bayestestR         0.13.1     2023-04-07 [2] CRAN (R 4.2.0)\n bit                4.0.5      2022-11-15 [2] CRAN (R 4.2.2)\n bit64              4.0.5      2020-08-30 [2] CRAN (R 4.2.0)\n blastula           0.3.3      2023-01-07 [2] CRAN (R 4.2.0)\n boot               1.3-28.1   2022-11-22 [2] CRAN (R 4.2.0)\n broom              1.0.4      2023-03-11 [2] CRAN (R 4.2.0)\n broom.helpers      1.14.0     2023-08-07 [2] CRAN (R 4.2.0)\n broom.mixed        0.2.9.4    2022-04-17 [2] CRAN (R 4.2.0)\n cachem             1.0.8      2023-05-01 [2] CRAN (R 4.2.0)\n callr              3.7.3      2022-11-02 [2] CRAN (R 4.2.0)\n car                3.1-2      2023-03-30 [2] CRAN (R 4.2.0)\n carData            3.0-5      2022-01-06 [2] CRAN (R 4.2.0)\n checkmate          2.2.0      2023-04-27 [2] CRAN (R 4.2.0)\n cli                3.6.1      2023-03-23 [2] CRAN (R 4.2.0)\n cluster            2.1.4      2022-08-22 [2] CRAN (R 4.2.2)\n coda               0.19-4     2020-09-30 [2] CRAN (R 4.2.0)\n codetools          0.2-19     2023-02-01 [2] CRAN (R 4.2.0)\n colorspace         2.1-0      2023-01-23 [2] CRAN (R 4.2.0)\n commonmark         1.9.0      2023-03-17 [2] CRAN (R 4.2.2)\n CompQuadForm       1.4.3      2017-04-12 [2] CRAN (R 4.2.0)\n cowplot            1.1.1      2020-12-30 [2] CRAN (R 4.2.0)\n crayon             1.5.2      2022-09-29 [2] CRAN (R 4.2.0)\n curl               5.0.0      2023-01-12 [2] CRAN (R 4.2.2)\n data.table         1.14.8     2023-02-17 [2] CRAN (R 4.2.0)\n datawizard         0.7.1      2023-04-03 [2] CRAN (R 4.2.0)\n devtools           2.4.5      2022-10-11 [2] CRAN (R 4.2.0)\n digest             0.6.33     2023-07-07 [1] CRAN (R 4.2.0)\n dplyr            * 1.1.3      2023-09-03 [1] CRAN (R 4.2.0)\n ellipsis           0.3.2      2021-04-29 [2] CRAN (R 4.2.0)\n emmeans            1.8.6      2023-05-11 [2] CRAN (R 4.2.2)\n EnvStats           2.8.0      2023-07-08 [2] CRAN (R 4.2.0)\n estimability       1.4.1      2022-08-05 [2] CRAN (R 4.2.0)\n evaluate           0.21       2023-05-05 [2] CRAN (R 4.2.0)\n fansi              1.0.5      2023-10-08 [1] CRAN (R 4.2.0)\n farver             2.1.1      2022-07-06 [2] CRAN (R 4.2.0)\n fastmap            1.1.1      2023-02-24 [2] CRAN (R 4.2.2)\n forcats          * 1.0.0      2023-01-29 [2] CRAN (R 4.2.0)\n foreign            0.8-84     2022-12-06 [2] CRAN (R 4.2.0)\n Formula            1.2-5      2023-02-24 [2] CRAN (R 4.2.0)\n fs                 1.6.2      2023-04-25 [2] CRAN (R 4.2.0)\n furrr              0.3.1      2022-08-15 [2] CRAN (R 4.2.0)\n future             1.32.0     2023-03-07 [2] CRAN (R 4.2.0)\n generics           0.1.3      2022-07-05 [2] CRAN (R 4.2.0)\n ggeffects          1.2.2      2023-05-04 [2] CRAN (R 4.2.0)\n ggforestplot       0.1.0      2022-10-24 [2] Github (NightingaleHealth/ggforestplot@547617e)\n ggplot2          * 3.4.4      2023-10-12 [1] CRAN (R 4.2.0)\n ggpubr             0.6.0      2023-02-10 [2] CRAN (R 4.2.0)\n ggsignif           0.6.4      2022-10-13 [2] CRAN (R 4.2.0)\n globals            0.16.2     2022-11-21 [2] CRAN (R 4.2.0)\n glue               1.6.2      2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra          2.3        2017-09-09 [2] CRAN (R 4.2.0)\n gt               * 0.9.0      2023-03-31 [2] CRAN (R 4.2.0)\n gtable             0.3.4      2023-08-21 [1] CRAN (R 4.2.0)\n gtsummary          1.7.2      2023-07-15 [2] CRAN (R 4.2.0)\n hardhat            1.3.0      2023-03-30 [2] CRAN (R 4.2.2)\n haven              2.5.2      2023-02-28 [2] CRAN (R 4.2.0)\n here             * 1.0.1      2020-12-13 [2] CRAN (R 4.2.0)\n Hmisc            * 5.1-0      2023-05-08 [2] CRAN (R 4.2.0)\n hms                1.1.3      2023-03-21 [2] CRAN (R 4.2.0)\n htmlTable          2.4.1      2022-07-07 [2] CRAN (R 4.2.0)\n htmltools          0.5.5      2023-03-23 [2] CRAN (R 4.2.2)\n htmlwidgets        1.6.2      2023-03-17 [2] CRAN (R 4.2.0)\n httpuv             1.6.11     2023-05-11 [2] CRAN (R 4.2.2)\n igraph             1.4.2      2023-04-07 [2] CRAN (R 4.2.0)\n insight            0.19.1     2023-03-18 [2] CRAN (R 4.2.0)\n irr              * 0.84.1     2019-01-26 [2] CRAN (R 4.2.0)\n jsonlite           1.8.4      2022-12-06 [2] CRAN (R 4.2.2)\n juicyjuice         0.1.0      2022-11-10 [2] CRAN (R 4.2.0)\n knitr              1.42       2023-01-25 [2] CRAN (R 4.2.0)\n labeling           0.4.3      2023-08-29 [1] CRAN (R 4.2.0)\n later              1.3.1      2023-05-02 [2] CRAN (R 4.2.0)\n lattice            0.21-8     2023-04-05 [2] CRAN (R 4.2.0)\n lavaan             0.6-16     2023-07-19 [2] CRAN (R 4.2.0)\n lazyeval           0.2.2      2019-03-15 [2] CRAN (R 4.2.0)\n lifecycle          1.0.3      2022-10-07 [2] CRAN (R 4.2.0)\n listenv            0.9.0      2022-12-16 [2] CRAN (R 4.2.0)\n lme4             * 1.1-33     2023-04-25 [2] CRAN (R 4.2.0)\n lpSolve          * 5.6.19     2023-09-13 [1] CRAN (R 4.2.0)\n lubridate        * 1.9.2      2023-02-10 [2] CRAN (R 4.2.0)\n magrittr           2.0.3      2022-03-30 [2] CRAN (R 4.2.0)\n ManyEcoEvo       * 1.0.0      2023-10-27 [1] Github (egouldo/ManyEcoEvo@5424771)\n markdown           1.7        2023-05-16 [2] CRAN (R 4.2.2)\n MASS               7.3-58.3   2023-03-07 [2] CRAN (R 4.2.0)\n mathjaxr           1.6-0      2022-02-28 [2] CRAN (R 4.2.0)\n Matrix           * 1.5-4      2023-04-04 [2] CRAN (R 4.2.0)\n memoise            2.0.1      2021-11-26 [2] CRAN (R 4.2.0)\n metadat          * 1.2-0      2022-04-06 [2] CRAN (R 4.2.0)\n metafor          * 4.2-0      2023-05-08 [2] CRAN (R 4.2.2)\n mime               0.12       2021-09-28 [2] CRAN (R 4.2.0)\n miniUI             0.1.1.1    2018-05-18 [2] CRAN (R 4.2.0)\n minqa              1.2.5      2022-10-19 [2] CRAN (R 4.2.0)\n mnormt             2.1.1      2022-09-26 [2] CRAN (R 4.2.0)\n modelbased         0.8.6      2023-01-13 [2] CRAN (R 4.2.0)\n MuMIn            * 1.47.5     2023-03-22 [2] CRAN (R 4.2.0)\n munsell            0.5.0      2018-06-12 [2] CRAN (R 4.2.0)\n mvtnorm            1.1-3      2021-10-08 [2] CRAN (R 4.2.0)\n NatParksPalettes   0.2.0      2022-10-09 [2] CRAN (R 4.2.0)\n nlme               3.1-162    2023-01-31 [2] CRAN (R 4.2.0)\n nloptr             2.0.3      2022-05-26 [2] CRAN (R 4.2.0)\n nnet               7.3-18     2022-09-28 [2] CRAN (R 4.2.2)\n nonnest2           0.5-6      2023-08-13 [2] CRAN (R 4.2.0)\n numDeriv         * 2016.8-1.1 2019-06-06 [2] CRAN (R 4.2.0)\n orchaRd            2.0        2023-06-28 [2] Github (daniel1noble/orchaRd@c40d961)\n parallelly         1.35.0     2023-03-23 [2] CRAN (R 4.2.0)\n parameters       * 0.21.0     2023-04-19 [2] CRAN (R 4.2.0)\n parsnip            1.1.0      2023-04-12 [2] CRAN (R 4.2.0)\n patchwork        * 1.1.2      2022-08-19 [2] CRAN (R 4.2.0)\n pbivnorm           0.6.0      2015-01-23 [2] CRAN (R 4.2.0)\n pbkrtest           0.5.2      2023-01-19 [2] CRAN (R 4.2.0)\n performance        0.10.3     2023-04-07 [2] CRAN (R 4.2.0)\n pillar             1.9.0      2023-03-22 [2] CRAN (R 4.2.0)\n pkgbuild           1.4.0      2022-11-27 [2] CRAN (R 4.2.0)\n pkgconfig          2.0.3      2019-09-22 [2] CRAN (R 4.2.0)\n pkgload            1.3.2      2022-11-16 [2] CRAN (R 4.2.0)\n pointblank         0.11.4     2023-04-25 [2] CRAN (R 4.2.0)\n pracma             2.4.2      2022-09-22 [2] CRAN (R 4.2.0)\n prettyunits        1.1.1      2020-01-24 [2] CRAN (R 4.2.0)\n processx           3.8.1      2023-04-18 [2] CRAN (R 4.2.0)\n profvis            0.3.8      2023-05-02 [2] CRAN (R 4.2.0)\n promises           1.2.0.1    2021-02-11 [2] CRAN (R 4.2.0)\n ps                 1.7.5      2023-04-18 [2] CRAN (R 4.2.0)\n purrr            * 1.0.2      2023-08-10 [1] CRAN (R 4.2.0)\n quadprog           1.5-8      2019-11-20 [2] CRAN (R 4.2.0)\n R6                 2.5.1      2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp               1.0.11     2023-07-06 [1] CRAN (R 4.2.0)\n readr            * 2.1.4      2023-02-10 [2] CRAN (R 4.2.0)\n remotes            2.4.2      2021-11-30 [2] CRAN (R 4.2.0)\n renv               1.0.2      2023-08-15 [1] CRAN (R 4.2.2)\n rlang              1.1.1      2023-04-28 [2] CRAN (R 4.2.0)\n rmarkdown          2.21       2023-03-26 [2] CRAN (R 4.2.2)\n rpart              4.1.19     2022-10-21 [2] CRAN (R 4.2.2)\n rprojroot          2.0.3      2022-04-02 [2] CRAN (R 4.2.0)\n rstatix            0.7.2      2023-02-01 [2] CRAN (R 4.2.0)\n rstudioapi         0.14       2022-08-22 [2] CRAN (R 4.2.0)\n sae                1.3        2020-03-01 [2] CRAN (R 4.2.0)\n sandwich           3.0-2      2022-06-15 [2] CRAN (R 4.2.0)\n sass               0.4.6      2023-05-03 [2] CRAN (R 4.2.0)\n scales             1.2.1      2022-08-20 [2] CRAN (R 4.2.0)\n see                0.8.0      2023-06-05 [2] CRAN (R 4.2.0)\n sessioninfo        1.2.2      2021-12-06 [2] CRAN (R 4.2.0)\n shiny              1.7.4      2022-12-15 [2] CRAN (R 4.2.0)\n sjlabelled         1.2.0      2022-04-10 [2] CRAN (R 4.2.0)\n snakecase          0.11.0     2019-05-25 [2] CRAN (R 4.2.0)\n specr            * 1.0.0      2023-01-20 [2] CRAN (R 4.2.0)\n stringi            1.7.12     2023-01-11 [2] CRAN (R 4.2.0)\n stringr          * 1.5.0      2022-12-02 [2] CRAN (R 4.2.0)\n survival           3.5-5      2023-03-12 [2] CRAN (R 4.2.0)\n tibble           * 3.2.1      2023-03-20 [2] CRAN (R 4.2.0)\n tidyr            * 1.3.0      2023-01-24 [2] CRAN (R 4.2.0)\n tidyselect         1.2.0      2022-10-10 [2] CRAN (R 4.2.0)\n tidyverse        * 2.0.0      2023-02-22 [2] CRAN (R 4.2.0)\n timechange         0.2.0      2023-01-11 [2] CRAN (R 4.2.0)\n tzdb               0.4.0      2023-05-12 [2] CRAN (R 4.2.2)\n urlchecker         1.0.1      2021-11-30 [2] CRAN (R 4.2.0)\n usethis            2.1.6      2022-05-25 [2] CRAN (R 4.2.0)\n utf8               1.2.4      2023-10-22 [1] CRAN (R 4.2.0)\n V8                 4.3.0      2023-04-08 [2] CRAN (R 4.2.0)\n vctrs              0.6.4      2023-10-12 [1] CRAN (R 4.2.0)\n vroom              1.6.3      2023-04-28 [2] CRAN (R 4.2.0)\n withr              2.5.1      2023-09-26 [1] CRAN (R 4.2.0)\n xfun               0.39       2023-04-20 [2] CRAN (R 4.2.0)\n xml2               1.3.4      2023-04-27 [2] CRAN (R 4.2.0)\n xtable             1.8-4      2019-04-21 [2] CRAN (R 4.2.0)\n yaml               2.3.7      2023-01-23 [2] CRAN (R 4.2.0)\n zoo                1.8-12     2023-04-13 [2] CRAN (R 4.2.0)\n\n [1] /Users/egould/Documents/GitHub/ManyAnalysts/renv/library/R-4.2/aarch64-apple-darwin20\n [2] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology.</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM1_summary.html#summary-statistics",
    "href": "supp_mat/SM1_summary.html#summary-statistics",
    "title": "SM A: Summarising Variation Among Analysis Specifications",
    "section": "A.1 Summary Statistics",
    "text": "A.1 Summary Statistics\n\nA.1.1 Number of analyses of different types\nAs described in the summary statistics section of the manuscript, 63 teams submitted 132 \\(Z_r\\) model estimates and 43 teams submitted 65 out of sample predictions for the blue tit dataset. Similarly, 40 submitted 79 \\(Z_r\\) model estimates and 14 teams submitted 24 out of sample predictions for the Eucalytpus dataset. The majority of the blue tit analyses specified normal error distributions and were non-Bayesian mixed effects models. Analyses of the Eucalyptus dataset rarely specified normal error distributions, likely because the response variable was in the form of counts. Mixed effects models were also common for Eucalytpus analyses (Table A.1).\n\n\nCode\nTable1 %&gt;% \n  rename_with(~ str_remove(., \"sum_\")) %&gt;% \n  group_by(dataset) %&gt;% \n  gt::gt(rowname_col = \"subset\") %&gt;% \n  gt::cols_label(dataset = \"dataset\",\n                 subset = \"Subset\",\n                 totalanalyses = \"No. Analyses\",\n                 teams = \"No. Teams\",\n                 linear = \"Normal Distribution\",\n                 mixed = \"Mixed Effect\") %&gt;% \n  gt::sub_values(columns = subset, values = c(\"effects\"), \n                 replacement = gt::md(\"$$Z_r$$\")) %&gt;% \n  gt::sub_values(columns = subset, values = c(\"predictions\"), \n                  replacement = gt::md(\"$$y_i$$\")) %&gt;% \n     gt::sub_values(columns = subset, values = c(\"all\"), \n                  replacement = gt::md(\"All analyses\")) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::text_transform(fn = function(x) ifelse(x == \"eucalyptus\", \n                                             gt::md(paste(\"*Eucalyptus*\")), x),\n                 locations = gt::cells_row_groups()) %&gt;% \n  gt::text_transform(fn = function(x) map(x, gt::md), \n                     locations = gt::cells_row_groups()) %&gt;% \n  gt::as_raw_html()\n\n\n\n\nTable A.1: Summary of the number of anaysis teams, total analyses, models with normal error distributions, mixed effects models, and models developed with Bayesian statistical methods for effect size analyses only (\\(Z_r\\)) and out-of-sample prediction only (\\(y_i\\)).\n\n\n\n\n  \n  \n\n\n\n\nNo. Analyses\nNo. Teams\nNormal Distribution\nMixed Effect\nBayesian\n\n\n\n\nblue tit\n\n\n$$Z_r$$\n132\n63\n125\n129\n10\n\n\n$$y_i$$\n65\n43\n60\n64\n10\n\n\nEucalyptus\n\n\n$$Z_r$$\n79\n40\n15\n62\n5\n\n\n$$y_i$$\n24\n14\n1\n16\n3\n\n\n\n\n\n\n\n\n\n\n\n\nA.1.2 Model composition\nThe composition of models varied substantially (Table A.2) in regards to the number of fixed and random effects, interaction terms and the number of data points used. For the blue tit dataset, models used up to 19 fixed effects, 12 random effects, and 10 interaction terms and had sample sizes ranging from 76 to 3720 For the Eucalyptus dataset models had up to 13 fixed effects, 4 random effects, 5 interaction terms and sample sizes ranging from 18 to 351.\n\n\nCode\nTable2 %&gt;% \n  rename(SD = sd) %&gt;%   \n  group_by(variable) %&gt;% \n  pivot_wider(\n    names_from = dataset,\n    names_sep = \".\",\n    values_from = c(mean, SD, min, max)\n  ) %&gt;% \n  gt::gt(rowname_col = \"subset\") %&gt;% \n gt::tab_spanner_delim(delim = \".\") %&gt;% \n    gt::fmt_scientific(columns = c(contains(\"mean\"), contains(\"SD\")),\n                     decimals = 2) %&gt;% \n   gt::cols_label_with(fn = Hmisc::capitalize) %&gt;% \n    gt::tab_style(\n    style = gt::cell_text(transform = \"capitalize\"),\n    locations = gt::cells_column_spanners()\n  ) %&gt;% \n   gt::cols_label_with(c(contains(\"Eucalyptus\")), \n                       fn = ~ gt::md(paste0(\"*\",.x, \"*\"))) %&gt;% \n    gt::sub_values(columns = subset, values = c(\"effects\"), \n                 replacement = gt::md(\"$$Z_r$$\")) %&gt;% \n   gt::sub_values(columns = subset, values = c(\"predictions\"), \n                  replacement = gt::md(\"$$y_i$$\")) %&gt;% \n     gt::sub_values(columns = subset, values = c(\"all\"), \n                  replacement = gt::md(\"All analyses\")) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::as_raw_html()\n\n\n\n\nTable A.2: Mean, standard deviation and range of number of fixed and random variables and interaction terms used in models and sample size used. Repeated for effect size analyses only (\\(Z_r\\)) and out-of-sample prediction only (\\(y_i\\)).\n\n\n\n\n  \n  \n\n\n\n\nmean\nSD\nmin\nmax\n\n\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\n\n\n\n\nfixed\n\n\n$$Z_r$$\n5.20\n5.01\n2.92\n3.83\n1\n1\n19\n13\n\n\n$$y_i$$\n4.78\n4.67\n2.35\n3.45\n1\n1\n10\n13\n\n\ninteractions\n\n\n$$Z_r$$\n4.40 × 10−1\n1.60 × 10−1\n1.11\n6.50 × 10−1\n0\n0\n10\n5\n\n\n$$y_i$$\n2.80 × 10−1\n1.70 × 10−1\n6.30 × 10−1\n4.80 × 10−1\n0\n0\n3\n2\n\n\nrandom\n\n\n$$Z_r$$\n3.53\n1.41\n2.08\n1.09\n0\n0\n10\n4\n\n\n$$y_i$$\n4.42\n9.60 × 10−1\n2.78\n8.10 × 10−1\n1\n0\n12\n3\n\n\nsamplesize\n\n\n$$Z_r$$\n2.62 × 103\n2.98 × 102\n9.39 × 102\n1.06 × 102\n76\n18\n3720\n351\n\n\n$$y_i$$\n2.84 × 103\n3.26 × 102\n7.76 × 102\n6.42 × 101\n396\n90\n3720\n350\n\n\n\n\n\n\n\n\n\n\n\n\nA.1.3 Choice of variables\nThe choice of variables also differed substantially among models (Table A.3). Considering all submitted analyses, the blue tit dataset had 52 candidate variables, which were used in a mean of 20.58 \\(Z_r\\) analyses (range 0- 101), the Eucalyptus dataset had 58 candidate variables which were used in a mean of 9.07 \\(Z_r\\) analyses (range 0-55).\n\n\nCode\n#table 3 - summary of mean, sd and range for the number of analyses in which each variable was used\nTable3 %&gt;% \n  rename(SD = sd) %&gt;% \n  pivot_wider(\n    names_from = dataset,\n    names_sep = \".\",\n    values_from = c(mean, SD, min, max)\n  ) %&gt;% \n  ungroup %&gt;% \n  gt::gt(rowname_col = \"subset\") %&gt;% \n    gt::tab_spanner_delim(delim = \".\") %&gt;% \n    gt::fmt_scientific(columns = c(contains(\"mean\"), contains(\"SD\")),\n                     decimals = 2) %&gt;% \n   gt::cols_label_with(fn = Hmisc::capitalize) %&gt;% \n   gt::cols_label_with(c(contains(\"Eucalyptus\")), fn = ~ gt::md(paste0(\"*\",.x, \"*\"))) %&gt;% \n    gt::sub_values(columns = subset, values = c(\"effects\"), \n                 replacement = gt::md(\"$$Z_r$$\")) %&gt;% \n   gt::sub_values(columns = subset, values = c(\"predictions\"), \n                  replacement = gt::md(\"$$y_i$$\")) %&gt;% \n     gt::sub_values(columns = subset, values = c(\"all\"), \n                  replacement = gt::md(\"All analyses\")) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n      tab_style(\n    style = cell_text(transform = \"capitalize\"),\n    locations = cells_column_spanners()\n  )\n\n\n\n\nTable A.3: Mean, SD, minimum and maximum number of analyses in which each variable was used, for effect size analyses only (\\(Z_r\\)), out-of-sample prediction only (\\(y_i\\)), using the full dataset.\n\n\n\n\n\n\n\n\n\n\nmean\nSD\nmin\nmax\n\n\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\nBlue tit\nEucalyptus\n\n\n\n\n$$Z_r$$\n2.06 × 101\n9.07\n2.71 × 101\n1.23 × 101\n0\n0\n101\n55\n\n\n$$y_i$$\n1.09 × 101\n2.31\n1.41 × 101\n3.88\n0\n0\n54\n17",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Summarising Variation Among Analysis Specifications</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM1_summary.html#effect-size-specification-analysis",
    "href": "supp_mat/SM1_summary.html#effect-size-specification-analysis",
    "title": "SM A: Summarising Variation Among Analysis Specifications",
    "section": "A.2 Effect Size Specification Analysis",
    "text": "A.2 Effect Size Specification Analysis\nWe used a specification curve (Simonsohn, Simmons, and Nelson 2015) to look for relationships between \\(Z_r\\) values and several modeling decisions, including the choice of independent and dependent variable, transformation of the dependent variable, and other features of the models that produced those \\(Z_r\\) values (Figure A.1, Figure A.2). Each effect can be matched to the model features that produced it by following a vertical line down the figure.\n\nA.2.1 Blue tit\nWe observed few clear trends in the blue tit specification curve (Figure A.1). The clearest trend was for the independent variable ‘contrast: reduced broods vs. unmanipulated broods’ to produce weak or even positive relationships, but never strongly negative relationships. The biological interpretation of this pattern is that nestlings in reduced broods averaged similar growth to nestlings in unmanipulated broods, and sometimes the nestlings in reduced broods even grew less than the nestlings in unmanipulated broods. Therefore, it may be that competition limits nestling growth primarily when the number of nestlings exceeds the clutch size produced by the parents, and not in unmanipulated broods. The other relatively clear trend was that the strongest negative relationships were never based on the independent variable ‘contrast: unmanipulated broods vs. enlarged broods’. These observations demonstrate the potential value of specification curves.\n\n\nCode\n# knitr::read_chunk(here::here(\"index.qmd\"), labels = \"calc_MA_mod_coefs\")\n\ncoefs_MA_mod &lt;- bind_rows( ManyEcoEvo_viz %&gt;%\n                             filter(model_name == \"MA_mod\",\n                                    exclusion_set == \"complete\",\n                                    expertise_subset == \"All\"),\n                           ManyEcoEvo_viz %&gt;%\n                             filter(model_name == \"MA_mod\",\n                                    exclusion_set == \"complete-rm_outliers\",\n                                    expertise_subset == \"All\")\n                           ) %&gt;%\n  hoist(tidy_mod_summary) %&gt;%\n  select(-starts_with(\"mod\"), -ends_with(\"plot\"), -estimate_type) %&gt;%\n  unnest(cols = c(tidy_mod_summary))\n\n\n\n\nCode\nanalytical_choices_bt &lt;- ManyEcoEvo_results$effects_analysis[[1]] %&gt;% \n  select(study_id, \n         response_transformation_description, # Don't need constructed, as this is accounted for in y\n         response_variable_name, \n         test_variable, \n         Bayesian, \n         linear_model,\n         model_subclass, \n         sample_size, \n         starts_with(\"num\"),\n         link_function_reported,\n         mixed_model) %&gt;% \n  mutate(across(starts_with(\"num\"), as.numeric),\n         response_transformation_description = case_when(is.na(response_transformation_description) ~ \"None\",\n                                                         TRUE ~ response_transformation_description)) %&gt;% \n  rename(y = response_variable_name, x = test_variable, model = linear_model) %&gt;% \n  select(study_id,x,y,model, model_subclass, response_transformation_description, link_function_reported, mixed_model, sample_size) %&gt;% \n  pivot_longer(-study_id, names_to = \"variable_type\", values_to = \"variable_name\",values_transform = as.character) %&gt;% \n  left_join(forest_plot_new_labels) %&gt;% \n  mutate(variable_name = case_when(is.na(user_friendly_name) ~ variable_name, TRUE ~ user_friendly_name)) %&gt;% \n  select(-user_friendly_name) %&gt;% \n  pivot_wider(names_from = variable_type, values_from = variable_name) %&gt;% \n  mutate(sample_size = as.numeric(sample_size))\n\n\nMA_mean_bt &lt;- ManyEcoEvo_viz$model[[1]] %&gt;% \n  broom::tidy(conf.int = TRUE) %&gt;% \n  rename(study_id = term)\n\nresults_bt &lt;- ManyEcoEvo_viz$model[[1]] %&gt;% \n  broom::tidy(conf.int = TRUE, include_studies = TRUE) %&gt;% \n  rename(study_id = term) %&gt;% \n  semi_join(analytical_choices_bt) %&gt;% \n  left_join(analytical_choices_bt)\n\nsamp_size_hist_bt &lt;- specr::plot_samplesizes(results_bt %&gt;% \n                                               rename(fit_nobs = sample_size)) +\n  cowplot::theme_half_open() +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank())\n\ncurve_bt &lt;- specr::plot_curve(results_bt) +\n  geom_hline(yintercept = 0, \n             linetype = \"dashed\", \n             color = \"black\") +\n  geom_pointrange(mapping = aes(x = 0, y = estimate, ymin = conf.low, ymax = conf.high ), \n                  data = MA_mean_bt,\n                  colour = \"black\", shape = \"diamond\") +\n  labs(x = \"\", y = \"Standardized Effect Size Zr\") +\n  cowplot::theme_half_open() +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank())\n\nspecs_bt &lt;- specr::plot_choices(results_bt %&gt;% \n                                  rename(\"Independent\\nVariable\" = x,\n                                         \"Dependent\\nVariable\" = y,\n                                         Model = model,\n                                         \"Model Subclass\" = model_subclass,\n                                         \"Mixed Model\" = mixed_model,\n                                         \"Response\\nTransformation\\nDescription\" = response_transformation_description,\n                                         \"Link Function\" = link_function_reported), \n                                choices = c(\"Independent\\nVariable\", \n                                            \"Dependent\\nVariable\", \n                                            \"Model\", \n                                            \"Model Subclass\", \n                                            \"Mixed Model\", \n                                            \"Response\\nTransformation\\nDescription\", \n                                            \"Link Function\")) +\n  labs(x = \"specifications (ranked)\") +\n  theme(strip.text.x = element_blank(),\n        strip.text.y =  element_text(size = 8, angle = 360, face = \"bold\"),\n        axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n\n\ncowplot::plot_grid(curve_bt, specs_bt, samp_size_hist_bt,\n                   ncol = 1,\n                   align = \"v\",\n                   rel_heights = c(1.5, 2.2, 0.8),\n                   axis = \"rbl\",labels = \"AUTO\") \n\n\n\n\n\n\n\n\nFigure A.1: A. Forest plot for blue tit analyses: standardized effect-sizes (circles) and their 95% confidence intervals are displayed for each analysis included in the meta-analysis model. The meta-analytic mean effect-size is denoted by a black diamond, with error bars also representing the 95% confidence interval. The dashed black line demarcates effect sizes of 0, whereby no effect of the test variable on the response variable is found. Blue points where Zr and its associated confidence intervals are greater than 0 indicate analyses that found a negative effect of sibling number on nestling growth. Gray coloured points have confidence intervals crossing 0, indicating no relationship between the test and response variable. Red points indicate the analysis found a positive relationship between sibling number and nestling growth. B. Analysis specification plot: for each analysis plotted in A, the corresponding combination of analysis decisions is plotted. Each decision and its alternative choices is grouped into its own facet, with the decision point described on the right of the panel, and the option shown on the left. Lines indicate the option chosen used in the corresponding point in plot A. C. Sample sizes of each analysis. Note that empty bars indicate analyst did not report sample size and sample size could not be derived by lead team.\n\n\n\n\n\n\n\nA.2.2 Eucalyptus\nIn the Eucalyptus specification curve, there are no strong trends (Figure A.2). It is, perhaps, the case that choosing the dependent variable ‘count of seedlings 0-0.5m high’ corresponds to more positive results and choosing ‘count of all Eucalytpus seedlings’ might find more negative results. Choosing the independent variable ‘sum of all grass types (with or without non-grass graminoids)’ might be associated with more results close to zero consistent with the absence of an effect.\n\n\nCode\nanalytical_choices_euc &lt;- ManyEcoEvo_results$effects_analysis[[2]] %&gt;% \n  select(study_id, \n         response_transformation_description, # Don't need constructed, as this is accounted for in y\n         response_variable_name, \n         test_variable, \n         Bayesian, \n         linear_model,\n         model_subclass, \n         sample_size, \n         starts_with(\"num\"),\n         transformation,\n         mixed_model,\n         link_function_reported) %&gt;% \n  mutate(across(starts_with(\"num\"), as.numeric),\n         response_transformation_description = case_when(is.na(response_transformation_description) ~ \"None\",\n                                                         TRUE ~ response_transformation_description),\n         response_variable_name = case_when(response_variable_name == \"average.proportion.of.plots.containing.at.least.one.euc.seedling.of.any.size\" ~ \"mean.prop.plots&gt;=1seedling\",\n                                            TRUE ~ response_variable_name)) %&gt;% \n  rename(y = response_variable_name, x = test_variable, model = linear_model) %&gt;% \n  select(study_id,x,y,model, model_subclass, response_transformation_description, link_function_reported, mixed_model, sample_size) %&gt;% \n  pivot_longer(-study_id, names_to = \"variable_type\", values_to = \"variable_name\",values_transform = as.character) %&gt;% \n  left_join(forest_plot_new_labels) %&gt;% \n  mutate(variable_name = case_when(is.na(user_friendly_name) ~ variable_name, TRUE ~ user_friendly_name)) %&gt;% \n  select(-user_friendly_name) %&gt;% \n  pivot_wider(names_from = variable_type, values_from = variable_name, values_fn = list) %&gt;% \n  unnest(cols = everything()) %&gt;% #TODO remove unnest and values_fn = list when origin of duplicate entry for R_1LRqq2WHrQaENtM-1-1-1 is identified\n  mutate(sample_size = as.numeric(sample_size))\n\n\n\nMA_mean_euc &lt;- ManyEcoEvo_viz %&gt;% \n  filter(model_name == \"MA_mod\", publishable_subset == \"All\", dataset == \"eucalyptus\", exclusion_set == \"complete\") %&gt;% \n  pluck(\"model\", 1) %&gt;% \n  broom::tidy(conf.int = TRUE) %&gt;% \n  rename(study_id = term) \n\nresults_euc &lt;- ManyEcoEvo_viz %&gt;% \n  filter(model_name == \"MA_mod\", publishable_subset == \"All\", dataset == \"eucalyptus\", exclusion_set == \"complete\") %&gt;% \n  pluck(\"model\", 1) %&gt;% \n  broom::tidy(conf.int = TRUE, include_studies = TRUE) %&gt;% \n  rename(study_id = term) %&gt;% \n  semi_join(analytical_choices_euc) %&gt;% \n  left_join(analytical_choices_euc)\n\nsamp_size_hist_euc &lt;- specr::plot_samplesizes(results_euc %&gt;% rename(fit_nobs = sample_size)) +\n  cowplot::theme_half_open() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())\n\ncurve_euc &lt;- specr::plot_curve(results_euc) +\n  geom_hline(yintercept = 0, \n             linetype = \"dashed\", \n             color = \"black\") +\n  geom_pointrange(mapping = aes(x = 0, y = estimate, ymin = conf.low, ymax = conf.high ), \n                  data = MA_mean_euc,\n                  colour = \"black\", shape = \"diamond\") +\n  labs(x = \"\", y = \"Standardized Effect Size Zr\") +\n  cowplot::theme_half_open() +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank())\n\nspecs_euc &lt;- specr::plot_choices(results_euc %&gt;% \n                                  rename(\"Independent\\nVariable\" = x,\n                                         \"Dependent\\nVariable\" = y,\n                                         Model = model,\n                                         \"Model Subclass\" = model_subclass,\n                                         \"Mixed Model\" = mixed_model,\n                                         \"Response\\nTransformation\\nDescription\" = response_transformation_description,\n                                         \"Link Function\" = link_function_reported), \n                                choices = c(\"Independent\\nVariable\", \n                                            \"Dependent\\nVariable\", \n                                            \"Model\", \n                                            \"Model Subclass\", \n                                            \"Mixed Model\", \n                                            \"Response\\nTransformation\\nDescription\", \n                                            \"Link Function\")) +\n  labs(x = \"specifications (ranked)\") +\n  theme(strip.text.x = element_blank(),\n        strip.text.y =  element_text(size = 8, angle = 360, face = \"bold\"),\n        axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n\ncowplot::plot_grid(curve_euc, specs_euc, samp_size_hist_euc,\n          ncol = 1,\n          align = \"v\",\n          rel_heights = c(1.5, 2.2, 0.8),\n          axis = \"rbl\",labels = \"AUTO\") \n\n\n\n\n\n\n\n\nFigure A.2: A. Forest plot for Eucalyptus analyses: standardized effect-sizes (circles) and their 95% confidence intervals are displayed for each analysis included in the meta-analysis model. The meta-analytic mean effect-size is denoted by a black diamond, with error bars also representing the 95% confidence interval. The dashed black line demarcates effect sizes of 0, whereby no effect of the test variable on the response variable is found. Blue points where \\(Z_r\\) and its associated confidence intervals are greater than 0 indicate analyses that found a positive relationship of grass cover on Eucalyptus seedling success. Gray coloured points have confidence intervals crossing 0, indicating no relationship between the test and response variable. Red points indicate the analysis found a negative relationship between grass cover and Eucalyptus seedling success. B. Analysis specification plot: for each analysis plotted in A, the corresponding combination of analysis decisions is plotted. Each decision and its alternative choices is grouped into its own facet, with the decision point described on the right of the panel, and the option shown on the left. Lines indicate the option chosen used in the corresponding point in plot A. C. Sample sizes of each analysis. Note that empty bars indicate analyst did not report sample size and sample size could not be derived by lead team.\n\n\n\n\n\n\n\nA.2.3 Post-hoc analysis: Exploring the effect of removing analyses with poor peer-review ratings on heterogeneity\nThe forest plots in Figure B.3 compare the distributions of \\(Z_r\\) effects from our full set of analyses with the distributions of \\(Z_r\\) effects from our post-hoc analyses which removed either analyses that were reviewed at least once as being ‘unpublishable’ or analyses that were reviewed at least once as being ‘unpublishable’ or requiring ‘major revisions’. Removing these analyses from the blue tit data had little impact on the overall distribution of the results. For the Eucalytpus analyses, removing ‘unpublishable’ analyses meant dropping the extreme outlier ‘Brooklyn-2-2-1’ which made a substantial difference to the amount of observerd deviation from the meta-analytic mean.\n\n\n\n\n\n\n\n\nFigure A.3: Forest plots of meta-analytic estimated standardized effect sizes (\\(Z_r\\), blue circles) and their 95% confidence intervals for each effect size included in the meta-analysis model. The meta-analytic mean effect size is denoted by a black triangle and a dashed vertical line, with error bars also representing the 95% confidence interval. The solid black vertical line demarcates effect size of 0, indicating no relationship between the test variable and the response variable. The left side of each panel shows the analysis team names (anonymous arbitrary names assigned by us), each followed by three numbers. The first number is the submission ID (some analyst teams submitted results to us on &gt;1 submission form), the second number is the analysis ID (some analyst teams included results of &gt;1 analysis in a given submission), and the third number is the effect ID (some analysts submitted values for &gt;1 effect per analysis). Thus, each row in each forest plot is uniquely identified, but it is possible to determine which effects come from which analyses and which analysis teams. The plots in the top row depict effects from analyses of blue tit data, and the bottom row plots depict effects from analyses of Eucalyptus data. The right-most plots depict all usable effect sizes. The plots on the left side exclude effects from analysis sets that received at least one rating of “unpublishable” from peer reviewers, and the plots in the middle exclude effects from analysis sets that received at least one rating of either “unpublishable” or “major revision” from peer reviewers.\n\n\n\n\n\n\n\n\n\nSimonsohn, Uri, Joseph P. Simmons, and Leif D. Nelson. 2015. “Specification Curve: Descriptive and Inferential Statistics on All Reasonable Specifications.” Manuscript. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2694998 .",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Summarising Variation Among Analysis Specifications</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM2_EffectSizeAnalysis.html#meta-analysis",
    "href": "supp_mat/SM2_EffectSizeAnalysis.html#meta-analysis",
    "title": "SM B: Effect Size Analysis",
    "section": "B.1 Meta-analysis",
    "text": "B.1 Meta-analysis\n\nB.1.1 Effect Sizes \\(Z_r\\)\n\nB.1.1.1 Effect of categorical review rating\nThe figures below (Figure B.1,Figure B.2) shows the fixed effect of categorical review rating on deviation from the meta-analytic mean. There is very little difference in deviation for analyses in any of the review categories. It is worth noting that each analysis features multiple times in these figures corresponding to the multiple reviewers that provided ratings.\n\n\n\n\n\n\n\n\nFigure B.1: Orchard plot of meta-analytic model fitted to all eucalyptus analyses with a fixed effect for categorical peer-review ratings, and random effects for analyst ID and reviewer ID. Black circles denote coefficient mean for each categorical publishability rating. Thick error bars represent 95% confidence intervals and whiskers indicate 95% prediction intervals. Effect sizes are represented by circles and their size corresponds to the precision of the estimate.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure B.2: Orchard plot of meta-analytic model fitted to all blue tit analyses with a fixed effect for categorical peer-review ratings, and random effects for analyst ID and reviewer ID. Black circles denote coefficient mean for each categorical publishability rating. Thick error bars represent 95% confidence intervals and whiskers indicate 95% prediction intervals. Effect sizes are represented by circles and their size corresponds to the precision of the estimate.\n\n\n\n\n\n\n\nB.1.1.2 Post-hoc analysis: Exploring the effect of removing analyses with poor peer-review ratings on heterogeneity\nIn Figure B.3 we display the results of our post-hoc analysis, examining the effect of removing analyses that were reviewed at least once as being ‘unpublishable’, ‘unpublishable’ or requiring ‘major revisions’, as compared with retaining the full set of analyses. Removing these analyses from the blue tit data had little impact on the overall amount of deviation or the distribution of the results. For the Eucalytpus analyses, removing ‘unpublishable’ analyses meant dropping the outlier ‘Brooklyn-2-2-1’ which made a substantial difference to the amount of observerd deviation from the meta-analytic mean.\n\n\n\n\n\n\n\n\nFigure B.3: Forest plots of meta-analytic estimated standardized effect sizes (\\(Zr\\), blue circles) and their 95% confidence intervals for each effect size included in the meta-analysis model. The meta-analytic mean effect size is denoted by a black triangle and a dashed vertical line, with error bars also representing the 95% confidence interval. The solid black vertical line demarcates effect size of 0, indicating no relationship between the test variable and the response variable. The left side of each panel shows the analysis team names (anonymous arbitrary names assigned by us), each followed by three numbers. The first number is the submission ID (some analyst teams submitted results to us on &gt;1 submission form), the second number is the analysis ID (some analyst teams included results of &gt;1 analysis in a given submission), and the third number is the effect ID (some analysts submitted values for &gt;1 effect per analysis). Thus, each row in each forest plot is uniquely identified, but it is possible to determine which effects come from which analyses and which analysis teams. The plots in the top row depict effects from analyses of blue tit data, and the bottom row plots depict effects from analyses of Eucalyptus data. The right-most plots depict all usable effect sizes. The, plots on the left side exclude effects from analysis sets that received at least one rating of “unpublishable” from peer reviewers, and the plots in the middle exclude effects from analysis sets that received at least one rating of either “unpublishable” or “major revision” from peer reviewers.\n\n\n\n\n\n\n\nB.1.1.3 Post-hoc analysis: Exploring the effect of excluding estimates in which we had reduced confidence\nFor each dataset (blue tit, Eucalyptus), we created a second, more conservative version, that excluded effects based on estimates of \\(df\\) that we considered less reliable (Table B.1). We compared the outcomes of analyses of the primary dataset (constituted according to our registered plan) with the outcomes of analyses of the more conservative version of the dataset. We also compared results from analyses of both of these versions of the dataset to versions with our post-hoc removal of outliers described in the main text. Our more conservative exclusions (based on unreliable estimates of \\(df\\)) had minimal impact on the meta-analytic mean for both blue tit and Eucalyptus analyses, regardless of whether outliers were excluded (Table B.1).\n\n\nCode\nManyEcoEvo_viz %&gt;% \n    dplyr::filter(estimate_type == \"Zr\", \n                  model_name == \"MA_mod\") %&gt;% \n  hoist(tidy_mod_summary) %&gt;% \n  unnest(tidy_mod_summary) %&gt;% \n    filter(publishable_subset == \"All\", expertise_subset == \"All\") %&gt;% \n  select(-publishable_subset, -expertise_subset) %&gt;% \n  select(dataset, \n         exclusion_set, \n         estimate, \n         std.error, \n         statistic, \n         p.value, \n         starts_with(\"conf\")) %&gt;% \n  mutate(exclusion_set = \n           case_when(exclusion_set == \"complete\" ~ \n                       \"Primary dataset\",\n                     exclusion_set == \"complete-rm_outliers\" ~ \n                       \"Primary dataset, outliers removed\",\n                     exclusion_set == \"partial\" ~ \n                       \"Conservative exclusions\",\n                     TRUE ~ \"Conservative exclusions, outliers removed\")) %&gt;% \ngroup_by(exclusion_set) %&gt;% \n  gt::gt() %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::fmt(columns = \"p.value\",\n          fns = function(x) gtsummary::style_pvalue(x, prepend_p = FALSE)) %&gt;% \n  gt::fmt_number(columns = c(-p.value, -dataset)) %&gt;% \n  gt::cols_label(estimate = gt::md(\"$$\\\\hat\\\\mu$$\"), \n                 std.error = gt::md(\"$$SE[\\\\hat\\\\mu]$$\"),\n                 conf.low = gt::md(\"95\\\\%CI\")) %&gt;% \n  gt::cols_merge(columns = starts_with(\"conf\"), \n                 pattern = \"[{1},{2}]\") %&gt;% \n  gt::cols_move(columns = conf.low, after = std.error) \n\n\n\n\nTable B.1: Estimated meta-analytic mean, standard error, and 95% confidence intervals, from analyses of the primary data set, the more conservative version of the dataset which excluded effects based on less reliable estimates of \\(df\\), and both of these datasets with outliers removed.\n\n\n\n\n\n\n\n\n\ndataset\n$$\\hat\\mu$$\n$$SE[\\hat\\mu]$$\n95%CI\nstatistic\np.value\n\n\n\n\nPrimary dataset\n\n\nblue tit\n−0.35\n0.03\n[−0.41,−0.28]\n−10.49\n&lt;0.001\n\n\neucalyptus\n−0.09\n0.06\n[−0.22,0.03]\n−1.47\n0.14\n\n\nConservative exclusions\n\n\nblue tit\n−0.36\n0.03\n[−0.42,−0.29]\n−10.50\n&lt;0.001\n\n\neucalyptus\n−0.11\n0.07\n[−0.24,0.03]\n−1.55\n0.12\n\n\nPrimary dataset, outliers removed\n\n\nblue tit\n−0.35\n0.03\n[−0.42,−0.29]\n−10.95\n&lt;0.001\n\n\neucalyptus\n−0.03\n0.01\n[−0.06,0.00]\n−2.23\n0.026\n\n\nConservative exclusions, outliers removed\n\n\nblue tit\n−0.36\n0.03\n[−0.43,−0.30]\n−11.09\n&lt;0.001\n\n\neucalyptus\n−0.04\n0.02\n[−0.07,−0.01]\n−2.52\n0.012\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_forest &lt;- function(data, intercept = TRUE, MA_mean = TRUE ){\n  if(MA_mean == FALSE){\n    data &lt;- filter(data, term != \"Overall\")\n  }\n  \n    p &lt;- ggplot(data, aes(y = term, \n                        x =  estimate, \n                        ymin = conf.low, \n                        ymax = conf.high,\n                        # shape = point_shape,\n                        colour = parameter_type)) +\n    geom_pointrange() +\n    ggforestplot::theme_forest() +\n    theme(axis.line = element_line(size = 0.10, colour = \"black\"),\n          axis.line.y = element_blank(),\n          text = element_text(family = \"Helvetica\"),\n          axis.text.y = element_blank()) +\n    guides(shape = \"none\", colour = \"none\") +\n    coord_flip() +\n    labs(y = \"Standardised Effect Size, Zr\",\n         x = element_blank()) +\n    scale_x_continuous(breaks = c(-4,-3,-2,-1,0,1),\n                       minor_breaks = seq(from = -4.5, to = 1.5, by = 0.5)) +\n    NatParksPalettes::scale_color_natparks_d(\"Glacier\")\n    \n    if(intercept == TRUE){\n      p &lt;- p + geom_hline(yintercept = 0)\n    }\n    if(MA_mean == TRUE){\n      p &lt;- p + geom_hline(aes(yintercept = meta_analytic_mean), \n                          data = data,\n                          colour = \"#01353D\", \n                          linetype = \"dashed\")\n    }\n    \n  return(p)\n}\n\n\n\n\nCode\ncomplete_euc_data &lt;- \n  ManyEcoEvo_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         estimate_type == \"Zr\", \n         model_name == \"MA_mod\",\n         dataset == \"eucalyptus\",\n         publishable_subset == \"All\") %&gt;% \n  select(model) %&gt;% \n  mutate(plot_data = map(model, \n                         .f = ~ broom::tidy(.x, \n                                            conf.int = TRUE, \n                                            include_studies = TRUE) %&gt;% \n                           dplyr::mutate(point_shape = \n                                           ifelse(stringr::str_detect(term, \"overall\"), \n                                                  \"diamond\", \n                                                  \"circle\"),\n                                         Parameter = \n                                           forcats::fct_reorder(term, \n                                                                estimate) %&gt;% \n                                           forcats::fct_reorder(., \n                                                                point_shape,\n                                                                .desc = TRUE))\n  ),\n  meta_analytic_mean = map_dbl(plot_data, \n                               ~ filter(.x, Parameter == \"overall\") %&gt;% \n                                 pull(estimate))) %&gt;% \n  select(plot_data, meta_analytic_mean) %&gt;% \n  unnest(cols = c(\"plot_data\")) %&gt;% \n  mutate(parameter_type = case_when(str_detect(Parameter, \"overall\") ~ \"mean\",\n                                    TRUE ~ \"study\"))\n\n# complete_euc_data &lt;- \n#   complete_euc_data %&gt;% \n#   rename(id_col = term) %&gt;% \n#   group_by(type) %&gt;%  \n#   group_split() %&gt;% \n#   set_names(., complete_euc_data$type %&gt;%  unique) %&gt;% \n#   # map_if(.x = ., names(.) == \"study\",\n#          # .f = ~ anonymise_teams(.x, TeamIdentifier_lookup)) %&gt;% \n#   bind_rows() %&gt;% \n#   rename(term = id_col)\n\nmin_outlier_euc &lt;- complete_euc_data %&gt;% \n  filter(type == \"study\") %&gt;% \n  slice_min(estimate, n = 3) %&gt;% \n  pull(term)\n\nsample_size_euc_Zr &lt;- ManyEcoEvo_results %&gt;% \n    filter(exclusion_set == \"complete\", dataset == \"eucalyptus\") %&gt;% \n    pluck(\"data\", 1) %&gt;% \n    select(id_col, sample_size) %&gt;% \n    rename(term = id_col) %&gt;% \n    mutate(sample_size = as.numeric(sample_size))\n\nmean_n_euc_Zr &lt;- sample_size_euc_Zr %&gt;% \n  drop_na(sample_size) %&gt;% \n  pull(sample_size) %&gt;% \n  mean() %&gt;% \n  round(2)\n\nN_outliers_Zr_euc &lt;- sample_size_euc_Zr %&gt;% \n  filter(term %in% min_outlier_euc) %&gt;% \n   arrange(desc(sample_size))\n\n\n\n\nB.1.1.4 Post-hoc analysis: Exploring the effect of including only analyses conducted by analysis teams with at least one member self-rated as “highly proficient” or “expert” in conducting statitistical analyses in their research area\n\n\nCode\nplot_forest &lt;- function(data, intercept = TRUE, MA_mean = TRUE){\n  if (MA_mean == FALSE){\n    data &lt;- filter(data, Parameter != \"overall\")\n  }\n  \n  p &lt;- ggplot(data, aes(y = estimate, \n                        x =  term, \n                        ymin = conf.low, \n                        ymax = conf.high,\n                        shape = parameter_type,\n                        colour = parameter_type)) +\n    geom_pointrange(fatten = 2) +\n    ggforestplot::theme_forest() +\n    theme(axis.line = element_line(linewidth = 0.10, colour = \"black\"),\n          axis.line.y = element_blank(),\n          text = element_text(family = \"Helvetica\")#,\n          # axis.text.y = element_blank()\n    ) +\n    guides(shape = guide_legend(title = NULL), \n           colour = guide_legend(title = NULL)) +\n    coord_flip() +\n    ylab(bquote(Standardised~Effect~Size~Z[r])) +\n    xlab(element_blank()) +\n    # scale_y_continuous(breaks = c(-4,-3,-2,-1,0,1),\n    # minor_breaks = seq(from = -4.5, to = 1.5, by = 0.5)) +\n    NatParksPalettes::scale_color_natparks_d(\"Glacier\")\n  \n  if(intercept == TRUE){\n    p &lt;- p + geom_hline(yintercept = 0)\n  }\n  if(MA_mean == TRUE){\n    p &lt;- p + geom_hline(aes(yintercept = meta_analytic_mean), \n                        data = data,\n                        colour = \"#01353D\", \n                        linetype = \"dashed\")\n  }\n  \n  return(p)\n}\n\nbt_experts_only &lt;- \n  ManyEcoEvo_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         estimate_type == \"Zr\", \n         model_name == \"MA_mod\",\n         dataset == \"blue tit\",\n         publishable_subset == \"All\", \n         expertise_subset == \"expert\") %&gt;% \n  select(model) %&gt;% \n  mutate(plot_data = map(model, \n                         .f = ~ broom::tidy(.x, \n                                            conf.int = TRUE, \n                                            include_studies = TRUE)%&gt;% \n                           dplyr::mutate(point_shape = \n                                           ifelse(stringr::str_detect(term, \"overall\"), \n                                                  \"diamond\", \n                                                  \"circle\"),\n                                         Parameter = \n                                           forcats::fct_reorder(term, \n                                                                estimate) %&gt;% \n                                           forcats::fct_reorder(., \n                                                                point_shape,\n                                                                .desc = TRUE))\n  ),\n  meta_analytic_mean = map_dbl(plot_data, \n                               ~ filter(.x, Parameter == \"overall\") %&gt;% \n                                 pull(estimate))) %&gt;% \n  select(plot_data, meta_analytic_mean) %&gt;% \n  unnest(cols = c(\"plot_data\")) %&gt;% \n  mutate(parameter_type = case_when(str_detect(Parameter, \"overall\") ~ \"mean\",\n                                    TRUE ~ \"study\")) \n\n# bt_experts_only &lt;- \n#   bt_experts_only %&gt;% \n#   rename(id_col = term) %&gt;% \n#   group_by(type) %&gt;%  \n#   group_split() %&gt;% \n#   set_names(., bt_experts_only$type %&gt;%  unique) %&gt;% \n#   # map_if(.x = ., names(.) == \"study\",\n#          # .f = ~ anonymise_teams(.x, TeamIdentifier_lookup)) %&gt;% \n#   bind_rows() %&gt;% \n#   rename(term = id_col)\n\nbt_forest_experts &lt;- bt_experts_only %&gt;% \n  arrange(desc(type)) %&gt;% \n  mutate(type = forcats::as_factor(type)) %&gt;% \n  group_by(type) %&gt;% \n  arrange(desc(estimate),.by_group = TRUE) %&gt;% \n  mutate(term = forcats::as_factor(term),\n         point_shape = case_when(str_detect(type, \"summary\") ~ \"mean\",\n                                 TRUE ~ \"study\")) %&gt;% \n  plot_forest(intercept = TRUE, MA_mean = TRUE) +\n  theme(axis.text.x = element_text(size = 15), \n        axis.title.x = element_text(size = 15),\n        axis.text.y = element_blank()\n  ) +\n  scale_y_continuous(limits = c(-1.6, 0.65)) \n\neuc_experts_only &lt;- \n  ManyEcoEvo_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         estimate_type == \"Zr\", \n         model_name == \"MA_mod\",\n         dataset == \"eucalyptus\",\n         publishable_subset == \"All\",\n         expertise_subset == \"expert\") %&gt;% \n  select(model) %&gt;% \n  mutate(plot_data = map(model, \n                         .f = ~ broom::tidy(.x, \n                                            conf.int = TRUE, \n                                            include_studies = TRUE) %&gt;% \n                           dplyr::mutate(point_shape = \n                                           ifelse(stringr::str_detect(term, \"overall\"), \n                                                  \"diamond\", \n                                                  \"circle\"),\n                                         Parameter = \n                                           forcats::fct_reorder(term, \n                                                                estimate) %&gt;% \n                                           forcats::fct_reorder(., \n                                                                point_shape,\n                                                                .desc = TRUE))\n  ),\n  meta_analytic_mean = map_dbl(plot_data, \n                               ~ filter(.x, Parameter == \"overall\") %&gt;% \n                                 pull(estimate))) %&gt;% \n  select(plot_data, meta_analytic_mean) %&gt;% \n  unnest(cols = c(\"plot_data\")) %&gt;% \n  mutate(parameter_type = case_when(str_detect(Parameter, \"overall\") ~ \"mean\",\n                                    TRUE ~ \"study\"))\n\n\n# euc_experts_only &lt;- \n#   euc_experts_only %&gt;% \n#   rename(id_col = term) %&gt;% \n#   group_by(type) %&gt;%  \n#   group_split() %&gt;% \n#   set_names(., euc_experts_only$type %&gt;%  unique) %&gt;% \n#   # map_if(.x = ., names(.) == \"study\",\n#          # .f = ~ anonymise_teams(.x, TeamIdentifier_lookup)) %&gt;% \n#   bind_rows() %&gt;% \n#   rename(term = id_col)\n\neuc_forest_experts &lt;- euc_experts_only %&gt;% \n  arrange(desc(type)) %&gt;% \n  mutate(type = forcats::as_factor(type)) %&gt;% \n  group_by(type) %&gt;% \n  arrange(desc(estimate),.by_group = TRUE) %&gt;% \n  mutate(term = forcats::as_factor(term),\n         point_shape = case_when(str_detect(type, \"summary\") ~ \"mean\",\n                                 TRUE ~ \"study\")) %&gt;% \n  plot_forest(intercept = TRUE, MA_mean = TRUE) +\n  theme(axis.text.x = element_text(size = 15), \n        axis.title.x = element_text(size = 15),\n        axis.text.y = element_blank()\n  ) +\n  scale_y_continuous(limits = c(-5, 1), \n                     breaks = c(-5, -4, -3, -2, -1, 0, 1) )\n\n# ---- Extract Viz & Summary Stats\n\nbt_forest_experts\n\neuc_forest_experts\n\n\n\n\n\n\n\n\n\n\n\n(a) Blue tit dataset analyses\n\n\n\n\n\n\n\n\n\n\n\n(b) Eucalyptus dataset analyses\n\n\n\n\n\n\nFigure B.4: Estimated meta-analytic mean effect size (\\(Z_r\\)), standard error, and 95% confidence intervals, from analyses of the primary data set with at least one member self-rated as “highly proficient” or “expert” in conducting statistical analyses in their research area.\n\n\n\n\n\n\n\nB.1.2 Out of sample predictions \\(y_i\\)\n\nB.1.2.1 Non-truncated \\(y_{i}\\) meta-analysis forest plot\nBelow is the non-truncated version of Figure 3.3 showing a forest plot of the out-of-sample predictions, \\(y_{i}\\), on the response-scale (stems counts), for Eucalyptus analyses, showing the full error bars of all model estimates.\n\n\nCode\nplot_forest_2 &lt;- function(data, intercept = TRUE, MA_mean = TRUE, y_zoom = numeric(2L)){\n  if(MA_mean == FALSE){\n    data &lt;- filter(data, study_id != \"overall\")\n  }\n  \n  plot_data &lt;- data %&gt;% \n    group_by(study_id) %&gt;% \n    group_nest() %&gt;% \n    hoist(data, \"estimate\",.remove = FALSE) %&gt;% \n    hoist(estimate, y50 = 2) %&gt;% \n    select(-estimate) %&gt;% \n    unnest(data) %&gt;% \n    arrange(desc(type)) %&gt;% \n    mutate(type = forcats::as_factor(type)) %&gt;% \n    group_by(type) %&gt;% \n    arrange(desc(y50),.by_group = TRUE) %&gt;% \n    mutate(study_id = forcats::as_factor(study_id),\n           point_shape = case_when(str_detect(type, \"summary\") ~ \"diamond\",\n                                   TRUE ~ \"circle\"))\n  \n  p &lt;- ggplot(plot_data, aes(y = estimate, \n                        x =  study_id,\n                        ymin = conf.low, \n                        ymax = conf.high,\n                        # shape = type,\n                        shape = point_shape,\n                        colour = estimate_type\n                        )) +\n    geom_pointrange(position = position_dodge(width = 0.5)) +\n    ggforestplot::theme_forest() +\n    theme(axis.line = element_line(linewidth = 0.10, colour = \"black\"),\n          axis.line.y = element_blank(),\n          text = element_text(family = \"Helvetica\")) +\n    guides(shape = \"none\", colour = \"none\") +\n    coord_flip(ylim = y_zoom) +\n    labs(y = \"Model estimated out of sample predictions, stem counts\",\n         x = element_blank()) +\n    scale_y_continuous(breaks = scales::breaks_extended(10)) +\n    NatParksPalettes::scale_color_natparks_d(\"Glacier\") \n  \n  if(intercept == TRUE){\n    p &lt;- p + geom_hline(yintercept = 0)\n  }\n  if(MA_mean == TRUE){\n    p &lt;- p +\n      geom_hline(aes(yintercept = plot_data %&gt;%\n                       filter(type == \"summary\", estimate_type == \"y25\") %&gt;%\n                       pluck(\"estimate\")),\n                 data = data,\n                 colour = \"#01353D\",\n                 linetype = \"dashed\") +\n      geom_hline(aes(yintercept = plot_data %&gt;%\n                       filter(type == \"summary\", estimate_type == \"y50\") %&gt;%\n                       pluck(\"estimate\")),\n                 data = data,\n                 colour = \"#088096\",\n                 linetype = \"dashed\") +\n      geom_hline(aes(yintercept = plot_data %&gt;%\n                       filter(type == \"summary\", estimate_type == \"y75\") %&gt;%\n                       pluck(\"estimate\")),\n                 data = data,\n                 colour = \"#58B3C7\" ,\n                 linetype = \"dashed\")\n  }\n  \n  print(p)\n}\n\n# TODO put into R/ and build into package to call!\nfit_MA_mv &lt;- function(effects_analysis, Z_colname, VZ_colname, estimate_type){\n  Zr &lt;- effects_analysis %&gt;%  pull({{Z_colname}})\n  VZr &lt;- effects_analysis %&gt;%  pull({{VZ_colname}})\n  mod &lt;- ManyEcoEvo::fit_metafor_mv(estimate = Zr, \n                        variance = VZr, \n                        estimate_type = estimate_type, \n                        data = effects_analysis)\n  return(mod)\n}\n\nback_transformed_predictions &lt;- \n  ManyEcoEvo_yi %&gt;% \n  dplyr::mutate(data = \n                  purrr::map(data, \n                             ~ dplyr::filter(.x,\n                                             stringr::str_detect(response_variable_type, \"constructed\",                                                                       negate = TRUE)))) %&gt;% \n  prepare_response_variables_yi(estimate_type = \"yi\",\n                                param_table = ManyEcoEvo:::analysis_data_param_tables) %&gt;% \n  generate_yi_subsets()\n\n\nraw_mod_data_logged &lt;- \n  back_transformed_predictions %&gt;% \n  filter(dataset == \"eucalyptus\") %&gt;%\n  group_by(estimate_type) %&gt;% \n  select(estimate_type, data) %&gt;% \n  unnest(data) %&gt;% \n  rename(study_id = id_col) %&gt;% \n  hoist(params, param_mean = list(\"value\", 1), param_sd = list(\"value\", 2)) %&gt;% \n  rowwise() %&gt;% \n  mutate(exclusion_threshold = param_mean + 3*param_sd) %&gt;% \n  filter(fit &lt; exclusion_threshold) %&gt;% \n  mutate(log_vals = map2(fit, se.fit, log_transform, 1000)) %&gt;% \n  unnest(log_vals) %&gt;%\n  select(study_id, \n         TeamIdentifier,\n         estimate_type, \n         starts_with(\"response_\"), \n         -response_id_S2, \n         ends_with(\"_log\")) %&gt;% \n  group_by(estimate_type) %&gt;% \n  nest()\n  \n\nmod_data_logged &lt;- raw_mod_data_logged %&gt;% \n  mutate(MA_mod = \n           map(data, \n               ~fit_MA_mv(.x, mean_log, std.error_log, \"yi\")))\n\n\nplot_data_logged &lt;- mod_data_logged %&gt;% \n  mutate(tidy_mod = map(.x = MA_mod,\n                        ~broom::tidy(.x,\n                                     conf.int = TRUE, \n                                     include_studies = TRUE) %&gt;% \n                          rename(study_id = term))) %&gt;% \n  select(tidy_mod) %&gt;% \n  unnest(cols = c(tidy_mod)) \n\nplot_data_logged %&gt;% \n  mutate(response_scale = map2(estimate, std.error, log_back, 1000)) %&gt;% \n  select(estimate_type, study_id, type, response_scale) %&gt;% \n  unnest(response_scale) %&gt;% \n  rename(estimate = mean_origin, conf.low = lower, conf.high = upper) %&gt;% \n#  filter(estimate &lt;1000) %&gt;% \n  plot_forest_2(MA_mean = T,y_zoom = c(0,140))\n\n\n\n\n\n\n\n\nFigure B.5: Forest plot of meta-analytic estimated out of sample predictions, \\(y_{i}\\), on the response-scale (stems counts), for Eucalyptus analyses. Circles represent individual analysis estimates. Triangles represent the meta-analytic mean for each prediction scenario. Navy blue coloured points correspond to \\(y_{25}\\) scenario, blue coloured points correspond to the \\(y_{50}\\) scenario, while light blue points correspond to the \\(y_{75}\\) scenario. Error bars are 95% confidence intervals. Outliers (observations more than 3SD above the mean) have been removed prior to model fitting.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Effect Size Analysis</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#transforming-response-variable-for-model-fitting",
    "href": "supp_mat/SM3_ExplainingDeviation.html#transforming-response-variable-for-model-fitting",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.1 Transforming response variable for model fitting",
    "text": "C.1 Transforming response variable for model fitting\nTo aid in interpreting explanatory models where the response variable has been Box-Cox transformed, we plotted the transformation relationship for each of our analysis datasets (Figure C.1). Note that timetk::step_box_cox() directly optimises the estimation of the transformation parameter, \\(\\lambda\\), using the “Guerrero” method such that \\(\\lambda\\) minimises the coefficient of variation for sub series of a numeric vector (see ?timetk::step_box_cox(), for further details see Dancho and Vaughan (2023)). Consequently, each dataset has its own unique value of the lambda parameter, and therefore a unique transformation relationship.\n\n\nCode\nback_transformed_predictions &lt;- \n  ManyEcoEvo_yi %&gt;% \n  dplyr::mutate(data = \n                  purrr::map(data, \n                             ~ dplyr::filter(.x,\n                                             stringr::str_detect(response_variable_type, \"constructed\",                                                                       negate = TRUE)))) %&gt;% \n  prepare_response_variables_yi(estimate_type = \"yi\",\n                                param_table = ManyEcoEvo:::analysis_data_param_tables) %&gt;% \n  generate_yi_subsets()\n\n\nraw_mod_data_logged &lt;- \n  back_transformed_predictions %&gt;% \n  filter(dataset == \"eucalyptus\") %&gt;%\n  group_by(estimate_type) %&gt;% \n  select(estimate_type, data) %&gt;% \n  unnest(data) %&gt;% \n  rename(study_id = id_col) %&gt;% \n  hoist(params, param_mean = list(\"value\", 1), param_sd = list(\"value\", 2)) %&gt;% \n  rowwise() %&gt;% \n  mutate(exclusion_threshold = param_mean + 3*param_sd) %&gt;% \n  filter(fit &lt; exclusion_threshold) %&gt;% \n  mutate(log_vals = map2(fit, se.fit, log_transform, 1000)) %&gt;% \n  unnest(log_vals) %&gt;%\n  select(study_id, \n         TeamIdentifier,\n         estimate_type, \n         starts_with(\"response_\"), \n         -response_id_S2, \n         ends_with(\"_log\")) %&gt;% \n  group_by(estimate_type) %&gt;% \n  nest()\n\nmod_data_logged &lt;- raw_mod_data_logged %&gt;% \n  mutate(MA_mod = \n           map(data, \n               ~fit_MA_mv(.x, mean_log, std.error_log, \"yi\")))\n\ndeviation_models_yi_euc &lt;- \n  raw_mod_data_logged %&gt;% \n  mutate(dataset = \"eucalyptus\", \n         exclusion_set = \"complete\") %&gt;% \n  select(dataset, estimate_type, exclusion_set, data) %&gt;%  # rearrange cols\n  left_join({ManyEcoEvo_yi %&gt;% \n      mutate(review_dat = map(data, select, id_col, review_data, mixed_model)) %&gt;% \n      select(dataset, review_dat, diversity_data)}, by = \"dataset\") %&gt;% \n  mutate(data = map(data, ~ rename(.x, \n                                   id_col = study_id,\n                                   Z_logged = mean_log,\n                                   VZ_logged = std.error_log)), #required by compute_MA_inputs() and meta_analyse_dataset()\n         data = map2(data, review_dat, left_join, by = \"id_col\"),\n         diversity_data = # this step filters diversity_data according to matches in data, is also applied in prepare_yi\n           map2(.x = diversity_data, \n                .y = data,\n                .f = ~ semi_join(.x, .y, by = \"id_col\") %&gt;% distinct), \n         .keep = \"unused\") %&gt;% #drops cols that aren't mutated\n  ManyEcoEvo::compute_MA_inputs() %&gt;% \n  ManyEcoEvo::meta_analyse_datasets()\n\ntransformation_plot_data &lt;- \nManyEcoEvo_yi_results %&gt;% \n  ungroup %&gt;% \n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\") %&gt;% \n  bind_rows(deviation_models_yi_euc) %&gt;% \n  bind_rows(ManyEcoEvo_results %&gt;%\n              filter(exclusion_set == \"complete\", publishable_subset == \"All\")) %&gt;% \n  select(dataset, estimate_type, effects_analysis) %&gt;% \n  hoist(effects_analysis, \"abs_deviation_score_estimate\",\n                               \"box_cox_abs_deviation_score_estimate\") %&gt;% \n  hoist(effects_analysis, \"lambda\", .simplify = TRUE, .transform = ~unique(.x)) %&gt;% \n  select(-effects_analysis) %&gt;% \n  unnest(cols = c(abs_deviation_score_estimate,\n                  box_cox_abs_deviation_score_estimate)) \n\ntransformation_plot_data %&gt;% \n    ggplot(aes(y = abs_deviation_score_estimate, \n             x = box_cox_abs_deviation_score_estimate)) + \n  geom_point() +\n  facet_grid(dataset~estimate_type, scales = \"free\") +\n  geom_label(aes(x = -Inf, y = Inf, \n                 label = latex2exp::TeX(paste(\"$\\\\lambda =$\", round(lambda, digits = 4)), output = \"character\"), \n                 hjust = -0.2, vjust = 2), \n             size = 4, parse = TRUE) +\n  theme_bw() +\n  xlab(\"Box-Cox transformed absolute deviation score\") +\n  ylab(\"Absolute deviation score\")\n\n\n\n\n\n\n\n\nFigure C.1: Box-Coxtransformed absolute deviation scores plotted against (untransformed) absolute deviation scores.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#sec-convergence-singularity",
    "href": "supp_mat/SM3_ExplainingDeviation.html#sec-convergence-singularity",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.2 Model Convergence and Singularity problems",
    "text": "C.2 Model Convergence and Singularity problems\nDuring model fitting, especially during fitting of models with random effects using lme4 (Bates et al. 2015), some models failed to converge while others were accompanied with console warnings of singular fit. However, the convergence checks from lme4 are known to be too strict (see ?performance::check_convergence() documentation for a discussion of this issue), consequently we checked for model warnings of convergence failure using the check_convergence() function from the performance package (Lüdecke et al. 2021). For all models we double-checked that they did not have singular fit by using performance::check_singularity. Despite passing performance::check_singularity(), parameters::parameters() was unable to properly estimate SE and confidence intervals for the random effects of some models, which suggests singular fit. For all models we also checked whether the SE of random effects could be calculated, and if not, marked these models as being singular. Analyses of singularity and convergence are presented throughout this document under the relevant section-heading for the analysis type and outcome, i.e. effect size (\\(Z_r\\)) or out-of-sample predictions (\\(y_i\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-reviewer-ratings",
    "href": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-reviewer-ratings",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.3 Deviation Scores as explained by Reviewer Ratings",
    "text": "C.3 Deviation Scores as explained by Reviewer Ratings\n\nC.3.1 Effect Sizes \\(Z_r\\)\nFor models of deviation explained by categorical peer ratings, including random effects for both the effect ID and the reviewer ID resulted in models with singular fit for both blue tit and Eucalyptus datasets (Table C.1). For the Eucalyptus dataset, when a random effect was included for Reviewer ID only, the model passed the check with performance::check_singularity(), however, the SD and CI could not be estimated by parameters::model_parameters() with a warning stating this was likely due to singular fit. When fitting models of deviation explained by categorical peer ratings, we consequently included a random effect for Reviewer ID only (See Table 3.6).\nFor models of deviation explained by continuous peer-review ratings, when including both random effects for effect ID and Reviewer ID model fits were singular for both datasets (Table C.1). For the Eucalyptus dataset when including a random effect only for Reviewer ID and dropping the random effect for effect ID, this model passed the performance::check_singularity() check, however, however, the SD and CI could not be estimated by parameters::model_parameters() with a warning stating this was likely due to singular fit. Consequently, for both blue tit and Euclayptus datasets, we fitted and analysed models of deviation explained by continuous peer review ratings with a random effect for Effect ID only (See Table 3.6).\n\n\nCode\nmodel &lt;- linear_reg() %&gt;%\n  set_engine(\"lmer\")\n\nbase_wf &lt;- workflow() %&gt;%\n  add_model(model)\n\nformula_study_id &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, study_id)) %&gt;% \n  add_model(model, formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + (1 | study_id ))\n\nformula_ReviewerId &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, reviewer_id)) %&gt;% \n  add_model(model, \n            formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + (1 | reviewer_id ))\n\nformula_both &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, reviewer_id, study_id)) %&gt;% \n  add_model(model,\n            formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + (1 | study_id) + (1 | reviewer_id))\n\n\n# ---- Create DF for combinatorial model specification ----\n\nmodel_vars &lt;- \n  bind_rows(\n    tidyr::expand_grid(outcome = \"box_cox_abs_deviation_score_estimate\",\n                       fixed_effects = c(\"publishable_as_is\", \n                                         \"rate_analysis\"),\n                       random_intercepts = c(\"study_id\", \n                                             \"reviewer_id\")) %&gt;% \n      rowwise() %&gt;% \n      mutate(random_intercepts = as.list(random_intercepts)),\n    tidyr::expand_grid(outcome = \"box_cox_abs_deviation_score_estimate\",\n                       fixed_effects = c(\"publishable_as_is\", \n                                         \"rate_analysis\"),\n                       random_intercepts = c(\"study_id\", \n                                             \"reviewer_id\")) %&gt;% \n      group_by(outcome, fixed_effects) %&gt;% \n      reframe(random_intercepts = list(random_intercepts))\n  )\n\n# ----- Run all models for all combinations of dataset, exclusion_set, and publishable_subset ----\n# And Extract \n\n\nall_model_fits &lt;- \n  model_vars %&gt;% \n  cross_join(., \n             {ManyEcoEvo_results %&gt;% \n                 select(dataset, \n                        exclusion_set, \n                        estimate_type, \n                        publishable_subset) %&gt;% \n                 filter(expertise_subset == \"All\") %&gt;% \n                 ungroup %&gt;% \n                 select(-expertise_subset)}) %&gt;% \n  left_join(., {  ManyEcoEvo_results %&gt;% \n      select(dataset, \n             exclusion_set, \n             estimate_type, \n             publishable_subset, \n             effects_analysis) %&gt;% \n      filter(expertise_subset == \"All\") %&gt;% \n      ungroup %&gt;% \n      select(-expertise_subset)},\n      by = join_by(dataset, \n                   exclusion_set, \n                   estimate_type,\n                   publishable_subset)) %&gt;% \n  ungroup %&gt;% \n  filter(publishable_subset == \"All\", \n         exclusion_set == \"complete\") %&gt;% \n  mutate(effects_analysis = \n            map(effects_analysis, \n                mutate, \n                weight = importance_weights(1/box_cox_var)),\n         effects_analysis = \n           map(effects_analysis, \n               ~ .x %&gt;% \n                 unnest(review_data) %&gt;% \n                 select(study_id, \n                        starts_with(\"box_cox_abs_dev\"), \n                        RateAnalysis, \n                        PublishableAsIs,\n                        ReviewerId,\n                        box_cox_var,\n                        weight) %&gt;% \n                 janitor::clean_names() %&gt;%\n                 mutate_if(is.character, factor) \n                 ),\n          model_workflows = pmap(.l = list(outcome, \n                                          fixed_effects, \n                                          random_intercepts), \n                                .f = create_model_workflow),\n         fitted_mod_workflow = map2(model_workflows, effects_analysis, poss_fit), #NOT MEANT TO BE TEST DAT\n         fitted_model = map(fitted_mod_workflow, extract_fit_engine),\n         convergence = map_lgl(fitted_model, performance::check_convergence),\n         singularity = map_lgl(fitted_model, performance::check_singularity),\n         params = map(fitted_model, parameters::parameters)\n  ) %&gt;% \n  unnest_wider(random_intercepts, names_sep = \"_\") %&gt;% \n  select(-outcome, \n         -model_workflows, \n         -fitted_mod_workflow, \n         -effects_analysis,\n         estimate_type) %&gt;% \n  replace_na(list(convergence = FALSE, singularity = TRUE)) \n\n\n# If singularity == FALSE and convergence == TRUE, but the model appears here, then that's because\n# the SD and CI's couldn't be estimated by parameters::\n\nZr_singularity_convergence &lt;- \n  all_model_fits %&gt;% \n  left_join({all_model_fits %&gt;% \n      unnest(params) %&gt;% \n      filter(Effects == \"random\") %&gt;% \n      filter(is.infinite(CI_high) | is.na(SE)) %&gt;% \n      distinct(fixed_effects, \n               random_intercepts_1,\n               random_intercepts_2, \n               dataset, \n               estimate_type,\n               convergence, \n               singularity) %&gt;% \n      mutate(SD_calc = FALSE)}) %&gt;% \n  mutate(SD_calc = ifelse(is.na(SD_calc), TRUE, SD_calc)) \n\n# ----- new code showing ALL model fits not just bad fits\n\nZr_singularity_convergence %&gt;% \n select(-fitted_model, -params, -exclusion_set, -publishable_subset, -estimate_type) %&gt;% \n  arrange(dataset,\n          fixed_effects,\n          random_intercepts_1,\n          random_intercepts_2\n          ) %&gt;% \n  mutate(across(starts_with(\"random\"), \n                ~ str_replace_all(.x, \"_\", \" \") %&gt;%\n                  Hmisc::capitalize() %&gt;% \n                  str_replace(\"id\", \"ID\")),\n         dataset = case_when(dataset == \"eucalyptus\" ~ Hmisc::capitalize(dataset), TRUE ~ dataset)) %&gt;% \n  group_by(dataset) %&gt;% \n   gt::gt() %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = scales::alpha(\"red\", 0.6)),\n      cell_text(color = \"white\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"singularity\", rows = singularity == TRUE),\n      cells_body(columns = \"convergence\", rows = convergence == FALSE), #TODO why didn't work here??\n      cells_body(columns = \"SD_calc\", rows = SD_calc == FALSE)\n    )\n  ) %&gt;% \n  gt::text_transform(fn = function(x) ifelse(x == TRUE, \"yes\", \"no\" ),\n                     locations = cells_body(columns = c(\"singularity\", \"convergence\", \"SD_calc\"))) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::cols_label(dataset = \"Dataset\",\n                 fixed_effects = \"Fixed Effect\",\n                 singularity = \"Singular Fit?\",\n                 convergence = \"Model converged?\",\n                 SD_calc = \"Can random effect SD be calculated?\") %&gt;% \n  gt::tab_spanner(label = \"Random Effects\",\n                  columns = gt::starts_with(\"random\")) %&gt;% \n  gt::sub_missing() %&gt;% \n  gt::cols_label_with(columns = gt::starts_with(\"random\"),\n                      fn = function(x) paste0(\"\")) %&gt;% \n  gt::tab_style(locations = \n                  cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                             columns = dataset),\n                style = cell_text(style = \"italic\")) %&gt;% \n  gt::text_transform(fn = function(x) str_replace(x, \"publishable_as_is\", \"Categorical Peer Rating\") %&gt;% \n                       str_replace(., \"rate_analysis\", \"Continuous Peer Rating\"),\n                     locations = cells_body(columns = c(\"fixed_effects\")))\n\n\n\n\nTable C.1: Singularity and convergence checking outcomes for models of deviation in effect-sizes \\(Z_r\\) explained by peer-review ratings for different random effect structures. Problematic checking outcomes are highlighted in red.\n\n\n\n\n\n\n\n\n\nFixed Effect\nRandom Effects\nModel converged?\nSingular Fit?\nCan random effect SD be calculated?\n\n\n\n\n\n\n\n\nblue tit\n\n\nCategorical Peer Rating\nReviewer ID\n—\nyes\nno\nyes\n\n\nCategorical Peer Rating\nStudy ID\nReviewer ID\nyes\nyes\nno\n\n\nCategorical Peer Rating\nStudy ID\n—\nyes\nno\nyes\n\n\nContinuous Peer Rating\nReviewer ID\n—\nyes\nno\nyes\n\n\nContinuous Peer Rating\nStudy ID\nReviewer ID\nno\nyes\nno\n\n\nContinuous Peer Rating\nStudy ID\n—\nyes\nno\nyes\n\n\nEucalyptus\n\n\nCategorical Peer Rating\nReviewer ID\n—\nyes\nno\nno\n\n\nCategorical Peer Rating\nStudy ID\nReviewer ID\nyes\nyes\nno\n\n\nCategorical Peer Rating\nStudy ID\n—\nyes\nno\nyes\n\n\nContinuous Peer Rating\nReviewer ID\n—\nyes\nno\nno\n\n\nContinuous Peer Rating\nStudy ID\nReviewer ID\nyes\nyes\nno\n\n\nContinuous Peer Rating\nStudy ID\n—\nyes\nno\nyes\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_data_logged &lt;- mod_data_logged %&gt;% \n  mutate(tidy_mod = map(.x = MA_mod,\n                        ~broom::tidy(.x,\n                                     conf.int = TRUE, \n                                     include_studies = TRUE) %&gt;% \n                          rename(study_id = term))) %&gt;% \n  select(tidy_mod) %&gt;% \n  unnest(cols = c(tidy_mod)) \n\nMA_yi_summary_stats &lt;- # ALL ON logged RESPONSE SCALE for EUC, standardized response values for BT\n  plot_data_logged %&gt;% \n  mutate(response_scale = map2(estimate, std.error, log_back, 100)) %&gt;% \n  select(estimate_type, study_id, type, response_scale) %&gt;% \n  unnest(response_scale) %&gt;% \n  rename(estimate = mean_origin, conf.low = lower, conf.high = upper) %&gt;% \n  nest(tidy_mod = -estimate_type) %&gt;% \n  mutate(dataset = \"eucalyptus\") %&gt;% \n  bind_rows({\n    ManyEcoEvo_yi_results %&gt;% \n      ungroup() %&gt;% \n      filter(exclusion_set == \"complete\", dataset == \"blue tit\") %&gt;% \n      select(dataset, estimate_type, MA_mod, effects_analysis, -exclusion_set) %&gt;% \n      group_by(estimate_type, dataset) %&gt;% \n      transmute(tidy_mod = map(.x = MA_mod,\n                               ~broom::tidy(.x,\n                                            conf.int = TRUE, \n                                            include_studies = TRUE) %&gt;% \n                                 rename(study_id = term)))\n  }) %&gt;% \n  mutate(MA_mean = map(tidy_mod, filter, type == \"summary\")) %&gt;% \n  hoist(MA_mean, \n        mean = \"estimate\", \n        MA_conf.low = \"conf.low\", \n        MA_conf.high = \"conf.high\") %&gt;% \n  mutate(max_min_est = map(tidy_mod, \n                           ~ filter(.x, type == \"study\") %&gt;%\n                             summarise(max_est = max(estimate),\n                                       min_est = min(estimate)))) %&gt;% \n  mutate(max_min_CI = map(tidy_mod, \n                          ~ filter(.x, type == \"study\") %&gt;%\n                            summarise(max_upper_CI = max(conf.high),\n                                      min_lower_CI = min(conf.low)))) %&gt;% \n  unnest_wider(col = c(max_min_est, max_min_CI)) %&gt;% \n  ungroup %&gt;% \n  rows_update({plot_data_logged %&gt;% #hells yes to this gem of a function!\n      mutate(dataset = \"eucalyptus\") %&gt;% \n      filter(type != \"summary\") %&gt;% \n      nest(tidy_mod = c(-estimate_type, -dataset))}, \n      by = c(\"dataset\", \"estimate_type\")) %&gt;% \n  mutate(no_effect = \n           map_int(tidy_mod, \n                   ~ filter(.x, \n                            estimate &gt;0 & conf.low &lt;= 0 | estimate &lt;0 & conf.high &gt;= 0, \n                            type == \"study\") %&gt;% \n                     nrow() ),\n         pos_sign = \n           map_int(tidy_mod, \n                   ~ filter(.x, estimate &gt;0, conf.low &gt; 0, \n                            type == \"study\") %&gt;% \n                     nrow()),\n         neg_sign = \n           map_int(tidy_mod, \n                   ~ filter(.x, estimate &lt; 0, conf.high &lt; 0, \n                            type == \"study\") %&gt;% \n                     nrow()),\n         total_effects = \n           map_int(tidy_mod,\n                   ~ filter(.x, \n                            type == \"study\") %&gt;% \n                     nrow()\n           )) %&gt;% \n  select(-tidy_mod, -MA_mean) %&gt;% \n  rename(MA_mean = mean)\n\n\n\n\nC.3.2 Out of sample predictions \\(y_i\\)\n\n\nCode\neuc_yi_results &lt;- \n  ManyEcoEvo::make_viz(deviation_models_yi_euc)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\nRandom effect variances not available. Returned R2 does not account for random effects.\nRandom effect variances not available. Returned R2 does not account for random effects.\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\nCode\nyi_convergence_singularity &lt;- \n  ManyEcoEvo_yi_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\",\n         model_name %in% c(\"box_cox_rating_cat\", \n                           \"box_cox_rating_cont\")) %&gt;% \n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %in% \n               c(\"box_cox_rating_cat\", \"box_cox_rating_cont\"))}) %&gt;% \n  mutate(singularity = map_lgl(model, possibly_check_singularity),\n         convergence = map_lgl(model, possibly_check_convergence),\n         params = map(model, parameters::parameters)) %&gt;% \n  select(dataset, estimate_type, model_name, singularity, convergence, params) %&gt;% \n  mutate(model_name = forcats::as_factor(model_name),\n         model_name = forcats::fct_relevel(model_name, \n                                           c(\"box_cox_rating_cat\", \n                                             \"box_cox_rating_cont\")),\n         model_name = forcats::fct_recode(model_name,\n                                          `Deviation explained by categorical ratings` = \"box_cox_rating_cat\",\n                                          `Deviation explained by continuous ratings` = \"box_cox_rating_cont\"),\n         dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                             TRUE ~ dataset)) %&gt;% \n  hoist(params, SD_calc = \"SE\",.remove = FALSE) %&gt;% \n  mutate(SD_calc = map_lgl(SD_calc, ~ is.na(.x) %&gt;% any(.) %&gt;% isFALSE(.)))\n\nyi_singularity_convergence_sorensen_mixed_mod &lt;- \n  ManyEcoEvo_yi_viz %&gt;% \n  bind_rows(euc_yi_results) %&gt;% \n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\",\n         model_name %in% c(\"sorensen_glm\")) %&gt;%\n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %in% c(\"sorensen_glm\",\n                               \"uni_mixed_effects\"))}) %&gt;% \n  mutate(singularity = \n           map_lgl(model, possibly_check_singularity),\n         convergence = \n           map_lgl(model, possibly_check_convergence_glm),\n         params = map(model, parameters::parameters),\n         dataset = \n           case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                             TRUE ~ dataset),\n         model_name = \n           forcats::as_factor(model_name),\n         model_name = \n           forcats::fct_relevel(model_name,\n                                c(\"sorensen_glm\",\"uni_mixed_effects\")),\n         model_name = forcats::fct_recode(model_name,\n                                          `Deviation explained by Sorensen's index` =  \"sorensen_glm\",\n                                          `Deviation explained by inclusion of random effects` =  \"uni_mixed_effects\")) %&gt;% \n  select(dataset, \n         estimate_type, \n         model_name, \n         singularity, \n         convergence,\n         params) %&gt;% \n  group_by(model_name) \n\n\nWe fitted the same deviation models on the yi dataset that we fitted for the Zr dataset. However, while all models converged, models of deviation explained by categorical peer-ratings suffered from singular fit for the following datasets and estimate types: blue tit - y25, Eucalyptus - y25, Eucalyptus - y75 (Table C.2). Results are therefore presented only for models with non-singular fit, converging for the following datasets and estimate types: blue tit - y50, blue tit - y75, Eucalyptus - y50 (Table C.2).\n\n\nCode\nyi_convergence_singularity %&gt;% \n  select(-params) %&gt;% \n  group_by(model_name) %&gt;% \n  gt::gt(rowname_col = \"dataset\") %&gt;% \n   gt::tab_style(locations = cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                                       columns = dataset),\n                style = cell_text(style = \"italic\")) %&gt;% \n  gt::cols_label(dataset = \"Dataset\",\n                 estimate_type = \"Estimate Type\",\n                 singularity = \"Singular Fit?\",\n                 convergence = \"Model converged?\",\n                 SD_calc = \"Can random effect SE be calculated?\") %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::text_transform(fn = function(x) ifelse(x == TRUE, \"yes\", \"no\" ),\n                     locations = cells_body(columns = c(\"singularity\",\n                                                        \"convergence\",\n                                                        \"SD_calc\")\n                                            )) %&gt;% \n  gt::text_transform(\n    locations = cells_stub(\n      rows = estimate_type != \"y25\"\n    ),\n    fn = function(x){\n      paste0(\"\")\n    }\n  ) %&gt;% \n  gt::tab_style(locations = cells_stub(rows = str_detect(dataset, \"Eucalyptus\")),\n                style = cell_text(style = \"italic\")) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = scales::alpha(\"red\", 0.6)),\n      cell_text(color = \"white\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"singularity\", rows = singularity == TRUE),\n      cells_body(columns = \"convergence\", rows = convergence == FALSE),\n      cells_body(columns = \"SD_calc\", rows = SD_calc == FALSE)\n    )\n  ) \n\n\n\n\nTable C.2: Singularity and convergence checking for models of deviation in out-of-sample-predictions \\(y_i\\) explained by peer-ratings.\n\n\n\n\n\n\n\n\n\n\nEstimate Type\nSingular Fit?\nModel converged?\nCan random effect SE be calculated?\n\n\n\n\nDeviation explained by continuous ratings\n\n\nblue tit\ny25\nno\nyes\nyes\n\n\n\ny50\nno\nyes\nyes\n\n\n\ny75\nno\nyes\nyes\n\n\nEucalyptus\ny25\nno\nyes\nyes\n\n\n\ny50\nno\nyes\nyes\n\n\n\ny75\nno\nyes\nyes\n\n\nDeviation explained by categorical ratings\n\n\nblue tit\ny25\nyes\nyes\nno\n\n\n\ny50\nno\nyes\nyes\n\n\n\ny75\nno\nyes\nyes\n\n\nEucalyptus\ny25\nyes\nyes\nno\n\n\n\ny50\nno\nyes\nyes\n\n\n\ny75\nyes\nyes\nno\n\n\n\n\n\n\n\n\n\n\nGroup means and \\(95\\%\\) confidence intervals for different categories of peer-review rating are all overlapping (Figure C.2). The fixed effect of peer review rating also explains virtually no variability in \\(y_i\\) deviation score (Table C.2).\n\n\nCode\n# Omit all singular models\nyi_violin_cat_plot_data &lt;- \n  ManyEcoEvo_yi_viz %&gt;% \n  filter(exclusion_set == \"complete\", #TODO NEED TO PULL OUT LAMBDA!\n         dataset == \"blue tit\",\n         model_name %in% c(\"box_cox_rating_cat\")) %&gt;% \n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %in% c(\"box_cox_rating_cat\"))}) %&gt;% \n  mutate( dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                              TRUE ~ dataset)) %&gt;% \n  semi_join({yi_convergence_singularity %&gt;% # filter out singular models #TODO rm mods with NA/Inf/0 random effect SE\n      filter(singularity == FALSE, \n             str_detect(model_name, \"categorical\")) }, \n      by = join_by(\"dataset\", \"estimate_type\")) %&gt;% \n  select(dataset:model, -exclusion_set, -model_name) %&gt;% \n  mutate(predictor_means = \n           map(model, modelbased::estimate_means),\n         model_data = map(model, ~pluck(.x, \"frame\") %&gt;% \n                            drop_na() %&gt;% \n                            as_tibble()),\n         plot_name = paste(dataset, \n                           estimate_type,\n                           \"violin_cat\",\n                           sep = \"_\")) %&gt;% \n     mutate(model_data = map(model_data, \n                          .f = ~ mutate(.x, PublishableAsIs =\n                           str_replace(PublishableAsIs,\n                                       \"publishable with \", \"\") %&gt;%\n                           str_replace(\"deeply flawed and \", \"\") %&gt;% \n                           capwords())),\n         predictor_means = map(predictor_means,\n           .f = ~ mutate(.x, PublishableAsIs =\n                           str_replace(PublishableAsIs,\n                                       \"publishable with \", \"\") %&gt;%\n                           str_replace(\"deeply flawed and \", \"\") %&gt;% \n                           capwords()))) \n\nyi_violin_cat_plot_data %&gt;% \n  pwalk(.l = list(.$model_data, .$predictor_means, .$plot_name),\n       .f = ~ plot_model_means_box_cox_cat(..1, \n                                           PublishableAsIs, \n                                           ..2,\n                                           new_order = \n                                            c(\"Unpublishable\",\n                                               \"Major Revision\",\n                                               \"Minor Revision\",\n                                               \"Publishable As Is\"),\n                                           ..3) %&gt;% \n         assign(x = ..3, value = ., envir = .GlobalEnv))\n\nlibrary(patchwork)\n`blue tit_y50_violin_cat` /\n  `blue tit_y75_violin_cat` /\n     Eucalyptus_y50_violin_cat /\n  patchwork::plot_layout(guides = 'collect') +\n  patchwork::plot_annotation(tag_levels = 'A') &\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nFigure C.2: Violin plot of Box-Cox transformed deviation from meta-analytic mean as a function of categorical peer-review rating. Grey points for each rating group denote model-estimated marginal mean deviation, and error bars denote 95% CI of the estimate. A blue tit dataset, \\(y_{50}\\) B blue tit dataset, \\(y_{75}\\) C Eucalyptus dataset, \\(y_{50}\\).\n\n\n\n\n\nModels of deviation explained by continuous ratinsg all converged, however models for the y25 out-of-sample predictions were singular for both Eucalyptus and blue tit datasets.\nThere was a lack of any clear relationships between quantitative review scores and \\(y_i\\) deviation scores (Table C.10). Plots of these relationships indicated either no relationship or extremely weak positive relationships (Figure C.3). Recall that positive relationships mean that as review scores became more favorable, the deviation from the meta-analytic mean increased, which is surprising. Because almost no variability in \\(y_i\\) deviation score was explained by reviewer ratings (Table C.10), this pattern does not appear to merit further consideration.\n\n\nCode\n# Omit all singular models\nyi_cont_plot_data &lt;-\n  ManyEcoEvo_yi_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\",\n         model_name %in% c(\"box_cox_rating_cont\")) %&gt;% \n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %in% c(\"box_cox_rating_cont\"))}) %&gt;% \n  mutate( dataset = case_when(str_detect(dataset,\n                                         \"eucalyptus\") ~\n                                \"Eucalyptus\",\n                              TRUE ~ dataset)) %&gt;% \n  semi_join({yi_convergence_singularity %&gt;% \n      filter(singularity == FALSE, SD_calc == TRUE, \n             str_detect(model_name, \"cont\")) }, \n      by = join_by(\"dataset\", \"estimate_type\")) %&gt;% \n  select(dataset:model, -exclusion_set, -model_name) %&gt;% \n  mutate(plot_data = map(model, pluck, \"frame\")) \n\nsubfigcaps &lt;- yi_cont_plot_data %&gt;% \n  mutate(dataset = \n           case_when(dataset == \"Eucalyptus\" ~ paste0(\"*\", dataset, \"*\"), \n                     TRUE ~ Hmisc::capitalize(dataset))) %&gt;%  \n  unite(plot_name, dataset, estimate_type, sep = \", \") %&gt;% \n  pull(plot_name)\n\nfig_cap_yi_deviation_cont_rating &lt;- \n  paste0(\"Scatterplots exaining Box-Cox transfored deviation fro the eta-analytic mean for $y_i$ estimates as a function of continuous ratings. Note that higher (less negative) values of the deviation score result from greater deviation from the meta-analytic mean.\", subfigcaps %&gt;% \n           paste0(paste0(paste0(\"**\", LETTERS[1:4], \"**\", sep = \"\"), sep = \": \"), ., collapse = \", \"), \".\")\n\n\n\nCode\nyi_cont_plots &lt;- \n  yi_cont_plot_data$plot_data %&gt;% \n  map(.f =  ~ plot_continuous_rating(.x))\n\npatchwork::wrap_plots(yi_cont_plots,heights = 4, byrow = TRUE) +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\n\n\nFigure C.3: Scatterplots exaining Box-Cox transfored deviation fro the eta-analytic mean for \\(y_i\\) estimates as a function of continuous ratings. Note that higher (less negative) values of the deviation score result from greater deviation from the meta-analytic mean.A: Blue tit, y25, B: Blue tit, y50, C: Blue tit, y75, D: Eucalyptus, y25, A: Eucalyptus, y50, B: Eucalyptus, y75.\n\n\n\n\n\n\n\n\nCode\nManyEcoEvo_yi_viz %&gt;%\n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\",\n         model_name %nin% c(\"MA_mod\", \"box_cox_rating_cat_no_int\")) %&gt;% \n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %nin% c(\"MA_mod\", \"box_cox_rating_cat_no_int\"))}) %&gt;% \n  mutate( dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                              TRUE ~ dataset),\n          model_name = forcats::as_factor(model_name) %&gt;% \n           forcats::fct_relevel(c(\"box_cox_rating_cat\", \n                                  \"box_cox_rating_cont\", \n                                  \"sorensen_glm\", \n                                  \"uni_mixed_effects\")) %&gt;% \n           forcats::fct_recode(`Deviation explained by categorical ratings` = \"box_cox_rating_cat\",\n                               `Deviation explained by continuous ratings` = \"box_cox_rating_cont\",\n                               `Deviation explained by Sorensen's index` =  \"sorensen_glm\",\n                               `Deviation explained by inclusion of random effects` =  \"uni_mixed_effects\")) %&gt;% \n  semi_join(\n    {bind_rows(yi_singularity_convergence_sorensen_mixed_mod, \n               yi_convergence_singularity) %&gt;% \n        filter(singularity == FALSE, convergence == TRUE, SD_calc == TRUE) },\n    by = join_by(\"dataset\", \"estimate_type\", \"model_name\")\n  ) %&gt;% \n  select(dataset, estimate_type, model_name, model) %&gt;% \n  mutate(tbl_output = map(model, parameters::parameters)\n  ) %&gt;% \n  select(dataset, \n         estimate_type,\n         model_name, \n         tbl_output) %&gt;% \n  unnest(tbl_output) %&gt;% \n  mutate(dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"*Eucalyptus*\",\n                             TRUE ~ dataset),\n         Group = case_when(Group == \"study_id\" ~ \"Effect ID\",\n                           Group == \"ReviewerId\" ~ \"Reviewer ID\",\n                           TRUE ~ Group),\n         df_error = as.integer(df_error),\n         Parameter =  str_remove(Parameter, \"PublishableAsIs\") %&gt;% \n           str_replace(\"diversity\", \"Sorensen's\") %&gt;% \n           str_replace_all(., \"_\", \" \") %&gt;%\n           str_remove(., \"1\") %&gt;% \n           Hmisc::capitalize() ) %&gt;%\n  group_by(model_name) %&gt;% \n  arrange(desc(model_name), \n          dataset, estimate_type) %&gt;%\n  select(-CI) %&gt;% \n    dplyr::filter(dataset != \"blue tit\" | str_detect(model_name, \"random\", negate = TRUE)) %&gt;% \n   gt::gt(rowname_col = \"dataset\") %&gt;% \n  gt::fmt(columns = \"p\",\n          fns = function(x) gtsummary::style_pvalue(x)) %&gt;% \n  gt::cols_label(CI_low = gt::md(\"95\\\\%CI\"),\n                 estimate_type = \"Estimate Type\") %&gt;% \n  gt::cols_label(df_error = \"df\") %&gt;% \n  gt::cols_merge(columns = starts_with(\"CI_\"), \n                 pattern = \"[{1},{2}]\") %&gt;% \n  gt::cols_move(columns = CI_low, after = SE) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::fmt(columns = c(Coefficient, SE, starts_with(\"CI_\"), t) ,\n          rows = Parameter %nin% c(\"RateAnalysis\", \"SD (Observations)\", \"mixed_model1\"),\n          fns = function(x) format(round(x, 2),nsmall = 2)) %&gt;%\n  gt::fmt(columns = c(Coefficient, SE, t, starts_with(\"CI_\")) ,\n          rows = Parameter %in% c(\"RateAnalysis\", \"SD (Observations)\", \"mixed_model1\"),\n          fns = function(x) ifelse(x &lt; 0.0009, \n                                   format(x, nsmall = 2, digits = 1),\n                                   round(x, digits = 2))) %&gt;%\n  gt::cols_move(columns = c(Effects, Group), after = Parameter) %&gt;% \n  gt::sub_missing(columns = c(Effects, Group, t, df_error, p), \n                  missing_text = \"\") %&gt;% \n  gt::text_transform(fn = function(x) map(x, gt::md), \n                     locations = gt::cells_row_groups()) %&gt;% \n   gt::text_transform(\n    locations = cells_stub(\n      rows = Parameter != \"(Intercept)\"\n    ),\n    fn = function(x){\n      paste0(\"\")\n    }\n  ) %&gt;% \n  gt::fmt(columns = c(Coefficient, SE, t, starts_with(\"CI_\")) ,\n          # rows = Parameter %in% c(\"RateAnalysis\", \"SD (Observations)\", \"mixed_model1\"),\n          fns = function(x) ifelse(x &lt; 0.0009, \n                                   format(x, nsmall = 2, digits = 1),\n                                   round(x, digits = 2))) %&gt;% \n    gt::tab_style(locations = gt::cells_stub(rows = str_detect(dataset, \"Eucalyptus\")),\n                style = cell_text(style = \"italic\")) %&gt;% \n  gt::as_raw_html()\n\n\n\n\nTable C.3: Parameter estimates for univariate models of Box-Cox transformed deviation from the mean \\(y_i\\) estimate as a function of categorical peer-review rating, continuous peer-review rating, and Sorensen’s index for blue tit and Eucalyptus analyses, and also for the inclusion of random effects for Eucalyptus analyses.\n\n\n\n\n  \n  \n\n\n\n\nEstimate Type\nParameter\nEffects\nGroup\nCoefficient\nSE\n95%CI\nt\ndf\np\n\n\n\n\nDeviation explained by continuous ratings\n\n\nEucalyptus\ny25\n(Intercept)\nfixed\n\n0.11\n0.12\n[-1e-01,0.35]\n0.96\n98\n0.3\n\n\n\ny25\nRateAnalysis\nfixed\n\n2e-13\n3e-08\n[-5e-08, 5e-08]\n8e-06\n98\n&gt;0.9\n\n\n\ny25\nSD (Intercept)\nrandom\nEffect ID\n0.55\n0.09\n[0.41,0.75]\n\n\n\n\n\n\ny25\nSD (Observations)\nrandom\nResidual\n2e-05\n2e-06\n[ 2e-05, 3e-05]\n\n\n\n\n\nEucalyptus\ny50\n(Intercept)\nfixed\n\n-3e-01\n0.01\n[-4e-01,-3e-01]\n-3e+01\n105\n&lt;0.001\n\n\n\ny50\nRateAnalysis\nfixed\n\n9e-18\n2e-11\n[-3e-11, 3e-11]\n5e-07\n105\n&gt;0.9\n\n\n\ny50\nSD (Intercept)\nrandom\nEffect ID\n0.6\n0.09\n[0.45,0.8]\n\n\n\n\n\n\ny50\nSD (Observations)\nrandom\nResidual\n1e-04\n8e-06\n[ 9e-05, 1e-04]\n\n\n\n\n\nEucalyptus\ny75\n(Intercept)\nfixed\n\n-3e-01\n0.04\n[-4e-01,-3e-01]\n-8e+00\n105\n&lt;0.001\n\n\n\ny75\nRateAnalysis\nfixed\n\n-1e-17\n6e-11\n[-1e-10, 1e-10]\n-2e-07\n105\n&gt;0.9\n\n\n\ny75\nSD (Intercept)\nrandom\nEffect ID\n0.51\n0.07\n[0.38,0.68]\n\n\n\n\n\n\ny75\nSD (Observations)\nrandom\nResidual\n4e-07\n3e-08\n[ 3e-07, 5e-07]\n\n\n\n\n\nblue tit\ny25\n(Intercept)\nfixed\n\n-1e+00\n0.04\n[-1e+00,-1e+00]\n-3e+01\n230\n&lt;0.001\n\n\n\ny25\nRateAnalysis\nfixed\n\n2e-16\n2e-10\n[-4e-10, 4e-10]\n1e-06\n230\n&gt;0.9\n\n\n\ny25\nSD (Intercept)\nrandom\nEffect ID\n0.32\n0.03\n[0.27,0.38]\n\n\n\n\n\n\ny25\nSD (Observations)\nrandom\nResidual\n2e-07\n1e-08\n[ 2e-07, 3e-07]\n\n\n\n\n\nblue tit\ny50\n(Intercept)\nfixed\n\n-2e+00\n0.06\n[-2e+00,-2e+00]\n-3e+01\n217\n&lt;0.001\n\n\n\ny50\nRateAnalysis\nfixed\n\n1e-15\n3e-10\n[-5e-10, 5e-10]\n4e-06\n217\n&gt;0.9\n\n\n\ny50\nSD (Intercept)\nrandom\nEffect ID\n0.4\n0.04\n[0.33,0.48]\n\n\n\n\n\n\ny50\nSD (Observations)\nrandom\nResidual\n8e-07\n4e-08\n[ 7e-07, 9e-07]\n\n\n\n\n\nblue tit\ny75\n(Intercept)\nfixed\n\n-1e+00\n0.04\n[-1e+00,-1e+00]\n-3e+01\n230\n&lt;0.001\n\n\n\ny75\nRateAnalysis\nfixed\n\n6e-14\n9e-09\n[-2e-08, 2e-08]\n7e-06\n230\n&gt;0.9\n\n\n\ny75\nSD (Intercept)\nrandom\nEffect ID\n0.33\n0.03\n[0.28,0.4]\n\n\n\n\n\n\ny75\nSD (Observations)\nrandom\nResidual\n1e-05\n6e-07\n[ 1e-05, 1e-05]\n\n\n\n\n\nDeviation explained by categorical ratings\n\n\nEucalyptus\ny50\n(Intercept)\nfixed\n\n-1e+00\n0.52\n[-2e+00,-2e-01]\n-2e+00\n103\n0.024\n\n\n\ny50\nPublishable with major revision\nfixed\n\n0.65\n0.57\n[-5e-01,1.78]\n1.14\n103\n0.3\n\n\n\ny50\nPublishable with minor revision\nfixed\n\n0.79\n0.55\n[-3e-01,1.88]\n1.44\n103\n0.2\n\n\n\ny50\nPublishable as is\nfixed\n\n1.3\n0.62\n[0.07,2.53]\n2.09\n103\n0.039\n\n\n\ny50\nSD (Intercept)\nrandom\nReviewer ID\n0.04\n1.88\n[ 6e-39,3.12801839703883e+35]\n\n\n\n\n\n\ny50\nSD (Observations)\nrandom\nResidual\n1.37\n0.11\n[1.17,1.6]\n\n\n\n\n\nblue tit\ny50\n(Intercept)\nfixed\n\n-1e+00\n0.29\n[-2e+00,-6e-01]\n-4e+00\n215\n&lt;0.001\n\n\n\ny50\nPublishable with major revision\nfixed\n\n-2e-01\n0.31\n[-8e-01,0.38]\n-7e-01\n215\n0.5\n\n\n\ny50\nPublishable with minor revision\nfixed\n\n-2e-01\n0.3\n[-8e-01,0.35]\n-8e-01\n215\n0.4\n\n\n\ny50\nPublishable as is\nfixed\n\n-4e-01\n0.32\n[-1e+00,0.21]\n-1e+00\n215\n0.2\n\n\n\ny50\nSD (Intercept)\nrandom\nReviewer ID\n0.23\n0.08\n[0.11,0.47]\n\n\n\n\n\n\ny50\nSD (Observations)\nrandom\nResidual\n0.71\n0.04\n[0.64,0.79]\n\n\n\n\n\nblue tit\ny75\n(Intercept)\nfixed\n\n-1e+00\n0.25\n[-2e+00,-9e-01]\n-6e+00\n228\n&lt;0.001\n\n\n\ny75\nPublishable with major revision\nfixed\n\n0.08\n0.25\n[-4e-01,0.59]\n0.33\n228\n0.7\n\n\n\ny75\nPublishable with minor revision\nfixed\n\n0.33\n0.25\n[-2e-01,0.83]\n1.28\n228\n0.2\n\n\n\ny75\nPublishable as is\nfixed\n\n0.36\n0.27\n[-2e-01,0.88]\n1.34\n228\n0.2\n\n\n\ny75\nSD (Intercept)\nrandom\nReviewer ID\n0.24\n0.06\n[0.15,0.39]\n\n\n\n\n\n\ny75\nSD (Observations)\nrandom\nResidual\n0.58\n0.03\n[0.52,0.64]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-the-distinctiveness-of-variables-in-each-analysis",
    "href": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-the-distinctiveness-of-variables-in-each-analysis",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.4 Deviation scores as explained by the distinctiveness of variables in each analysis",
    "text": "C.4 Deviation scores as explained by the distinctiveness of variables in each analysis\n\nC.4.1 Effect Sizes \\(Z_r\\)\n\n\nC.4.2 Out of sample predictions \\(y_i\\)\nGiven the convergence and singularity issues encountered with most other analyses, we also checked for convergence and singularity issues in models of deviation explained by Sorensen’s similarity index for \\(y_i\\) estimates (Table C.4). All models fitted without problem.\n\n\nCode\nyi_sorensen_plot_data &lt;- \n  ManyEcoEvo_yi_viz %&gt;% \n  filter(exclusion_set == \"complete\", \n         dataset == \"blue tit\",\n         str_detect(model_name, \"sorensen_glm\")) %&gt;% \n  bind_rows({euc_yi_results %&gt;% \n      filter(model_name %in% c(\"sorensen_glm\"))}) %&gt;% \n  mutate( dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                              TRUE ~ dataset)) %&gt;% \n  select(dataset, estimate_type, model_name, model) %&gt;% \n  semi_join(\n    {yi_singularity_convergence_sorensen_mixed_mod %&gt;% \n        filter(str_detect(model_name, \"Sorensen\"), \n               singularity == FALSE)},\n    by = join_by(\"dataset\", \"estimate_type\")\n  ) %&gt;% \n  mutate(dataset = case_when(dataset == \"Eucalyptus\" ~ paste0(\"*\", dataset, \"*\"),\n                             TRUE ~ Hmisc::capitalize(dataset)),\n         plot_data = map(model, ~ pluck(.x, \"fit\", \"data\") %&gt;% \n                           rename(box_cox_abs_deviation_score_estimate = ..y))) %&gt;% \n  unite(plot_names, dataset, estimate_type, sep = \", \")\n\nyi_sorensen_subfigcaps &lt;- \n  yi_sorensen_plot_data$plot_names %&gt;% \n  paste0(paste0(paste0(\"**\", LETTERS[1:4], \"**\", sep = \"\"), sep = \": \"), ., collapse = \", \")\n\nyi_sorensen_fig_cap &lt;- paste0(\"Scatter plots examining Box-Cox transformed deviation from the meta-analytic mean for $y_i$ estimates as a function of Sorensen's similarity index. Note that higher (less negative) values of the deviation score result from greater deviation from the meta-analytic mean. \",\n                              yi_sorensen_plot_data$plot_names %&gt;% paste0(paste0(paste0(\"**\", LETTERS[1:6], \"**\", sep = \"\"), sep = \": \"), ., collapse = \", \"),\n                              \".\")\n\n\n\nCode\nyi_sorensen_plots &lt;- \n  map2(.x = yi_sorensen_plot_data$model, \n       .y = yi_sorensen_plot_data$plot_data,\n       .f = ~ walk_plot_effects_diversity(model = .x, plot_data = .y))\n\npatchwork::wrap_plots(yi_sorensen_plots,heights = 4, byrow = TRUE) +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\n\n\nFigure C.4: Scatter plots examining Box-Cox transformed deviation from the meta-analytic mean for \\(y_i\\) estimates as a function of Sorensen’s similarity index. Note that higher (less negative) values of the deviation score result from greater deviation from the meta-analytic mean. A: Blue tit, y25, B: Blue tit, y50, C: Blue tit, y75, D: Eucalyptus, y25, E: Eucalyptus, y50, F: Eucalyptus, y75.\n\n\n\n\n\n\n\n\nCode\nyi_singularity_convergence_sorensen_mixed_mod %&gt;% \n filter(dataset != \"blue tit\" | str_detect(model_name, \n                                            \"random\", \n                                            negate = TRUE)) %&gt;% \n  select(-params) %&gt;% \n  gt::gt(rowname_col = \"dataset\") %&gt;% \n  gt::tab_style(locations = cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                                       columns = dataset),\n                style = cell_text(style = \"italic\")) %&gt;% \n  gt::cols_label(dataset = \"Dataset\",\n                 estimate_type = \"Estimate Type\",\n                 singularity = \"Singular Fit?\",\n                 convergence = \"Model converged?\") %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::text_transform(fn = function(x) ifelse(x == TRUE, \"yes\", \"no\" ),\n                     locations = cells_body(columns = c(\"singularity\",\n                                                        \"convergence\")\n                                            )) %&gt;% \n  gt::text_transform(\n    locations = cells_stub(\n      rows = estimate_type != \"y25\"\n    ),\n    fn = function(x){\n      paste0(\"\")\n    }\n  ) %&gt;% \n  gt::tab_style(locations = cells_stub(rows = str_detect(dataset, \"Eucalyptus\")),\n                style = cell_text(style = \"italic\")) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = scales::alpha(\"red\", 0.6)),\n      cell_text(color = \"white\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"singularity\", rows = singularity == TRUE),\n      cells_body(columns = \"convergence\", rows = convergence == FALSE)\n    )\n  ) \n\n\n\n\nTable C.4: Singularity and convergence checks for models of deviation explained by Sorensen’s similarity index and inclusion of random effects for out-of-sample predictions, \\(y_i\\). Models of Deviation explained by inclusion of random effects are not presented for blue tit analyses because the number of models not using random effects was less than our preregistered threshold.\n\n\n\n\n\n\n\n\n\n\nEstimate Type\nSingular Fit?\nModel converged?\n\n\n\n\nDeviation explained by Sorensen's index\n\n\nblue tit\ny25\nno\nyes\n\n\n\ny50\nno\nyes\n\n\n\ny75\nno\nyes\n\n\nEucalyptus\ny25\nno\nyes\n\n\n\ny50\nno\nyes\n\n\n\ny75\nno\nyes\n\n\nDeviation explained by inclusion of random effects\n\n\nEucalyptus\ny25\nno\nyes\n\n\n\ny50\nno\nyes\n\n\n\ny75\nno\nyes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-the-inclusion-of-random-effects",
    "href": "supp_mat/SM3_ExplainingDeviation.html#deviation-scores-as-explained-by-the-inclusion-of-random-effects",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.5 Deviation scores as explained by the inclusion of random effects",
    "text": "C.5 Deviation scores as explained by the inclusion of random effects\n\nC.5.1 Out of sample predictions \\(y_i\\)\nThere were only three blue tit analyses that did not include random effects, which is below the pre-registered threshold for fitting a model of the Box-Cox transformed deviation from the meta-analytic mean as a function of whether the analysis included random-effects. However, 16 Eucalyptus analyses included in the out-of-sample (\\(y_{i}\\)) results included only fixed effects, which crossed our pre-registered threshold.\nConsequently, we performed this analysis for the Eucalyptus dataset only, here we present results for the out of sample prediction \\(y_{i}\\) results. There is consistent evidence of somewhat higher Box-Cox-transformed deviation values for models including a random effect, meaning the models including random effects averaged slightly higher deviation from the meta-analytic means. This is most evident for the \\(y_{50}\\) predictions which both shows the greatest difference in Box-Cox transformed deviation values (Figure C.5) and explains the most variation in \\(y_i\\) deviation score (Table C.10, Table C.10).\n\n\nCode\nyi_deviation_RE_plot_data &lt;- \n  euc_yi_results %&gt;% \n  filter(str_detect(model_name, \"uni_mixed_effects\")) %&gt;%\n  select(dataset, estimate_type, model) %&gt;% \n  mutate(predictor_means = map(model, .f = ~ pluck(.x, \"fit\") %&gt;% \n                                 modelbased::estimate_means(.)),\n         plot_data = map(model, pluck, \"fit\", \"data\"),\n         plot_data = map(plot_data, \n                         rename, \n                         box_cox_abs_deviation_score_estimate = ..y)) %&gt;% \n  mutate(dataset = Hmisc::capitalize(dataset) %&gt;% paste0(\"*\", ., \"*\")) %&gt;%\n  unite(plot_names, dataset, estimate_type, sep = \", \")\n\nyi_deviation_RE_plot_subfigcaps &lt;- yi_deviation_RE_plot_data %&gt;% \n  pull(plot_names)\n\nyi_deviation_RE_plot_figcap &lt;- \n  paste0(\"Violin plot of Box-Cox transformed deviation from meta-analytic mean as a function of presence or absence of random effects in the analyst's model. White points for each rating group denote model-estimated marginal mean deviation, and error bars denote 95% CI of the estimate. Note that higher (less negative) values of Box-Cox transformed deviation result from greater deviation from the meta-analytic mean. \",\n         yi_deviation_RE_plot_data %&gt;% \n           pull(plot_names) %&gt;% \n           paste0(paste0(paste0(\"**\", LETTERS[1:nrow(yi_deviation_RE_plot_data)], \"**\", sep = \"\"), sep = \": \"), ., collapse = \", \"),\n         \".\")\n\n\n\nCode\nyi_deviation_RE_plots &lt;- \n  yi_deviation_RE_plot_data %&gt;% \n  map2(.x = .$plot_data, .y = .$predictor_means, \n       .f = ~ plot_model_means_RE(.x, mixed_model, .y))\n\npatchwork::wrap_plots(yi_deviation_RE_plots, byrow = TRUE) +\n  patchwork::plot_annotation(tag_levels = 'A') +\n  patchwork::plot_layout(guides = 'collect') &\n  theme(legend.position = \"bottom\", axis.ticks = element_blank()) &\n  xlab(NULL)\n\n\n\n\n\n\n\n\n\n\nFigure C.5: Violin plot of Box-Cox transformed deviation from meta-analytic mean as a function of presence or absence of random effects in the analyst’s model. White points for each rating group denote model-estimated marginal mean deviation, and error bars denote 95% CI of the estimate. Note that higher (less negative) values of Box-Cox transformed deviation result from greater deviation from the meta-analytic mean. A: Eucalyptus, y25, B: Eucalyptus, y50, C: Eucalyptus, y75.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#multivariate-analysis",
    "href": "supp_mat/SM3_ExplainingDeviation.html#multivariate-analysis",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.6 Multivariate Analysis",
    "text": "C.6 Multivariate Analysis\n\nC.6.1 Effect Sizes \\(Z_r\\)\n\n\nCode\n# lmer(box_cox_abs_deviation_score_estimate ~ RateAnalysis + PublishableAsIs + mean_diversity_index + (1|study_id) + (1|ReviewerId), data = ManyEcoEvo_results$effects_analysis[[1]] %&gt;% unnest(review_data)) \n\nbt_multivar_mod &lt;- lmer(box_cox_abs_deviation_score_estimate ~ RateAnalysis + PublishableAsIs + mean_diversity_index + mixed_model + (1|ReviewerId), \n     data = ManyEcoEvo_results$effects_analysis[[1]] %&gt;% \n       unnest(review_data))\n\neuc_multivar_mod &lt;- \nlmer(box_cox_abs_deviation_score_estimate ~ RateAnalysis + PublishableAsIs + mean_diversity_index + mixed_model + (1|ReviewerId), \n     data = ManyEcoEvo_results$effects_analysis[[2]] %&gt;% \n       unnest(review_data))\n\nbt_multivar_mod_R &lt;- bt_multivar_mod %&gt;% MuMIn::r.squaredGLMM()\neuc_multivar_mod_R &lt;- euc_multivar_mod %&gt;% MuMIn::r.squaredGLMM()\nbt_multivar_mod_sigma&lt;- bt_multivar_mod %&gt;% sigma()\neuc_multivar_mod_sigma&lt;- euc_multivar_mod %&gt;% sigma()\n\n\n\n\n\n\nTable C.5: Parameter estimates from models explaining Box-Cox transformed deviation scores from the mean \\(Z_r\\) as a function of continuous and categorical peer-review ratings in multivariate analyses. Standard Errors (SE), 95% Confidence Intervals (95%CI) are reported for all estimates, while t values, degrees of freedom and p-values are presented for fixed-effects.\n\n\n\n\n\n\n\n\n\nParameter\nEffects\nGroup\nCoefficient\nSE\n95%CI\nt\ndf\np\n\n\n\n\nblue tit\n\n\n(Intercept)\nfixed\n\n-1.978\n0.379\n[-2.723,-1.234]\n-5.222\n442\n0\n\n\nRateAnalysis\nfixed\n\n-0.005\n0.003\n[-0.012,0.001]\n-1.547\n442\n0.123\n\n\nPublishable as is\nfixed\n\n0.142\n0.265\n[-0.378,0.662]\n0.537\n442\n0.592\n\n\nPublishable with major revision\nfixed\n\n-0.121\n0.18\n[-0.476,0.233]\n-0.671\n442\n0.502\n\n\nPublishable with minor revision\nfixed\n\n-0.005\n0.227\n[-0.451,0.44]\n-0.024\n442\n0.981\n\n\nMean Sorensen's index\nfixed\n\n0.409\n0.363\n[-0.303,1.122]\n1.129\n442\n0.26\n\n\nMixed model\nfixed\n\n0.734\n0.204\n[0.333,1.134]\n3.599\n442\n0\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.205\n0.048\n[0.13,0.324]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n0.653\n0.024\n[0.608,0.701]\n\n\n\n\n\nEucalyptus\n\n\n(Intercept)\nfixed\n\n-3.128\n0.808\n[-4.718,-1.538]\n-3.872\n302\n0\n\n\nRateAnalysis\nfixed\n\n-0.011\n0.006\n[-0.024,0.001]\n-1.778\n302\n0.076\n\n\nPublishable as is\nfixed\n\n1.167\n0.58\n[0.026,2.308]\n2.012\n302\n0.045\n\n\nPublishable with major revision\nfixed\n\n0.871\n0.4\n[0.084,1.658]\n2.179\n302\n0.03\n\n\nPublishable with minor revision\nfixed\n\n0.776\n0.484\n[-0.177,1.728]\n1.602\n302\n0.11\n\n\nMean Sorensen's index\nfixed\n\n0.546\n0.958\n[-1.339,2.432]\n0.57\n302\n0.569\n\n\nMixed model\nfixed\n\n0.185\n0.212\n[-0.233,0.603]\n0.871\n302\n0.384\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.331\n0.105\n[0.178,0.616]\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n1.092\n0.049\n[1.001,1.192]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe multivariate models did a poor job of explaining how different from the meta-analytic mean each analysis would be. For the blue tit analyses the \\(R^{2}\\) value for the whole model was 0.13 and for the fixed effects component was 0.04, and the residual standard deviation for the model was 0.65. Further, all of the fixed effects had 95% confidence intervals that overlaped 0. This evidence is all consistent with none of the predictor variables in this model (continuous review rating, categorical review rating, distinctiveness of variables included) having any meaningful effect on how far \\(Z_r\\) estimates fell from the meta-analytic mean for the blue tit analyses. The pattern is largely similar for the Eucalyptus multivariate analysis, in which \\(R^{2}\\) for the whole model was 0.11 and for the fixed effects component was 0.03, and the residual standard deviation for the model was 1.09. There is somewhat more of a hint of a pattern when examining the paramaeter estimates from the Eucalyptus analysis. In the case of the fixed effect of categorical reviewer ratings, analyses that were reviewed as ‘publishable as is’ and ‘publishable with major revisions’ appeared to produce results more different from the meta-analytic mean than those that were in the reference class of ‘deeply flawed and unpublishable’. However, the estimates are very uncertain (Eucalyptus fixed effect for ‘publishable as is’ 1.17 (95% CI 0.03,2.3), and for ‘publishable with major revision’ 0.14 (95% CI -0.38,0.66). Further, the collinearity between the categorical and continuous ratings make interpretation of effects involving either of these two variables unclear, and so we recommend against interpreting the pattern observed here. We report this analysis only for the sake of transparency.\n\n\nCode\nlist(bt_multivar_mod, euc_multivar_mod) %&gt;% \n  set_names(c(\"blue tit\", \"eucalyptus\")) %&gt;% \n  map_dfr(broom.mixed::glance, .id = \"dataset\") %&gt;% \n  left_join(list(bt_multivar_mod, euc_multivar_mod) %&gt;% \n              set_names(c(\"blue tit\", \"eucalyptus\")) %&gt;% \n              map_dfr(performance::performance, .id = \"dataset\")) %&gt;% \n  select(dataset, starts_with(\"R2_\"),  ICC, RMSE, Sigma) %&gt;% \n  mutate(dataset = case_when(str_detect(dataset, \"eucalyptus\") ~ \"Eucalyptus\",\n                             TRUE ~ dataset)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt(columns = function(x) rlang::is_bare_numeric(x),\n          fns = function(x) round(x, 2)) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::cols_label(R2_conditional = gt::md(\"$$R^{2}_{Conditional}$$\"),\n                 R2_marginal = gt::md(\"$$R^{2}_{Marginal}$$\"),\n                 Sigma = gt::md(\"$$\\\\sigma$$\"),\n                 dataset = \"Dataset\") %&gt;% \n  gt::tab_style(locations = cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                                       columns = dataset),\n                style = cell_text(style = \"italic\")) %&gt;% \n  gt::as_raw_html()\n\n\n\n\nTable C.6: Model summary metrics for multivariate models. \\(\\sigma\\) is the residual standard deviation, ICC is the intra-class correlation coefficient, and \\({R}_{M}^2\\) and \\({R}_{C}^2\\) are the marginal and conditional \\(R^2\\), respectively.\n\n\n\n\n  \n  \n\n\n\nDataset\n$$R^{2}_{Conditional}$$\n$$R^{2}_{Marginal}$$\nICC\nRMSE\n$$\\sigma$$\n\n\n\n\nblue tit\n0.13\n0.04\n0.09\n0.63\n0.65\n\n\nEucalyptus\n0.11\n0.03\n0.08\n1.05\n1.09\n\n\n\n\n\n\n\n\n\n\n\n\nC.6.2 Out of sample predictions \\(y_i\\)\n\n\nCode\nformula_study_id &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, \n                                rate_analysis, \n                                mean_diversity_index, \n                                mixed_model, \n                                study_id)) %&gt;% \n  add_model(model, \n            formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + rate_analysis + mean_diversity_index + mixed_model + (1 | study_id ))\n\nformula_ReviewerId &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, \n                                rate_analysis, \n                                mean_diversity_index, \n                                mixed_model,\n                                reviewer_id)) %&gt;% \n  add_model(model, \n            formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + rate_analysis + mean_diversity_index + mixed_model + (1 | reviewer_id ))\n\nformula_both &lt;- workflow() %&gt;%\n  add_variables(outcomes = box_cox_abs_deviation_score_estimate, \n                predictors =  c(publishable_as_is, \n                                rate_analysis, \n                                mean_diversity_index, \n                                mixed_model,\n                                reviewer_id, \n                                study_id)) %&gt;% \n  add_model(model,\n            formula = box_cox_abs_deviation_score_estimate ~ publishable_as_is + rate_analysis + mean_diversity_index + mixed_model + (1 | study_id) + (1 | reviewer_id))\n\n\n# ---- Create DF for combinatorial model specification ----\npossibly_parameters &lt;- possibly(parameters::parameters, otherwise = NA)\n\nposs_extract_fit_engine &lt;- possibly(extract_fit_engine, otherwise = NA)\n\nmodel_vars_multivar &lt;- \n  bind_rows(\n    tidyr::expand_grid(outcome = \"box_cox_abs_deviation_score_estimate\",\n                       random_intercepts = c(\"study_id\", \n                                             \"reviewer_id\")) %&gt;% \n      rowwise() %&gt;% \n      mutate(random_intercepts = as.list(random_intercepts)),\n    tidyr::expand_grid(outcome = \"box_cox_abs_deviation_score_estimate\",\n                       random_intercepts = c(\"study_id\", \n                                             \"reviewer_id\")) %&gt;% \n      group_by(outcome) %&gt;% \n      reframe(random_intercepts = list(random_intercepts))\n  ) %&gt;% \n  mutate(fixed_effects = list(c(\"publishable_as_is\", \n                                \"rate_analysis\", \n                                \"mean_diversity_index\", \n                                \"mixed_model\")))\n\nall_model_fits_multivar &lt;- \n  ManyEcoEvo_yi_results %&gt;% \n  filter(dataset == \"blue tit\", exclusion_set == \"complete\") %&gt;% \n  ungroup %&gt;% \n  select(dataset, estimate_type, effects_analysis) %&gt;% \n  bind_rows({deviation_models_yi_euc %&gt;% \n      ungroup %&gt;% \n      select(dataset, estimate_type, effects_analysis) }) %&gt;% \n  cross_join(model_vars_multivar) %&gt;% \n  mutate(effects_analysis = \n           map(effects_analysis, \n               mutate, \n               weight = importance_weights(1/box_cox_var)),\n         effects_analysis = \n           map(effects_analysis, \n               ~ .x %&gt;% \n                 unnest(review_data) %&gt;% \n                 select(study_id, \n                        starts_with(\"box_cox_abs_dev\"), \n                        RateAnalysis, \n                        PublishableAsIs,\n                        ReviewerId,\n                        box_cox_var,\n                        weight,\n                        mean_diversity_index,\n                        mixed_model) %&gt;% \n                 janitor::clean_names() %&gt;%\n                 mutate_if(is.character, factor) \n           ),\n         model_workflows = pmap(.l = list(outcome, \n                                          fixed_effects, \n                                          random_intercepts), \n                                .f = create_model_workflow),\n         fitted_mod_workflow = map2(model_workflows, \n                                    effects_analysis, \n                                    poss_fit), \n         fitted_model = map(fitted_mod_workflow, poss_extract_fit_engine),\n         convergence = map_if(fitted_model, \n                              ~ !is.na(.x),\n                              possibly_check_convergence) %&gt;% \n           as.logical(),\n         singularity = map_if(fitted_model, \n                              ~ !is.na(.x),\n                              possibly_check_singularity) %&gt;% \n           as.logical(),\n         params = map_if(fitted_model, \n                      ~ !is.na(.x),\n                      parameters::parameters),\n         fixed_effects = map_chr(fixed_effects, paste0, collapse = \", \")\n  ) %&gt;% \n  unnest_wider(random_intercepts, names_sep = \"_\") %&gt;% \n    select(-outcome, \n         -model_workflows, \n         -fitted_mod_workflow, \n         -effects_analysis,\n         estimate_type) %&gt;% \n  replace_na(list(convergence = FALSE, singularity = TRUE)) \n\nyi_multivar_singularity_convergence &lt;- \n  all_model_fits_multivar %&gt;% \n  left_join({all_model_fits_multivar %&gt;% \n      unnest(params) %&gt;% \n      filter(Effects == \"random\") %&gt;% \n      filter(is.infinite(CI_high) | is.na(SE)) %&gt;% \n      distinct(fixed_effects, \n               random_intercepts_1,\n               random_intercepts_2, \n               dataset, \n               estimate_type,\n               convergence, \n               singularity) %&gt;% \n      mutate(SD_calc = FALSE)}) %&gt;% \n  mutate(SD_calc = ifelse(is.na(SD_calc), TRUE, SD_calc))\n\n# If singularity == FALSE and convergence == TRUE, but the model appears here, then that's because\n# the SD and CI's couldn't be estimated by parameters::\n\nyi_multivar_singularity_convergence %&gt;% \n  select(-fixed_effects, -fitted_model, -params) %&gt;% \n  arrange(random_intercepts_1,\n          random_intercepts_2, \n          dataset,\n          estimate_type) %&gt;% \n  mutate(across(starts_with(\"random\"), \n                ~ str_replace_all(.x, \"_\", \" \") %&gt;%\n                  Hmisc::capitalize() %&gt;% \n                  str_replace(\"id\", \"ID\")),\n         dataset = str_replace(dataset, \"eucalyptus\", \"*Eucalyptus*\")) %&gt;% \n  group_by(dataset) %&gt;% \n  gt::gt() %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = scales::alpha(\"red\", 0.6)),\n      cell_text(color = \"white\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"singularity\", rows = singularity == TRUE),\n      cells_body(columns = \"convergence\", rows = convergence == FALSE), #TODO why didn't work here??\n      cells_body(columns = \"SD_calc\", rows = SD_calc == FALSE)\n    )\n  ) %&gt;% \n  gt::text_transform(fn = function(x) ifelse(x == TRUE, \"yes\", \"no\" ),\n                     locations = cells_body(columns = c(\"singularity\", \"convergence\", \"SD_calc\"))) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::cols_label(dataset = \"Dataset\",\n                 singularity = \"Singular Fit?\",\n                 convergence = \"Model converged?\",\n                 SD_calc = \"Can random effect SE be calculated?\") %&gt;% \n  gt::tab_spanner(label = \"Random Effects\",\n                  columns = gt::starts_with(\"random\")) %&gt;% \n  gt::sub_missing() %&gt;% \n  gt::cols_label_with(columns = gt::starts_with(\"random\"),\n                      fn = function(x) paste0(\"\")) %&gt;% \n  gt::text_transform(fn = function(x) map(x, gt::md), locations = cells_row_groups())\n\n\n\n\nTable C.7: Singularity and convergence for all random effects structure combinations of multivariate models trialled for all subsets of out of sample predictions \\(y_i\\).\n\n\n\n\n\n\n\n\n\nestimate_type\nRandom Effects\nModel converged?\nSingular Fit?\nCan random effect SE be calculated?\n\n\n\n\n\n\n\n\nblue tit\n\n\ny25\nReviewer ID\n—\nyes\nno\nyes\n\n\ny50\nReviewer ID\n—\nyes\nno\nyes\n\n\ny75\nReviewer ID\n—\nyes\nno\nyes\n\n\ny25\nStudy ID\nReviewer ID\nyes\nyes\nno\n\n\ny50\nStudy ID\nReviewer ID\nno\nyes\nyes\n\n\ny75\nStudy ID\nReviewer ID\nyes\nno\nno\n\n\ny25\nStudy ID\n—\nyes\nno\nno\n\n\ny50\nStudy ID\n—\nyes\nno\nyes\n\n\ny75\nStudy ID\n—\nyes\nno\nno\n\n\nEucalyptus\n\n\ny25\nReviewer ID\n—\nyes\nyes\nno\n\n\ny50\nReviewer ID\n—\nyes\nyes\nno\n\n\ny75\nReviewer ID\n—\nyes\nno\nyes\n\n\ny25\nStudy ID\nReviewer ID\nno\nyes\nyes\n\n\ny50\nStudy ID\nReviewer ID\nyes\nyes\nno\n\n\ny75\nStudy ID\nReviewer ID\nno\nyes\nyes\n\n\ny25\nStudy ID\n—\nyes\nno\nno\n\n\ny50\nStudy ID\n—\nyes\nno\nyes\n\n\ny75\nStudy ID\n—\nno\nyes\nyes\n\n\n\n\n\n\n\n\n\n\nFor the blue tit analyses, models with Reviewer ID as the only random effect were the only models that converged, and that weren’t singular (Table C.7). Conversely, of the different random effects structures we trialled for the Eucalyptus analyses, none successfully fitted, with models either failing to converge due to complete separation (lme4:: error: Downdated VtV is not positive definite, see https://github.com/lme4/lme4/issues/483). Consequently we did not fit multivariate models on out-of-sample predictions for the Eucalyptus dataset, and instead deviated from our intended plan of using random effects for both Effect ID and Reviewer ID, and instead using a single random effect for Reviewer ID (Table C.8, Table C.9).\n\n\nCode\nyi_multivar_singularity_convergence %&gt;% \n  select(-params) %&gt;% \n  filter(dataset == \"blue tit\", \n         random_intercepts_1 == \"reviewer_id\") %&gt;%\n  mutate(broom_summary = \n           map(fitted_model, broom.mixed::glance), \n         performance_summary = \n           map(fitted_model, performance::performance)) %&gt;% \n  unnest(c(performance_summary, \n           broom_summary), names_sep = \"-\") %&gt;% \n  select(dataset, estimate_type, \n         contains(c(\"RMSE\", \"sigma\",  \"R2\", \"nobs\", \"ICC\")), \n         -contains(\"AICc\")) %&gt;% \n  rename_with(~ str_remove(.x, \"performance_summary-\") %&gt;%\n                str_remove(\"broom_summary-\")) %&gt;% \n  select(-sigma) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt(columns = function(x) rlang::is_bare_numeric(x),\n          fns = function(x) round(x, 3)) %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::cols_label(R2_conditional = gt::md(\"$$R^{2}_{Conditional}$$\"),\n                 R2_marginal = gt::md(\"$$R^{2}_{Marginal}$$\"),\n                 Sigma = gt::md(\"$$\\\\sigma$$\"),\n                 dataset = \"Dataset\",\n                 nobs = gt::md(\"$N_{Obs}$\")) %&gt;% \n  gt::tab_style(locations = cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                                       columns = dataset),\n                style = cell_text(style = \"italic\")) %&gt;% \n   gt::cols_hide(dataset) %&gt;% \n  gt::as_raw_html()\n\n\n\n\nTable C.8: Model summary statistic for non-singular, converging multivariate models fit to out-of-sample predictions for blue tit dataset\n\n\n\n\n  \n  \n\n\n\nestimate_type\nRMSE\n$$\\sigma$$\n$$R^{2}_{Conditional}$$\n$$R^{2}_{Marginal}$$\n\\(N_{Obs}\\)\nICC\n\n\n\n\ny25\n0.568\n2.965\n0.005\n0.001\n234\n0.004\n\n\ny50\n0.646\n8.68\n0.002\n0.001\n221\n0.001\n\n\ny75\n0.526\n3.338\n0.009\n0.002\n234\n0.006\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nyi_multivar_singularity_convergence %&gt;% \n  filter(dataset == \"blue tit\", \n         random_intercepts_1 == \"reviewer_id\") %&gt;% \n  select(estimate_type, params) %&gt;% \n  unnest(params) %&gt;% \n  relocate(c(Effects, Group), .after = Parameter) %&gt;% \n  group_by() %&gt;% \n  gt::gt(rowname_col = \"estimate_type\") %&gt;% \n   gt::text_transform(fn = function(x) str_replace(x, \"publishable_as_is\", \"Categorical Peer Rating\") %&gt;% \n                       str_replace(., \"rate_analysis\", \"Continuous Peer Rating\") %&gt;% \n                        str_replace(., \"mean_diversity_index\", \"Sorensen's Index\") %&gt;% \n                        str_replace(., \"mixed_model\", \"Mixed Model\"),\n                     locations = cells_body(columns = c(\"Parameter\"))) %&gt;% \n   gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n   gt::sub_missing(missing_text = \"\") %&gt;% \n  gt::fmt(columns = function(x) rlang::is_bare_numeric(x),\n          fns = function(x) format(x, digits = 3)) %&gt;% \n      gt::fmt(columns = \"p\",\n          fns = function(x) gtsummary::style_pvalue(x)) %&gt;% \n  gt::text_transform(\n    locations = cells_stub(\n      rows = Parameter != \"(Intercept)\"\n    ),\n    fn = function(x){\n      paste0(\"\")\n    }\n  ) %&gt;% \n  gt::text_transform(locations = cells_body(columns = Group, rows = Group == \"reviewer_id\"),\n                     fn = function(x){\n                       str_replace(x, \"_\", \" \") %&gt;% \n                         Hmisc::capitalize() %&gt;% \n                         str_replace(\"id\", \"ID\")\n                     }) %&gt;% \n  \n  gt::cols_label(CI_low = gt::md(\"95\\\\%CI\")) %&gt;% \n  gt::cols_label(df_error = \"df\") %&gt;% \n  gt::cols_merge(columns = starts_with(\"CI_\"), \n                 pattern = \"[{1},{2}]\") %&gt;% \n  gt::cols_hide(\"CI\")\n\n\n\n\nTable C.9: Parameter estimates for converging, non-singular multivariate models fitted to blue tit out-of-sample-prediction estimates \\(y_i\\).\n\n\n\n\n\n\n\n\n\n\nParameter\nEffects\nGroup\nCoefficient\nSE\n95%CI\nt\ndf\np\n\n\n\n\ny25\n(Intercept)\nfixed\n\n-0.434955\n0.53569\n[-1.49e+00, 6.21e-01]\n-0.8119\n225\n0.4\n\n\n\nCategorical Peer Ratingpublishable as is\nfixed\n\n-0.449000\n0.34988\n[-1.14e+00, 2.40e-01]\n-1.2833\n225\n0.2\n\n\n\nCategorical Peer Ratingpublishable with major revision\nfixed\n\n-0.252737\n0.26102\n[-7.67e-01, 2.62e-01]\n-0.9683\n225\n0.3\n\n\n\nCategorical Peer Ratingpublishable with minor revision\nfixed\n\n-0.393090\n0.30802\n[-1.00e+00, 2.14e-01]\n-1.2762\n225\n0.2\n\n\n\nContinuous Peer Rating\nfixed\n\n0.002821\n0.00403\n[-5.12e-03, 1.08e-02]\n0.6999\n225\n0.5\n\n\n\nSorensen's Index\nfixed\n\n-0.939151\n0.53402\n[-1.99e+00, 1.13e-01]\n-1.7586\n225\n0.080\n\n\n\nMixed Model\nfixed\n\n0.233694\n0.35361\n[-4.63e-01, 9.31e-01]\n0.6609\n225\n0.5\n\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.177679\n1.35306\n[ 5.86e-08, 5.39e+05]\n\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n2.965341\n0.15919\n[ 2.67e+00, 3.29e+00]\n\n\n\n\n\ny50\n(Intercept)\nfixed\n\n0.592400\n0.62298\n[-6.36e-01, 1.82e+00]\n0.9509\n212\n0.3\n\n\n\nCategorical Peer Ratingpublishable as is\nfixed\n\n-0.570991\n0.43757\n[-1.43e+00, 2.92e-01]\n-1.3049\n212\n0.2\n\n\n\nCategorical Peer Ratingpublishable with major revision\nfixed\n\n-0.175489\n0.32476\n[-8.16e-01, 4.65e-01]\n-0.5404\n212\n0.6\n\n\n\nCategorical Peer Ratingpublishable with minor revision\nfixed\n\n-0.382768\n0.38562\n[-1.14e+00, 3.77e-01]\n-0.9926\n212\n0.3\n\n\n\nContinuous Peer Rating\nfixed\n\n0.002927\n0.00501\n[-6.96e-03, 1.28e-02]\n0.5838\n212\n0.6\n\n\n\nSorensen's Index\nfixed\n\n-2.591515\n0.63803\n[-3.85e+00,-1.33e+00]\n-4.0618\n212\n&lt;0.001\n\n\n\nMixed Model\nfixed\n\n-0.414392\n0.38626\n[-1.18e+00, 3.47e-01]\n-1.0728\n212\n0.3\n\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.290603\n7.44674\n[ 4.48e-23, 1.89e+21]\n\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n8.679757\n0.48307\n[ 7.78e+00, 9.68e+00]\n\n\n\n\n\ny75\n(Intercept)\nfixed\n\n-0.878646\n0.55697\n[-1.98e+00, 2.19e-01]\n-1.5775\n225\n0.12\n\n\n\nCategorical Peer Ratingpublishable as is\nfixed\n\n0.416202\n0.37277\n[-3.18e-01, 1.15e+00]\n1.1165\n225\n0.3\n\n\n\nCategorical Peer Ratingpublishable with major revision\nfixed\n\n0.125623\n0.27863\n[-4.23e-01, 6.75e-01]\n0.4509\n225\n0.7\n\n\n\nCategorical Peer Ratingpublishable with minor revision\nfixed\n\n0.384996\n0.33221\n[-2.70e-01, 1.04e+00]\n1.1589\n225\n0.2\n\n\n\nContinuous Peer Rating\nfixed\n\n0.000102\n0.00427\n[-8.32e-03, 8.53e-03]\n0.0239\n225\n&gt;0.9\n\n\n\nSorensen's Index\nfixed\n\n0.154068\n0.57797\n[-9.85e-01, 1.29e+00]\n0.2666\n225\n0.8\n\n\n\nMixed Model\nfixed\n\n-0.710920\n0.34294\n[-1.39e+00,-3.51e-02]\n-2.0730\n225\n0.039\n\n\n\nSD (Intercept)\nrandom\nReviewer ID\n0.268886\n1.14527\n[ 6.37e-05, 1.14e+03]\n\n\n\n\n\n\nSD (Observations)\nrandom\nResidual\n3.338100\n0.17928\n[ 3.00e+00, 3.71e+00]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM3_ExplainingDeviation.html#sec-yi-summary",
    "href": "supp_mat/SM3_ExplainingDeviation.html#sec-yi-summary",
    "title": "SM C: Explaining Variation in Deviation Scores",
    "section": "C.7 Model Summary Metrics for out-of-sample predictions \\(y_i\\)",
    "text": "C.7 Model Summary Metrics for out-of-sample predictions \\(y_i\\)\n\n\nCode\ntbl_data_yi_deviation_model_params %&gt;% \n  dplyr::filter(dataset != \"blue tit\" | str_detect(model_name, \"random\", negate = TRUE)) %&gt;% \n  gt::gt(rowname_col = \"dataset\") %&gt;% \n  gt::opt_stylize(style = 6, color = \"gray\") %&gt;% \n  gt::sub_missing(missing_text = \"\") %&gt;% \n  gt::fmt(columns = function(x) rlang::is_bare_numeric(x),\n          fns = function(x) format(x, digits = 3)) %&gt;% \n  gt::cols_label(dataset = \"Dataset\",\n                 R2 = gt::md(\"$$R^2$$\"),\n                 R2_conditional = \"$$R^{2}_{Conditional}$$\",\n                 R2_marginal = \"$$R^{2}_{Marginal}$$\",\n                 Sigma = \"$$\\\\sigma$$\",\n                 nobs = \"$$N_{Obs.}$$\",\n                 estimate_type = \"Prediction Scenario\") %&gt;% \n  gt::tab_style(locations = cells_body(rows = str_detect(dataset, \"Eucalyptus\"),\n                                       columns = dataset),\n                                      style = cell_text(style = \"italic\")) %&gt;% \n  gt::text_transform(\n    locations = cells_stub(\n      rows = estimate_type != \"y25\"\n    ),\n    fn = function(x){\n      paste0(\"\")\n    }\n  ) %&gt;% \n  gt::tab_style(locations = gt::cells_stub(rows = str_detect(dataset, \"Eucalyptus\")),\n                style = cell_text(style = \"italic\"))\n\n\n\n\nTable C.10: Model summary metrics for models of Box-Cox transformed deviation from the mean \\(y_i\\) estimate as a function of categorical peer-review rating, continuous peer-review rating, and Sorensen’s index for blue tit and Eucalyptus analyses, and also for the inclusion of random effects for Eucalyptus analyses. Coefficient of determination, \\(R^2\\), is reported for models of deviation as a function of Sorensen diversity scores and presence of random effects, while \\(R^{2}_{Conditional}\\), \\(R^{2}_{Marginal}\\) and the intra-class correlation (ICC) are reported for models of deviation as explained by peer-review ratings. For all models the residual standard deviation \\(\\sigma\\), root mean squared error (RMSE) were calculated. The number of observations (\\(N_{Obs.}\\)) is displayed for reference.\n\n\n\n\n\n\n\n\n\n\nPrediction Scenario\n$$R^2$$\n$$R^{2}_{Conditional}$$\n$$R^{2}_{Marginal}$$\nICC\n$$\\sigma$$\nRMSE\n$$N_{Obs.}$$\n\n\n\n\nDeviation explained by Sorensen's index\n\n\nblue tit\ny25\n0.002237\n\n\n\n6.10e-01\n6.00e-01\n62\n\n\n\ny50\n0.038908\n\n\n\n7.28e-01\n7.16e-01\n59\n\n\n\ny75\n0.004627\n\n\n\n6.51e-01\n6.41e-01\n62\n\n\nEucalyptus\ny25\n0.000879\n\n\n\n1.24e+00\n1.18e+00\n22\n\n\n\ny50\n0.021860\n\n\n\n1.30e+00\n1.25e+00\n24\n\n\n\ny75\n0.004549\n\n\n\n1.12e+00\n1.07e+00\n24\n\n\nDeviation explained by continuous ratings\n\n\nblue tit\ny25\n\n1.0000\n1.44e-28\n1.00000\n2.42e-07\n5.27e-15\n234\n\n\n\ny50\n\n1.0000\n2.66e-27\n1.00000\n7.76e-07\n1.82e-14\n221\n\n\n\ny75\n\n1.0000\n1.08e-23\n1.00000\n1.13e-05\n5.99e-12\n234\n\n\nEucalyptus\ny25\n\n1.0000\n6.70e-23\n1.00000\n2.22e-05\n1.93e-11\n102\n\n\n\ny50\n\n1.0000\n9.46e-32\n1.00000\n1.02e-04\n5.20e-16\n109\n\n\n\ny75\n\n1.0000\n3.47e-31\n1.00000\n3.91e-07\n4.46e-16\n109\n\n\nDeviation explained by categorical ratings\n\n\nblue tit\ny25\n\n\n5.49e-03\n\n5.99e-01\n5.94e-01\n234\n\n\n\ny50\n\n0.1052\n1.25e-02\n0.09379\n7.11e-01\n6.79e-01\n221\n\n\n\ny75\n\n0.1820\n3.54e-02\n0.15195\n5.75e-01\n5.41e-01\n234\n\n\nEucalyptus\ny25\n\n\n2.00e-02\n\n1.21e+00\n1.19e+00\n102\n\n\n\ny50\n\n0.0442\n4.32e-02\n0.00101\n1.37e+00\n1.34e+00\n109\n\n\n\ny75\n\n\n1.82e-02\n\n1.11e+00\n1.09e+00\n109\n\n\nDeviation explained by inclusion of random effects\n\n\nEucalyptus\ny25\n0.091845\n\n\n\n1.18e+00\n1.13e+00\n22\n\n\n\ny50\n0.206062\n\n\n\n1.17e+00\n1.12e+00\n24\n\n\n\ny75\n0.000143\n\n\n\n1.12e+00\n1.07e+00\n24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using Lme4.” Journal Article. 2015 67 (1): 48. https://doi.org/10.18637/jss.v067.i01.\n\n\nDancho, Matt, and Davis Vaughan. 2023. Timetk: A Tool Kit for Working with Time Series. https://CRAN.R-project.org/package=timetk.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. “Performance: An r Package for Assessment, Comparison and Testing of Statistical Models.” Journal Article. Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Explaining Variation in Deviation Scores</span>"
    ]
  },
  {
    "objectID": "supp_mat/SM4_case_study_datasets.html",
    "href": "supp_mat/SM4_case_study_datasets.html",
    "title": "SM D: Correlation Matrices of Case Study Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ManyEcoEvo)\nlibrary(GGally)\nset.seed(1234)\n\n\nPairwise-correlation plots for the Eucalyptus and blue tit case-study data provided to analysts are shown in Figure D.1 and Figure D.2, respectively. Plots were created with R package GGally (Schloerke et al. 2022).\n\n\nCode\nManyEcoEvo::euc_data %&gt;%\n  select(where(is_double),\n         -Date, \n         -`Quadrat no`,\n         -Easting, \n         -Northing,\n         -`small*0.25+medium*1.25+large*2.5`,\n         -`average.proportion.of.plots.containing.at.least.one.euc.seedling.of.any.size`) %&gt;%\n  GGally::ggpairs()\n\n\n\n\n\n\n\n\nFigure D.1: Pairwise correllation plot for all Eucalyptus dataset variables except for Date, Quadrat no, Easting, Northing.\n\n\n\n\n\n\n\nCode\nManyEcoEvo::blue_tit_data %&gt;%\n  naniar::replace_with_na_all(condition = ~ .x == \".\") %&gt;% \n  mutate(across(c(contains(\"_ring\"), \n                  rear_nest_trt, \n                  hatch_year, \n                  hatch_nest_breed_ID,\n                  hatch_Area,\n                  hatch_Box,\n                  day14_measurer,\n                  contains(\"hatch_Box\"),\n                  starts_with(\"rear_\"),\n                  starts_with(\"hatch_nest\"),\n                  home_or_away,\n                  -rear_d0_rear_nest_brood_size,\n                  contains(\"manipulation\"),\n                  chick_sex_molec,\n                  Date_of_day14,\n                  `Extra-pair_paternity`,\n                  -rear_Cs_in,\n                  -rear_Cs_out,\n                  chick_survival_to_first_breed_season,\n                  -rear_Cs_at_start_of_rearing), \n                as.factor),\n         across(where(is.character), as.numeric),\n         across(c(rear_Cs_out,\n                  rear_Cs_in,\n                  rear_Cs_at_start_of_rearing),\n                as.integer)) %&gt;% \n  select(where(is.numeric), -`day 14 weight`) %&gt;% \n  GGally::ggpairs() \n\n\n\n\n\n\n\n\nFigure D.2: Pairwise correlation plot of all numeric variables in blue tit case study dataset\n\n\n\n\n\n\n\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2022. GGally: Extension to ’Ggplot2’.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Correlation Matrices of Case Study Data</span>"
    ]
  }
]