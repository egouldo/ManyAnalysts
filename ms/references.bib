
@article{aczel2021,
	title = {Consensus-based guidance for conducting and reporting multi-analyst studies},
	author = {Aczel, Balazs and Szaszi, Barnabas and Nilsonne, Gustav and van den Akker, Olmo R and Albers, Casper J and van Assen, Marcel ALM and Bastiaansen, Jojanneke A and Benjamin, Daniel and Boehm, Udo and Botvinik-Nezer, Rotem and Bringmann, Laura F and Busch, Niko A and Caruyer, Emmanuel and Cataldo, Andrea M and Cowan, Nelson and Delios, Andrew and van Dongen, Noah NN and Donkin, Chris and van Doorn, Johnny B and Dreber, Anna and Dutilh, Gilles and Egan, Gary F and Gernsbacher, Morton Ann and Hoekstra, Rink and Hoffmann, Sabine and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Jonas, Kai J and Kindel, Alexander T and Kirchler, Michael and Kunkels, Yoram K and Lindsay, D Stephen and Mangin, Jean-Francois and Matzke, Dora and {Munafò}, Marcus R and Newell, Ben R and Nosek, Brian A and Poldrack, Russell A and van Ravenzwaaij, Don and Rieskamp, {Jörg} and Salganik, Matthew J and Sarafoglou, Alexandra and Schonberg, Tom and Schweinsberg, Martin and Shanks, David and Silberzahn, Raphael and Simons, Daniel J and Spellman, Barbara A and St-Jean, Samuel and Starns, Jeffrey J and Uhlmann, Eric Luis and Wicherts, Jelte and Wagenmakers, Eric-Jan},
	year = {2021},
	month = {11},
	date = {2021-11-09},
	journal = {eLife},
	volume = {10},
	doi = {10.7554/elife.72185},
	url = {http://dx.doi.org/10.7554/eLife.72185},
	langid = {en}
}

@software{AllaireQuarto2024,
	author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
	doi = {10.5281/zenodo.5960048},
	month = feb,
	title = {{Quarto}},
	url = {https://github.com/quarto-dev/quarto-cli},
	version = {1.4},
	year = {2024}
}

@article{arif2023,
   author = {Arif, Suchinta and MacNeil, M. Aaron},
   title = {Applying the structural causal model framework for observational causal inference in ecology},
   journal = {Ecological Monographs},
   volume = {93},
   number = {1},
   pages = {e1554},
   abstract = {Abstract Ecologists are often interested in answering causal questions from observational data but generally lack the training to appropriately infer causation. When applying statistical analysis (e.g., generalized linear model) on observational data, common statistical adjustments can often lead to biased estimates between variables of interest due to processes such as confounding, overcontrol, and collider bias. To overcome these limitations, we present an overview of structural causal modeling (SCM), an emerging causal inference framework that can be used to determine cause-and-effect relationships from observational data. The SCM framework uses directed acyclic graphs (DAGs) to visualize researchers' assumptions about the causal structure of a system or process under study. Following this, a DAG-based graphical rule known as the backdoor criterion can be applied to determine statistical adjustments (or lack thereof) required to determine causal relationships from observational data. In the presence of unobserved confounding variables, an additional rule called the frontdoor criterion can be employed to determine causal effects. Here, we use simulated ecological examples to review how the backdoor and frontdoor criteria can return accurate causal estimates between variables of interest, as well as how biases can arise when these criteria are not used. We further provide an overview of studies that have applied the SCM framework in ecology. SCM, along with its application of DAGs, has been widely used in other disciplines to make valid causal inferences from observational data. Their use in ecology holds tremendous potential for quantifying causal relationships and investigating a range of ecological questions without randomized experiments.},
   ISSN = {0012-9615},
   DOI = {https://doi.org/10.1002/ecm.1554},
   url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1554},
   year = {2023},
   type = {Journal Article}
}


@article{atkinson2022,
   author = {Atkinson, Joe and Brudvig, Lars A. and Mallen-Cooper, Max and Nakagawa, Shinichi and Moles, Angela T. and Bonser, Stephen P.},
   title = {Terrestrial ecosystem restoration increases biodiversity and reduces its variability, but not to reference levels: A global meta-analysis},
   journal = {Ecology Letters},
   volume = {25},
   number = {7},
   pages = {1725-1737},
   abstract = {Abstract Ecological restoration projects often have variable and unpredictable outcomes, and these can limit the overall impact on biodiversity. Previous syntheses have investigated restoration effectiveness by comparing average restored conditions to average conditions in unrestored or reference systems. Here, we provide the first quantification of the extent to which restoration affects both the mean and variability of biodiversity outcomes, through a global meta-analysis of 83 terrestrial restoration studies. We found that, relative to unrestored (degraded) sites, restoration actions increased biodiversity by an average of 20%, while decreasing the variability of biodiversity (quantified by the coefficient of variation) by an average of 14%. As restorations aged, mean biodiversity increased and variability decreased relative to unrestored sites. However, restoration sites remained, on average, 13% below the biodiversity of reference (target) ecosystems, and were characterised by higher (20%) variability. The lower mean and higher variability in biodiversity at restored sites relative to reference sites remained consistent over time, suggesting that sources of variation (e.g. prior land use, restoration practices) have an enduring influence on restoration outcomes. Our results point to the need for new research confronting the causes of variability in restoration outcomes, and close variability and biodiversity gaps between restored and reference conditions.},
   ISSN = {1461-023X},
   DOI = {https://doi.org/10.1111/ele.14025},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.14025},
   year = {2022},
   type = {Journal Article}
}

@article{auspurg2021,
   author = {Auspurg, Katrin and Brüderl, Josef},
   title = {Has the credibility of the social sciences been credibly destroyed? reanalyzing the “Many analysts, one data set” project},
   journal = {Socius},
   volume = {7},
   pages = {23780231211024421},
   abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an article in which 29 teams analyzed the same research question with the same data: Are soccer referees more likely to give red cards to players with dark skin tone than light skin tone? The results obtained by the teams differed extensively. Many concluded from this widely noted exercise that the social sciences are not rigorous enough to provide definitive answers. In this article, we investigate why results diverged so much. We argue that the main reason was an unclear research question: Teams differed in their interpretation of the research question and therefore used diverse research designs and model specifications. We show by reanalyzing the data that with a clear research question, a precise definition of the parameter of interest, and theory-guided causal reasoning, results vary only within a narrow range. The broad conclusion of our reanalysis is that social science research needs to be more precise in its “estimands” to become credible.},
   keywords = {credibility crisis,crowdsourcing,reproduction,replication,causal reasoning,robustness analysis,multiverse analysis,sensitivity analysis,estimands},
   DOI = {10.1177/23780231211024421},
   url = {https://journals.sagepub.com/doi/abs/10.1177/23780231211024421},
   year = {2021},
   type = {Journal Article}
}

@article{babel2019,
title={Decision-making in model construction: unveiling habits},
ISSN={1364-8152},
abstractNote={The first stages of the modeling process remain thus far poorly studied, in particular the modeler’s decision-making in regard to the inclusion and representation of environmental phenomena within the model. Anchoring itself in the theoretical and methodological …},
journal={Environmental Modelling & Software},
author={Babel, Lucie and Vinck, Dominique and Karssenberg, Derek},
year={2019} 
}


@article{bakker2018,
title={Ensuring the quality and specificity of preregistrations},
DOI={10.31234/osf.io/cdgyh},
abstractNote={Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of ‘researcher degrees of freedom’ aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called “OSF Preregistration”, http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the “Standard” format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff’s Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
journal={PsyArXiv},
author={Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte M.},
year={2018} 
}

@article{borenstein2017,
   author = {Borenstein, Michael and Higgins, Julian P. T. and Hedges, Larry and Rothstein, Hannah},
   title = {Basics of meta-analysis: I2 is not an absolute measure of heterogeneity},
   journal = {Research Synthesis Methods},
   volume = {8},
   pages = {5-18},
   DOI = {10.1002/jrsm.1230},
   year = {2017},
   type = {Journal Article}
}

@Article{botvinik-nezer2020,
author = {Botvinik-Nezer, Rotem and Holzmeister, Felix and Camerer, Colin F. and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A. and Adcock, Alison and Avesani, Paolo and Baczkowski, Blazej and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nadège and Beaton, Derek and Beitner, Julia and Benoit, Roland and Berkers, Ruud and Bhanji, Jamil and Biswal, Bharat and Bobadilla-Suarez, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine and Bowring, Alexander and Braem, Senne and Brooks, Hayley and Brudner, Emily and Calderon, Cristian and Camilleri, Julia and Castrellon, Jaime and Cecchetti, Luca and Cieslik, Edna and Cole, Zachary and Collignon, Olivier and Cox, Robert and Cunningham, William and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles and De Luca, Alberto and Delgado, Mauricio and Demetriou, Lysia and Dennison, Jeffrey and Di, Xin and Dickie, Erin and Dobryakova, Ekaterina and Donnat, Claire and Dukart, Juergen and Duncan, Niall W. and Durnez, Joke and Eed, Amr and Eickhoff, Simon and Erhart, Andrew and Fontanesi, Laura and Fricke, G. Matthew and Galvan, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle and Golowin, Sergej and González-García, Carlos and Gorgolewski, Krzysztof and Grady, Cheryl and Green, Mikella and Guassi Moreira, João and Guest, Olivia and Hakimi, Shabnam and Hamilton, J. Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott and Hughes, Matthew and Iacovella, Vittorio and Iordan, Alexandru and Isager, Peder and Isik, Ayse Ilkay and Jahn, Andrew and Johnson, Matthew and Johnstone, Tom and Joseph, Michael and Juliano, Anthony and Kable, Joseph and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy and Kucukboyaci, Nuri Erkut and Kuhl, Brice A. and Kupek, Sebastian and Laird, Angela R. and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y. C. and Lim, Phui Cheng and Lintz, Evan N. and Liphardt, Schuyler W. and {Losecaat Vermeer}, Annabel B. and Love, Bradley C. and Mack, Michael L. and Malpica, Norberto and Marins, Theo and Maumet, Camille and McDonald, Kelsey and McGuire, Joseph T. and Melero, Helena and {Méndez Leal}, Adriana S. and Meyer, Benjamin and Meyer, Kristin N. and Mihai, Glad and Mitsis, Georgios D. and Moll, Jorge and Nielson, Dylan M. and Nilsonne, Gustav and Notter, Michael P. and Olivetti, Emanuele and Onicas, Adrian I. and Papale, Paolo and Patil, Kaustubh R. and Peelle, Jonathan E. and Pérez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and {Reuter-Lorenz}, Patricia A. and Reynolds, Richard C. and Ricciardi, Emiliano and Rieck, Jenny R. and {Rodriguez-Thompson}, Anais M. and Romyn, Anthony and Salo, Taylor and {Samanez-Larkin}, Gregory R. and {Sanz-Morales}, Emilio and Schlichting, Margaret L. and Schultz, Douglas H. and Shen, Qiang and Sheridan, Margaret A. and Silvers, Jennifer A. and Skagerlund, Kenny and Smith, Alec and Smith, David V. and {Sokol-Hessner}, Peter and Steinkamp, Simon R. and Tashjian, Sarah M. and Thirion, Bertrand and Thorp, John N. and Tinghög, Gustav and Tisdall, Loreen and Tompson, Steven H. and {Toro-Serey}, Claudio and {Torre Tresols},  Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and {van ‘t Veer}, Anna E. and Verguts, Tom and Vettel, Jean M. and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B. and Weeda, Wouter D. and Weis, Susanne and White, David J. and Wisniewski, David and Xifra-Porxas, Alba and Yearling, Emily A. and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S. L. and Zhang, Lei and Zhang, Xu and Zosky, Joshua E. and Nichols, Thomas E. and Poldrack, Russell A. and Schonberg, Tom},
title = {Variability in the analysis of a single neuroimaging dataset by many teams.}, 
journal = {Nature}, 
volume = {582}, 
number = {7810}, 
pages = {84--88}, 
year = {2020}, 
abstract = {Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses$^{1}$. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset$^{2-5}$. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.}, 
keywords = {}
}



@article{breznau2022,
title={Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty},
abstractNote={How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6% of the total variance in effect sizes and 10% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the …},
author={Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Nguyen, Hung H. V. and Adem, Muna and Adriaans, Jule and Alvarez-Benjumea, Amalia and Andersen, Henrik K. and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix S. and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes N. and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin B. and Carlos-Castillo, Juan and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Amélie and Grömping, Max and Groß, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and Hövermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignácz, Zsófia S. and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Kołczyńska, Marta and Kuk, John and Kunißen, Katharina and Kurti Sinatra, Dafina and Langenkamp, Alexander and Lersch, Philipp M. and Löbel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan E. and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar J. and McManus, Patricia and McWagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan and Moya, Cristóbal and Neunhoeffer, Marcel and Nüst, Daniel and Nygård, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna O. and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel R. and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Regine and Schmidt, Katja M. and Schmidt-Catran, Alexander and Schmiedeberg, Claudia and Schneider, Jürgen and Schoonvelde, Martijn and Schulte-Cloos, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, Jürgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian and Vagni, Giacomo and Van Assche, Jasper and van der Linden, Meta and van der Noll, Jolanda and Van Hootegem, Arno and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and Żółtak, Tomasz},
year={2022},
	month = {10},
	date = {2022-10-28},
	journal = {Proceedings of the National Academy of Sciences},
	pages = {e2203150119},
	volume = {119},
	number = {44},
	doi = {10.1073/pnas.2203150119},
	url = {https://doi.org/10.1073/pnas.2203150119},
	langid = {en}
}


@article{briga2021,
   author = {Briga, Michael and Verhulst, Simon},
   title = {Mosaic metabolic ageing: Basal and standard metabolic rates age in opposite directions and independent of environmental quality, sex and life span in a passerine},
   journal = {Functional Ecology},
   volume = {35},
   number = {5},
   pages = {1055-1068},
   abstract = {Abstract Crucial to our understanding of the ageing process is identifying how traits change with age, which variables alter their ageing process and how these traits associate with fitness. Here we investigated metabolic ageing in outdoor-living captive zebra finches experiencing foraging costs. We longitudinally monitored 407 individuals over 6 years and collected 3,213 measurements of two independent mass-adjusted metabolic traits: basal metabolic rate (BMRm) at thermoneutral temperatures and standard metabolic rate (SMRm), measured as BMRm but at ambient temperatures below thermoneutrality. We define mosaic or asynchronous ageing as the difference in standardized absolute ageing rates between traits, and we estimate the degree of asynchrony using the within-individual correlation of change in trait values with age. BMRm decreased linearly with age, consistent with earlier reports. In contrast, SMRm increased linearly with age. The absolute standardized change with age was significantly faster for BMRm compared to SMRm, and the within-individual correlation of age related change was negligible. To the best of our knowledge, this is the first quantification of SMRm ageing, and the finding that SMRm and BMRm age in opposite directions. Neither metabolic rate nor metabolic ageing rate were associated with variation in life span between individuals. Moreover, experimental manipulations of environmental quality that decreased BMRm and SMRm and shortened life span by 6 months (12%) did not affect the ageing of either metabolic trait. Females lived 2 months (4%) shorter than males, but none of the metabolic traits showed sex-specific differences at any age. Our findings indicate, in contrast to the current view, that baseline energy requirements increase with age, because animals do not generally live in thermoneutral conditions, and illustrate the importance of studying the ageing phenotype in an ecologically realistic setting. A free Plain Language Summary can be found within the Supporting Information of this article.},
   ISSN = {0269-8463},
   DOI = {https://doi.org/10.1111/1365-2435.13785},
   url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2435.13785},
   year = {2021},
   type = {Journal Article}
}


@book{burnham2002,
   author = {Burnham, K. P. and Anderson, D. R.},
   title = {Model selection and multimodel inference: a practical information-theoretical approach},
   publisher = {Springer-Verlag},
   address = {New York},
   edition = {2nd},
   DOI = {10.1007/b97636},
   year = {2002},
   type = {Book}
}

@article{cade2015,
   author = {Cade, Brian S.},
   title = {Model averaging and muddled multimodel inferences},
   journal = {Ecology},
   volume = {96},
   number = {9},
   pages = {2370-2382},
   abstract = {[Three flawed practices associated with model averaging coefficients for predictor variables in regression models commonly occur when making multimodel inferences in analyses of ecological data. Model-averaged regression coefficients based on Akaike information criterion (AIC) weights have been recommended for addressing model uncertainty but they are not valid, interpretable estimates of partial effects for individual predictors when there is multicollinearity among the predictor variables. Multicollinearity implies that the scaling of units in the denominators of the regression coefficients may change across models such that neither the parameters nor their estimates have common scales, therefore averaging them makes no sense. The associated sums of AIC model weights recommended to assess relative importance of individual predictors are really a measure of relative importance of models, with little information about contributions by individual predictors compared to other measures of relative importance based on effects size or variance reduction. Sometimes the model-averaged regression coefficients for predictor variables are incorrectly used to make model-averaged predictions of the response variable when the models are not linear in the parameters. I demonstrate the issues with the first two practices using the college grade point average example extensively analyzed by Burnham and Anderson. I show how partial standard deviations of the predictor variables can be used to detect changing scales of their estimates with multicollinearity. Standardizing estimates based on partial standard deviations for their variables can be used to make the scaling of the estimates commensurate across models, a necessary but not sufficient condition for model averaging of the estimates to be sensible. A unimodal distribution of estimates and valid interpretation of individual parameters are additional requisite conditions. The standardized estimates or equivalently the t statistics on unstandardized estimates also can be used to provide more informative measures of relative importance than sums of AIC weights. Finally, I illustrate how seriously compromised statistical interpretations and predictions can be for all three of these flawed practices by critiquing their use in a recent species distribution modeling technique developed for predicting Greater Sage-Grouse (Centrocercus urophasianus) distribution in Colorado, USA. These model averaging issues are common in other ecological literature and ought to be discontinued if we are to make effective scientific contributions to ecological knowledge and conservation of natural resources.]},
   ISSN = {00129658, 19399170},
   url = {http://www.jstor.org.ezproxy.whitman.edu/stable/24702343},
   year = {2015},
   type = {Journal Article}
}


@article{capilla-lasheras2022,
   author = {Capilla-Lasheras, Pablo and Thompson, Megan J. and Sánchez-Tójar, Alfredo and Haddou, Yacob and Branston, Claire J. and Réale, Denis and Charmantier, Anne and Dominoni, Davide M.},
   title = {A global meta-analysis reveals higher variation in breeding phenology in urban birds than in their non-urban neighbours},
   journal = {Ecology Letters},
   volume = {25},
   number = {11},
   pages = {2552-2570},
   abstract = {Abstract Cities pose a major ecological challenge for wildlife worldwide. Phenotypic variation, which can result from underlying genetic variation or plasticity, is an important metric to understand eco-evolutionary responses to environmental change. Recent work suggests that urban populations might have higher levels of phenotypic variation than non-urban counterparts. This prediction, however, has never been tested across species nor over a broad geographical range. Here, we conducted a meta-analysis of the avian literature to compare urban versus non-urban means and variation in phenology (i.e. lay date) and reproductive effort (i.e. clutch size, number of fledglings). First, we show that urban populations reproduce earlier and have smaller broods than non-urban conspecifics. Second, we show that urban populations have higher phenotypic variation in laying date than non-urban populations. This result arises from differences between populations within breeding seasons, conceivably due to higher landscape heterogeneity in urban habitats. These findings reveal a novel effect of urbanisation on animal life histories with potential implications for species adaptation to urban environments (which will require further investigation). The higher variation in phenology in birds subjected to urban disturbance could result from plastic responses to a heterogeneous environment, or from higher genetic variation in phenology, possibly linked to higher evolutionary potential.},
   ISSN = {1461-023X},
   DOI = {https://doi.org/10.1111/ele.14099},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.14099},
   year = {2022},
   type = {Journal Article}
}


@article{coretta2023,
   author = {Coretta, Stefano and Casillas, Joseph V. and Roessig, Simon and Franke, Michael and Ahn, Byron and Al-Hoorie, Ali H. and Al-Tamimi, Jalal and Alotaibi, Najd E. and AlShakhori, Mohammed K. and Altmiller, Ruth M. and Arantes, Pablo and Athanasopoulou, Angeliki and Baese-Berk, Melissa M. and Bailey, George and Sangma, Cheman Baira A and Beier, Eleonora J. and Benavides, Gabriela M. and Benker, Nicole and BensonMeyer, Emelia P. and Benway, Nina R. and Berry, Grant M. and Bing, Liwen and Bjorndahl, Christina and Bolyanatz, Mariška and Braver, Aaron and Brown, Violet A. and Brown, Alicia M. and Brugos, Alejna and Buchanan, Erin M. and Butlin, Tanna and Buxó-Lugo, Andrés and Caillol, Coline and Cangemi, Francesco and Carignan, Christopher and Carraturo, Sita and Caudrelier, Tiphaine and Chodroff, Eleanor and Cohn, Michelle and Cronenberg, Johanna and Crouzet, Olivier and Dagar, Erica L. and Dawson, Charlotte and Diantoro, Carissa A. and Dokovova, Marie and Drake, Shiloh and Du, Fengting and Dubuis, Margaux and Duême, Florent and Durward, Matthew and Egurtzegi, Ander and Elsherif, Mahmoud M. and Esser, Janina and Ferragne, Emmanuel and Ferreira, Fernanda and Fink, Lauren K. and Finley, Sara and Foster, Kurtis and Foulkes, Paul and Franzke, Rosa and Frazer-McKee, Gabriel and Fromont, Robert and García, Christina and Geller, Jason and Grasso, Camille L. and Greca, Pia and Grice, Martine and Grose-Hodge, Magdalena S. and Gully, Amelia J. and Halfacre, Caitlin and Hauser, Ivy and Hay, Jen and Haywood, Robert and Hellmuth, Sam and Hilger, Allison I. and Holliday, Nicole and Hoogland, Damar and Huang, Yaqian and Hughes, Vincent and Icardo Isasa, Ane and Ilchovska, Zlatomira G. and Jeon, Hae-Sung and Jones, Jacq and Junges, Mágat N. and Kaefer, Stephanie and Kaland, Constantijn and Kelley, Matthew C. and Kelly, Niamh E. and Kettig, Thomas and Khattab, Ghada and Koolen, Ruud and Krahmer, Emiel and Krajewska, Dorota and Krug, Andreas and Kumar, Abhilasha A. and Lander, Anna and Lentz, Tomas O. and Li, Wanyin and Li, Yanyu and Lialiou, Maria and Lima, Ronaldo M. and others },
   title = {Multidimensional signals and analytic flexibility: estimating degrees of freedom in human-speech analyses},
   journal = {Advances in Methods and Practices in Psychological Science},
   volume = {6},
   number = {3},
   pages = {25152459231162567},
   abstract = {Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis that can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling but also from decisions regarding the quantification of the measured behavior. In this study, we gave the same speech-production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further found little to no evidence that the observed variability can be explained by analysts’ prior beliefs, expertise, or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system, and calibrate their (un)certainty in their conclusions.},
   keywords = {crowdsourcing science,data analysis,scientific transparency,speech,acoustic analysis},
   DOI = {10.1177/25152459231162567},
   url = {https://journals.sagepub.com/doi/abs/10.1177/25152459231162567},
   year = {2023},
   type = {Journal Article}
}



@article{culina2020,
	title = {Low availability of code in ecology: A call for urgent action},
	author = {Culina, Antica and van den Berg, Ilona and Evans, Simon and {Sánchez-Tójar}, Alfredo},
	year = {2020},
	month = {07},
	date = {2020-07-28},
	journal = {PLOS Biology},
	pages = {e3000763},
	volume = {18},
	number = {7},
	doi = {10.1371/journal.pbio.3000763},
	url = {http://dx.doi.org/10.1371/journal.pbio.3000763},
	langid = {en}
}


@article{dekogel1997,
   author = {DeKogel, C. H.},
   title = {Long-term effects of brood size manipulation on morphological development and sex-specific mortality of offspring},
   journal = {Journal of Animal Ecology},
   volume = {66},
   number = {2},
   pages = {167-178},
   keywords = {clutch size optimization
growth strategies
longevity
offspring
quality
sex ratio
starlings sturnus-vulgaris
kestrel falco-tinnunculus
life-history
traits
tit parus-major
zebra finches
clutch size
great tit
nestling
weight
reproduction
birds
Environmental Sciences & Ecology
Zoology},
   ISSN = {0021-8790},
   url = {<Go to ISI>://WOS:A1997WQ19600003},
   year = {1997},
   type = {Journal Article}
}

@article{deressa2023,
   author = {Deressa, Teshome and Stern, David and Vangronsveld, Jaco and Minx, Jan and Lizin, Sebastien and Malina, Robert and Bruns, Stephan},
   title = {More than half of statistically significant research findings in the environmental sciences are actually not},
   journal = {EcoEvoRxiv},
   DOI = {https://doi.org/10.32942/X24G6Z},
   year = {2023},
   type = {Journal Article}
}

@article{desbureaux2021,
title={Subjective modeling choices and the robustness of impact evaluations in conservation science},
volume={35},
ISSN={0888-8892},
number={5},
journal={Conservation Biology},
author={Desbureaux, Sébastien},
year={2021},
pages={1615–1626} 
}


@article{dormann2013,
   author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carré, Gabriel and Marquéz, Jaime R. García and Gruber, Bernd and Lafourcade, Bruno and Leitão, Pedro J. and Münkemüller, Tamara and McClean, Colin and Osborne, Patrick E. and Reineking, Björn and Schröder, Boris and Skidmore, Andrew K. and Zurell, Damaris and Lautenbach, Sven},
   title = {Collinearity: a review of methods to deal with it and a simulation study evaluating their performance},
   journal = {Ecography},
   volume = {36},
   number = {1},
   pages = {27-46},
   abstract = {Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the ‘folk lore’-thresholds of correlation coefficients between predictor variables of |r| >0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.},
   ISSN = {0906-7590},
   DOI = {https://doi.org/10.1111/j.1600-0587.2012.07348.x},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1600-0587.2012.07348.x},
   year = {2013},
   type = {Journal Article}
}



@proceedings{dragicevic2019,
author = {Dragicevic, P. and Jansen, Y. and Sarma, A. and Kay, M. and Chevalier, F.}, 
editor = {}, 
title = {Increasing the Transparency of Research papers with Explorable Multiverse Analyses}, 
booktitle = {CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, 
volume = {}, 
publisher = {Association for Computing Machinery}, 
address = {New York, NY, USA}, 
pages = {}, 
year = {2019}, 
abstract = {}, 
   DOI = {10.1145/3290605.3300295},
   url = {https://doi.org/10.1145/3290605.3300295},
keywords = {}
}


@article{fanelli2017,
   author = {Fanelli, Daniele and Costas, Rodrigo and Ioannidis, John P. A.},
   title = {Meta-assessment of bias in science},
   journal = {Proceedings of the National Academy of Sciences},
   volume = {114},
   pages = {3714-3719},
   abstract = {Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that US studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.},
   DOI = {10.1073/pnas.1618569114},
   url = {http://www.pnas.org/content/early/2017/03/15/1618569114.abstract},
   year = {2017},
   type = {Journal Article}
}

@article{fanelli2013,
   author = {Fanelli, Daniele and Ioannidis, John P. A.},
   title = {US studies may overestimate effect sizes in softer research},
   journal = {Proceedings of the National Academy of Sciences},
   volume = {110},
   number = {37},
   pages = {15031-15036},
   abstract = {Many biases affect scientific research, causing a waste of resources, posing a threat to human health, and hampering scientific progress. These problems are hypothesized to be worsened by lack of consensus on theories and methods, by selective publication processes, and by career systems too heavily oriented toward productivity, such as those adopted in the United States (US). Here, we extracted 1,174 primary outcomes appearing in 82 meta-analyses published in health-related biological and behavioral research sampled from the Web of Science categories Genetics &amp; Heredity and Psychiatry and measured how individual results deviated from the overall summary effect size within their respective meta-analysis. We found that primary studies whose outcome included behavioral parameters were generally more likely to report extreme effects, and those with a corresponding author based in the US were more likely to deviate in the direction predicted by their experimental hypotheses, particularly when their outcome did not include additional biological parameters. Nonbehavioral studies showed no such “US effect” and were subject mainly to sampling variance and small-study effects, which were stronger for non-US countries. Although this latter finding could be interpreted as a publication bias against non-US authors, the US effect observed in behavioral research is unlikely to be generated by editorial biases. Behavioral studies have lower methodological consensus and higher noise, making US researchers potentially more likely to express an underlying propensity to report strong and significant findings.},
   DOI = {10.1073/pnas.1302997110},
   url = {https://www.pnas.org/content/pnas/110/37/15031.full.pdf},
   year = {2013},
   type = {Journal Article}
}

@article{fidler2006,
   author = {Fidler, Fiona and Burgman, Mark A. and Cumming, Geoff and Buttrose, Robert and Thomason, Neil},
   title = {Impact of criticism of null-hypothesis significance testing on statistical reporting practices in conservation biology},
   journal = {Conservation Biology},
   volume = {20},
   number = {5},
   pages = {1539-1544},
   abstract = {Abstract:Over the last decade, criticisms of null-hypothesis significance testing have grown dramatically, and several alternative practices, such as confidence intervals, information theoretic, and Bayesian methods, have been advocated. Have these calls for change had an impact on the statistical reporting practices in conservation biology? In 2000 and 2001, 92% of sampled articles in Conservation Biology and Biological Conservation reported results of null-hypothesis tests. In 2005 this figure dropped to 78%. There were corresponding increases in the use of confidence intervals, information theoretic, and Bayesian techniques. Of those articles reporting null-hypothesis testing—which still easily constitute the majority—very few report statistical power (8%) and many misinterpret statistical nonsignificance as evidence for no effect (63%). Overall, results of our survey show some improvements in statistical practice, but further efforts are clearly required to move the discipline toward improved practices.
Resumen:En la última década, las críticas a la prueba de significancia estadística han crecido dramáticamente, y se han propuesto varias prácticas alternativas, como los intervalos de confianza, teoría de la información y métodos Bayesianos. ¿Han tenido impacto estos llamados al cambio sobre las prácticas de reporte estadístico en biología de la conservación? En 200 y 2001, 92% de los artículos muestreados en Conservation Biology y Biological Conservation reportaron pruebas de hipótesis nulas. En 2005 esta cifra cayó a 78%. Hubo incrementos correspondientes en el uso de intervalos de confianza, teoría de la información y técnicas Bayesianas. De los artículos que reportan pruebas de hipótesis nulas—que fácilmente aun constituyen la mayoría—muy pocos (8%) reportan el poder estadístico y muchos mal interpretan la no significancia estadística como evidencia de no efecto (63%). En general, los resultados de nuestro muestreo muestran algunas mejorías en la práctica estadística, pero claramente se requieren mayores esfuerzos para que la disciplina tenga mejores prácticas.},
   keywords = {Bayesian methods
confidence intervals
statistical power
statistical significance testing
intervalos de confianza
métodos Bayesianos
poder estadístico
prueba de significancia esta- dística},
   ISSN = {1523-1739},
   DOI = {10.1111/j.1523-1739.2006.00525.x},
   url = {http://dx.doi.org/10.1111/j.1523-1739.2006.00525.x},
   year = {2006},
   type = {Journal Article}
}

@article{fidler2017,
   author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
   title = {Metaresearch for evaluating reproducibility in ecology and evolution},
   journal = {BioScience},
   volume = {67},
   number = {3},
   pages = {282-289},
   note = {10.1093/biosci/biw159},
   abstract = {Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literatures may contain unverifiable claims. The conditions contributing to irreproducibility in other disciplines are also present in ecology. These include a large discrepancy between the proportion of “positive” or “significant” results and the average statistical power of empirical research, incomplete reporting of sampling stopping rules and results, journal policies that discourage replication studies, and a prevailing publish-or-perish research culture that encourages questionable research practices. We argue that these conditions constitute sufficient reason to systematically evaluate the reproducibility of the evidence base in ecology and evolution. In some cases, the direct replication of ecological research is difficult because of strong temporal and spatial dependencies, so here, we propose metaresearch projects that will provide proxy measures of reproducibility.},
   ISSN = {0006-3568},
   DOI = {10.1093/biosci/biw159},
   url = {http://dx.doi.org/10.1093/biosci/biw159},
   year = {2017},
   type = {Journal Article}
}


@article{flournoy2020,
	title = {Improving practices and inferences in developmental cognitive neuroscience},
	author = {Flournoy, John C. and Vijayakumar, Nandita and Cheng, Theresa W. and Cosme, Danielle and Flannery, Jessica E. and Pfeifer, Jennifer H.},
	year = {2020},
	month = {10},
	date = {2020-10},
	journal = {Developmental Cognitive Neuroscience},
	pages = {100807},
	volume = {45},
	doi = {10.1016/j.dcn.2020.100807},
	url = {http://dx.doi.org/10.1016/j.dcn.2020.100807},
	langid = {en}
}


@article{forstmeier2017,
   author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, T. H.},
   title = {Detecting and avoiding likely false-positive findings – a practical guide},
   journal = {Biological Reviews},
   volume = {92},
   pages = {1941-1968},
   DOI = {10.1111/brv.12315},
   year = {2017},
   type = {Journal Article}
}

@article{fraser2018,
   author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
   title = {Questionable research practices in ecology and evolution},
   journal = {PLOS ONE},
   volume = {13},
   number = {7},
   pages = {e0200303},
   abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
   DOI = {10.1371/journal.pone.0200303},
   url = {https://doi.org/10.1371/journal.pone.0200303},
   year = {2018},
   type = {Journal Article}
}

@book{gelman2007,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2007},
  publisher={Cambridge university press}
}

@article{gelman2009,
   author = {Gelman, Andrew and Weakliem, David},
   title = {Of beauty, sex, and power},
   journal = {American Scientist},
   volume = {97},
   pages = {310-316},
   abstract = {A series of papers in the Journal of Theoretical Biology has found evidence that beautiful parents have more daughters, violent men have more sons, and other sex-ratio patterns (Kanazawa, 2005, 2006, 2007, 2008). These papers have been shown to have statistical errors, but the question remains how to interpret findings that are intriguing, potentially important, but not statistically significant. From a classical statistical perspective, these studies have insufficient power to detect the magnitudes of effects (on the order of 1 percentage point) that could be expected based on earlier studies of sex ratios. The anticipated small effects can also be handled using a Bayesian prior distribution. These concerns are relevant to other studies of small effects and also to the reporting of such studies.},
   year = {2009},
   type = {Journal Article}
}


@article{gelman2013,
title={The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
abstractNote={Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated …},
journal={Department of Statistics, Columbia University},
author={Gelman, Andrew and Loken, Eric},
year={2013} 
}

@article{getz2017,
title={Making ecological models adequate},
volume={21},
DOI={10.1111/ele.12893},
number={2},
journal={Ecology Letters},
author={Getz, Wayne M and Marshall, Charles R and Carlson, Colin J and Giuggioli, Luca and Ryan, Sadie J and Romañach, Stephanie S and Boettiger, Carl and Chamberlain, Samuel D and Larsen, Laurel and D’Odorico, Paolo and et al.},
editor={Coulson, Tim},
year={2017},
month={Dec},
pages={153–166} 
}



@article{grueber2011,
   author = {Grueber, C. E. and Nakagawa, S. and Laws, R. J. and Jamieson, I. G.},
   title = {Multimodel inference in ecology and evolution: challenges and solutions},
   journal = {Journal of Evolutionary Biology},
   volume = {24},
   number = {4},
   pages = {699-711},
   DOI = {doi:10.1111/j.1420-9101.2010.02210.x},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1420-9101.2010.02210.x},
   year = {2011},
   type = {Journal Article}
}


@article{gurevitch2018,
	title = {Meta-analysis and the science of research synthesis},
	author = {Gurevitch, J. and Koricheva, J. and Nakagawa, S. and Stewart, G.},
	year = {2018},
	month = {03},
	date = {2018-03},
	journal = {Nature},
	pages = {175--182},
	volume = {555},
	number = {7695},
	doi = {10.1038/nature25753},
	}

@article{higgins2003,
   author = {Higgins, Julian P T and Thompson, Simon G and Deeks, Jonathan J and Altman, Douglas G},
   title = {Measuring inconsistency in meta-analyses},
   journal = {BMJ},
   volume = {327},
   number = {7414},
   pages = {557-560},
   DOI = {10.1136/bmj.327.7414.557},
   url = {https://www.bmj.com/content/bmj/327/7414/557.full.pdf},
   year = {2003},
   type = {Journal Article}
}

@article{hoffmann2021,
title={The multiplicity of analysis strategies jeopardizes replicability: lessons learned across disciplines.},
volume={8},
DOI={10.1098/rsos.201925},
abstractNote={For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.},
number={4},
journal={Royal Society Open Science},
author={Hoffmann, S and Schönbrodt, F and Elsas, R and Wilson, R and Strasser, U and Boulesteix, AL},
year={2021},
month={Apr},
pages={201925} 
}


@article{huntington-klein2021,
   author = {Huntington-Klein, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R. and Burli, Pralhad and Chen, Naibin and Grieco, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
   title = {The influence of hidden researcher decisions in applied microeconomics},
   journal = {Economic Inquiry},
   volume = {59},
   number = {3},
   pages = {944-960},
   abstract = {Abstract Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3–4 times the mean reported standard error.},
   ISSN = {0095-2583},
   DOI = {https://doi.org/10.1111/ecin.12992},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ecin.12992},
   year = {2021},
   type = {Journal Article}
}


@inbook{jennions2013,
   author = {Jennions, M. D. and Lortie, C. J. and Rosenberg, M. S. and Rothstein, H. R.},
   title = {Publication and related biases },
   booktitle = {Handbook of meta-analysis in ecology and evolution},
   editor = {Koricheva, J. and Gurevitch, J. and Mengersen, K.},
   publisher = {Princeton University Press},
   address = {Princeton, USA},
   chapter = {14},
   pages = {207-236},
   year = {2013},
   type = {Book Section}
}




@article{kimmel2023,
   author = {Kimmel, Kaitlin and Avolio, Meghan L. and Ferraro, Paul J.},
   title = {Empirical evidence of widespread exaggeration bias and selective reporting in ecology},
   journal = {Nature Ecology & Evolution},
   abstract = {In many scientific disciplines, common research practices have led to unreliable and exaggerated evidence about scientific phenomena. Here we describe some of these practices and quantify their pervasiveness in recent ecology publications in five popular journals. In an analysis of over 350 studies published between 2018 and 2020, we detect empirical evidence of exaggeration bias and selective reporting of statistically significant results. This evidence implies that the published effect sizes in ecology journals exaggerate the importance of the ecological relationships that they aim to quantify. An exaggerated evidence base hinders the ability of empirical ecology to reliably contribute to science, policy, and management. To increase the credibility of ecology research, we describe a set of actions that ecologists should take, including changes to scientific norms about what high-quality ecology looks like and expectations about what high-quality studies can deliver.},
   ISSN = {2397-334X},
   DOI = {10.1038/s41559-023-02144-3},
   url = {https://doi.org/10.1038/s41559-023-02144-3},
   year = {2023},
   type = {Journal Article}
}


@article{klein2014,
   author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Jr., Reginald B. Adams and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kappes, Heather Barry and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Swol, Lyn M. Van and Thompson, Donna and Veer, A. E. van ‘t and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
   title = {Investigating variation in replicability: a "many labs" replication project},
   journal = {Social Psychology},
   volume = {45},
   number = {3},
   pages = {142-152},
   abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
   DOI = {10.1027/1864-9335/a000178},
   year = {2014},
   type = {Journal Article}
}


@article{klein2018,
   author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and others },
   title = {Many Labs 2: investigating variation in replicability across samples and settings},
   journal = {Advances in Methods and Practices in Psychological Science},
   volume = {1},
   number = {4},
   pages = {443-490},
   abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p < .05), we found that 15 (54%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p < .0001), 14 (50%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25%) of the replications yielded effect sizes larger than the original ones, and 21 (75%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (< 0.20) in 16 of the replications (57%), and 9 effects (32%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
   keywords = {social psychology,cognitive psychology,replication,culture,individual differences,sampling effects,situational effects,meta-analysis,Registered Report,open data,open materials,preregistered},
   DOI = {10.1177/2515245918810225},
   url = {https://journals.sagepub.com/doi/abs/10.1177/2515245918810225},
   year = {2018},
   type = {Journal Article}
}

@book{knight2000,
   author = {Knight, K.},
   title = {Mathematical Statistics},
   publisher = {Chapman and Hall},
   address = {New York},
   year = {2000},
   type = {Book}
}

@article{koricheva2014,
   author = {Koricheva, Julia and Gurevitch, Jessica},
   title = {Uses and misuses of meta-analysis in plant ecology},
   journal = {Journal of Ecology},
   volume = {102},
   number = {4},
   pages = {828-844},
   abstract = {Summary The number of published meta-analyses in plant ecology has increased greatly over the last two decades. Meta-analysis has made a significant contribution to the field, allowing review of evidence for various ecological hypotheses and theories, estimation of effects of major environmental drivers (climate change, habitat fragmentation, invasive species, air pollution), assessment of management and conservation strategies, and comparison of effects across different temporal and spatial scales, taxa and ecosystems, as well as research gap identification. We identified 322 meta-analyses published in the field of plant ecology between 1996 and 2013 in 95 different journals and assessed their methodological and reporting quality according to standard criteria. Despite significant recent developments in the methodology of meta-analysis, the quality of published meta-analyses was uneven and showed little improvement over time. We found many cases of imprecise and inaccurate usage of the term ‘meta-analysis’ in plant ecology, particularly confusion between meta-analysis and vote counting and incorrect application of statistical techniques designed for primary studies to meta-analytical data, without recognition of the violation of statistical assumptions of the analyses. Methodological issues for meta-analyses in plant ecology include incomplete reporting of search strategy used to retrieve primary studies, failure to test for possible publication bias and to conduct sensitivity analysis to test the robustness of the results, as well as lack of availability of the data set used for the analyses. The use of meta-analysis is particularly common in community ecology, ecophysiology and ecosystem ecology, but meta-analyses in ecophysiology are more likely not to meet standard quality criteria than papers in other subdisciplines. Fewer meta-analyses have been conducted in plant population ecology. Synthesis. Over the past two decades, plant ecologists have embraced meta-analysis as a statistical tool to combine results across studies, and much has been learned as a result. However, as the popularity and usage of meta-analysis in the field of plant ecology has grown, establishment of quality standards, as has been done in other disciplines, becomes increasingly important. In order to improve the quality of future meta-analyses in plant ecology, we suggest adoption of a checklist of quality criteria for meta-analysis for use by research synthesists, peer reviewers and journal editors.},
   ISSN = {0022-0477},
   DOI = {https://doi.org/10.1111/1365-2745.12224},
   url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2745.12224},
   year = {2014},
   type = {Journal Article}
}

@article{kou-giesbrecht2021,
   author = {Kou-Giesbrecht, Sian and Menge, Duncan N. L.},
   title = {Nitrogen-fixing trees increase soil nitrous oxide emissions: a meta-analysis},
   journal = {Ecology},
   volume = {102},
   number = {8},
   pages = {e03415},
   abstract = {Abstract Nitrogen-fixing trees are an important nitrogen source to terrestrial ecosystems. While they can fuel primary production and drive carbon dioxide sequestration, they can also potentially stimulate soil emissions of nitrous oxide, a potent greenhouse gas. However, studies on the influence of nitrogen-fixing trees on soil nitrous oxide emissions have not been quantitatively synthesized. Here, we show in a meta-analysis that nitrogen-fixing trees more than double soil nitrous oxide emissions relative to non-fixing trees and soils. If planted in reforestation projects at the global scale, nitrogen-fixing trees could increase global soil nitrous oxide emissions from natural terrestrial ecosystems by up to 4.1%, offsetting climate change mitigation via reforestation by up to 4.4%.},
   ISSN = {0012-9658},
   DOI = {https://doi.org/10.1002/ecy.3415},
   url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3415},
   year = {2021},
   type = {Journal Article}
}


@article{kuznetsova2017,
   author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
   title = {lmerTest Package: tests in linear mixed effects models},
   journal = {Journal of Statistical Software},
   volume = {82},
   number = {13},
   pages = {1-26},
   keywords = {denominator degree of freedom, Satterthwaite&#039
s approximation, ANOVA, R, linear mixed effects models, lme4},
   ISSN = {1548-7660},
   DOI = {10.18637/jss.v082.i13},
   url = {https://doi.org/10.18637/jss.v082.i13},
   year = {2017},
   type = {Journal Article}
}

@article{leybourne2021,
   author = {Leybourne, Daniel J. and Preedy, Katharine F. and Valentine, Tracy A. and Bos, Jorunn I. B. and Karley, Alison J.},
   title = {Drought has negative consequences on aphid fitness and plant vigor: Insights from a meta-analysis},
   journal = {Ecology and Evolution},
   volume = {11},
   number = {17},
   pages = {11915-11929},
   abstract = {Abstract Aphids are abundant in natural and managed vegetation, supporting a diverse community of organisms and causing damage to agricultural crops. Due to a changing climate, periods of drought are anticipated to increase, and the potential consequences of this for aphid–plant interactions are unclear. Using a meta-analysis and synthesis approach, we aimed to advance understanding of how increased drought incidence will affect this ecologically and economically important insect group and to characterize any potential underlying mechanisms. We used qualitative and quantitative synthesis techniques to determine whether drought stress has a negative, positive, or null effect on aphid fitness and examined these effects in relation to (a) aphid biology, (b) geographical region, and (c) host plant biology. Across all studies, aphid fitness is typically reduced under drought. Subgroup analysis detected no difference in relation to aphid biology, geographical region, or the aphid–plant combination, indicating the negative effect of drought on aphids is potentially universal. Furthermore, drought stress had a negative impact on plant vigor and increased plant concentrations of defensive chemicals, suggesting the observed response of aphids is associated with reduced plant vigor and increased chemical defense in drought-stressed plants. We propose a conceptual model to predict drought effects on aphid fitness in relation to plant vigor and defense to stimulate further research.},
   ISSN = {2045-7758},
   DOI = {https://doi.org/10.1002/ece3.7957},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.7957},
   year = {2021},
   type = {Journal Article}
}


@Proceedings{liu2020,
author = {Liu, Yang and Althoff, Tim and Heer, Jeffrey}, 
editor = {}, 
title = {Paths Explored, Paths Omitted, Paths Obscured: Decision Points & Selective Reporting in End-to-End Data Analysis}, 
booktitle = {CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, 
volume = {}, 
publisher = {Association for Computing Machinery}, 
address = {New York, NY, USA}, 
pages = {}, 
year = {2020}, 
abstract = {}, 
   DOI = {10.1145/3313831.3376533},
   url = {https://doi.org/10.1145/3313831.3376533},
keywords = {}
}


@article{lu2014,
   author = {Lu, Xun and White, Halbert},
   title = {Robustness checks and robustness tests in applied economics},
   journal = {Journal of Econometrics},
   volume = {178},
   pages = {194-206},
   abstract = {A common exercise in empirical studies is a “robustness check”, where the researcher examines how certain “core” regression coefficient estimates behave when the regression specification is modified by adding or removing regressors. If the coefficients are plausible and robust, this is commonly interpreted as evidence of structural validity. Here, we study when and how one can infer structural validity from coefficient robustness and plausibility. As we show, there are numerous pitfalls, as commonly implemented robustness checks give neither necessary nor sufficient evidence for structural validity. Indeed, if not conducted properly, robustness checks can be completely uninformative or entirely misleading. We discuss how critical and non-critical core variables can be properly specified and how non-core variables for the comparison regression can be chosen to ensure that robustness checks are indeed structurally informative. We provide a straightforward new Hausman (1978) type test of robustness for the critical core coefficients, additional diagnostics that can help explain why robustness test rejection occurs, and a new estimator, the Feasible Optimally combined GLS (FOGLeSs) estimator, that makes relatively efficient use of the robustness check regressions. A new procedure for Matlab, testrob, embodies these methods.},
   keywords = {Robustness
Causal effect
Conditional exogeneity
Specification test
Combined estimator},
   ISSN = {0304-4076},
   DOI = {https://doi.org/10.1016/j.jeconom.2013.08.016},
   url = {https://www.sciencedirect.com/science/article/pii/S0304407613001668},
   year = {2014},
   type = {Journal Article}
}

@article{ludecke2020,
	title = {Extracting, Computing and Exploring the Parameters of Statistical Models using R},
	author = {{Lüdecke}, Daniel and Ben-Shachar, Mattan and Patil, Indrajeet and Makowski, Dominique},
	year = {2020},
	date = {2020},
	journal = {Journal of Open Source Software},
	pages = {2445},
	volume = {5},
	number = {53},
	doi = {10.21105/joss.02445}
}

@article{luke2017,
   author = {Luke, S. G.},
   title = {Evaluating significance in linear mixed-effects models in R},
   journal = {Behavior Research Methods},
   volume = {49},
   number = {4},
   pages = {1494-1502},
   year = {2017},
   type = {Journal Article}
}

@inbook{miles2008,
   author = {Miles, C.},
   title = {Testing market-based instruments for conservation in northern Victoria},
   booktitle = {Biodiversity: Integrating Conservation and Production: Case Studies from Australian Farms, Forests and Fisheries},
   editor = {Norton, T.  and Lefroy, T. and Bailey, K. and Unwin, G.},
   publisher = {CSIRO Publishing},
   address = {Melbourne, Australia},
   pages = {133-146},
   year = {2008},
   type = {Book Section}
}


@article{morrissey2018,
	title = {Multiple regression is not multiple regressions: the meaning of multiple regression and the non-problem of collinearity},
	doi = {10.3998/ptpbio.16039257.0010.003},
	number = {3},
	journal = {Philosophy, Theory, and Practice in Biology},
	author = {Michael B. Morrissey and Graeme D. Ruxton},
	volume = {10},
	year = {2018},
	publisher = {Ann Arbor: Mi: Michigan Publishing, University of Michigan Library},
doi = {10.3998/ptpbio.16039257.0010.003},
}


@article{munoz2018,
   author = {Muñoz, John and Young, Cristobal},
   title = {We Ran 9 Billion Regressions: Eliminating False Positives through Computational Model Robustness},
   journal = {Sociological Methodology},
   volume = {48},
   number = {1},
   pages = {1-33},
   abstract = {False positive findings are a growing problem in many research literatures. We argue that excessive false positives often stem from model uncertainty. There are many plausible ways of specifying a regression model, but researchers typically report only a few preferred estimates. This raises the concern that such research reveals only a small fraction of the possible results and may easily lead to nonrobust, false positive conclusions. It is often unclear how much the results are driven by model specification and how much the results would change if a different plausible model were used. Computational model robustness analysis addresses this challenge by estimating all possible models from a theoretically informed model space. We use large-scale random noise simulations to show (1) the problem of excess false positive errors under model uncertainty and (2) that computational robustness analysis can identify and eliminate false positives caused by model uncertainty. We also draw on a series of empirical applications to further illustrate issues of model uncertainty and estimate instability. Computational robustness analysis offers a method for relaxing modeling assumptions and improving the transparency of applied research.},
   keywords = {model uncertainty,false positive,type I error,computational methods,robust,multimodel analysis},
   DOI = {10.1177/0081175018777988},
   url = {https://journals.sagepub.com/doi/abs/10.1177/0081175018777988},
   year = {2018},
   type = {Journal Article}
}

@article{nakagawa2007,
   author = {Nakagawa, S. and Cuthill, I. C.},
   title = {Effect size, confidence interval and statistical significance: a practical guide for biologists},
   journal = {Biological Reviews},
   volume = {82},
   number = {4},
   pages = {591-605},
   abstract = {Null hypothesis significance testing (NHST) is the dominant statistical approach in biology, although it has many, frequently unappreciated, problems. Most importantly, NHST does not provide us with two crucial pieces of information: (1) the magnitude of an effect of interest, and (2) the precision of the estimate of the magnitude of that effect. All biologists should be ultimately interested in biological importance, which may be assessed using the magnitude of an effect, but not its statistical significance. Therefore, we advocate presentation of measures of the magnitude of effects (i.e. effect size statistics) and their confidence intervals (CIs) in all biological journals. Combined use of an effect size and its CIs enables one to assess the relationships within data more effectively than the use of p values, regardless of statistical significance. In addition, routine presentation of effect sizes will encourage researchers to view their results in the context of previous research and facilitate the incorporation of results into future meta-analysis, which has been increasingly used as the standard method of quantitative review in biology. In this article, we extensively discuss two dimensionless (and thus standardised) classes of effect size statistics: d statistics (standardised mean difference) and r statistics (correlation coefficient), because these can be calculated from almost all study designs and also because their calculations are essential for meta-analysis. However, our focus on these standardised effect size statistics does not mean unstandardised effect size statistics (e.g. mean difference and regression coefficient) are less important. We provide potential solutions for four main technical problems researchers may encounter when calculating effect size and CIs: (1) when covariates exist, (2) when bias in estimating effect size is possible, (3) when data have non-normal error structure and/or variances, and (4) when data are non-independent. Although interpretations of effect sizes are often difficult, we provide some pointers to help researchers. This paper serves both as a beginner’s instruction manual and a stimulus for changing statistical practice for the better in the biological sciences.},
   keywords = {Bonferroni correction
confidence interval
effect size
effect statistic
meta-analysis
null hypothesis significance testing
p value
power analysis
statistical significance},
   ISSN = {1469-185X},
   DOI = {10.1111/j.1469-185X.2007.00027.x},
   url = {http://dx.doi.org/10.1111/j.1469-185X.2007.00027.x},
   year = {2007},
   type = {Journal Article}
}

@article{nakagawa2017,
	title = {Meta-evaluation of meta-analysis: ten appraisal questions for biologists},
	author = {Nakagawa, S. and Noble, D. W. and Senior, A. M. and Lagisz, M.},
	year = {2017},
	month = {03},
	date = {2017-03},
	journal = {BMC Biology},
	pages = {18},
	volume = {15},
	number = {1},
	doi = {10.1186/s12915-017-0357-7}
	}

@article{nakagawa2022,
   author = {Nakagawa, S. and Lagisz, Malgorzata and Jennions, Michael D. and Koricheva, Julia and Noble, Daniel W. A. and Parker, Timothy H. and Sánchez-Tójar, Alfredo and Yang, Yefeng and O'Dea, Rose E.},
   title = {Methods for testing publication bias in ecological and evolutionary meta-analyses},
   journal = {Methods in Ecology and Evolution},
   volume = {13},
   number = {1},
   pages = {4-21},
   abstract = {Abstract Publication bias threatens the validity of quantitative evidence from meta-analyses as it results in some findings being overrepresented in meta-analytic datasets because they are published more frequently or sooner (e.g. ‘positive’ results). Unfortunately, methods to test for the presence of publication bias, or assess its impact on meta-analytic results, are unsuitable for datasets with high heterogeneity and non-independence, as is common in ecology and evolutionary biology. We first review both classic and emerging publication bias tests (e.g. funnel plots, Egger's regression, cumulative meta-analysis, fail-safe N, trim-and-fill tests, p-curve and selection models), showing that some tests cannot handle heterogeneity, and, more importantly, none of the methods can deal with non-independence. For each method, we estimate current usage in ecology and evolutionary biology, based on a representative sample of 102 meta-analyses published in the last 10 years. Then, we propose a new method using multilevel meta-regression, which can model both heterogeneity and non-independence, by extending existing regression-based methods (i.e. Egger's regression). We describe how our multilevel meta-regression can test not only publication bias, but also time-lag bias, and how it can be supplemented by residual funnel plots. Overall, we provide ecologists and evolutionary biologists with practical recommendations on which methods are appropriate to employ given independent and non-independent effect sizes. No method is ideal, and more simulation studies are required to understand how Type 1 and Type 2 error rates are impacted by complex data structures. Still, the limitations of these methods do not justify ignoring publication bias in ecological and evolutionary meta-analyses.},
   ISSN = {2041-210X},
   DOI = {https://doi.org/10.1111/2041-210X.13724},
   url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13724},
   year = {2022},
   type = {Journal Article}
}

@article{nakagawa2023, 
 title={Quantitative evidence synthesis: a practical guide on meta-analysis, meta-regression, and publication bias tests for environmental sciences}, volume={12}, ISSN={2047-2382}, url={https://doi.org/10.1186/s13750-023-00301-6}, 
 DOI={10.1186/s13750-023-00301-6}, 
 number={1}, 
 journal={Environmental Evidence}, 
 author={Nakagawa, S. and Yang, Yefeng and Macartney, Erin L. and Spake, Rebecca and Lagisz, Malgorzata}, 
 year={2023}, 
 month=apr, 
 pages={8} 
}

@article{nicolaus2009,
   author = {Nicolaus, M. and Michler, S. P. M. and Ubels, R. and van der Velde, M. and Komdeur, J. and Both, C. and Tinbergen, J. M.},
   title = {Sex-specific effects of altered competition on nestling growth and survival: an experimental manipulation of brood size and sex ratio},
   journal = {Journal of Animal Ecology},
   volume = {78},
   number = {2},
   pages = {414-426},
   keywords = {intraspecific competition
optimal clutch size
Parus major
sex
allocation
sexual size dimorphism
tit parus-major
sparrowhawk accipiter-nisus
starling sturnus-vulgaris
sexually dimorphic birds
cell-mediated-immunity
individual great tits
red-winged blackbirds
clutch-size
body-size
environmental sensitivity
Environmental Sciences & Ecology
Zoology},
   ISSN = {0021-8790},
   DOI = {10.1111/j.1365-2656.2008.01505.x},
   url = {<Go to ISI>://WOS:000263038800014},
   year = {2009},
   type = {Journal Article}
}

@article{noble2017,
   author = {Noble, Daniel W. A. and Lagisz, Malgorzata and O'Dea, Rose E. and Nakagawa, Shinichi},
   title = {Nonindependence and sensitivity analyses in ecological and evolutionary meta-analyses},
   journal = {Molecular Ecology},
   volume = {26},
   number = {9},
   pages = {2410-2425},
   keywords = {hierarchical structure
meta-analysis
meta-regression
mixed models
multilevel models
quantitative research synthesis
random effects},
   ISSN = {1365-294X},
   DOI = {10.1111/mec.14031},
   url = {http://dx.doi.org/10.1111/mec.14031},
   year = {2017},
   type = {Journal Article}
}

@article{ohara2010,
	title = {Do not log{-}transform count data},
	author = {{O{\textquoteright}Hara}, Robert B. and Kotze, D. Johan},
	year = {2010},
	month = {05},
	date = {2010-05-04},
	journal = {Methods in Ecology and Evolution},
	pages = {118--122},
	volume = {1},
	number = {2},
	doi = {10.1111/j.2041-210x.2010.00021.x},
	url = {http://dx.doi.org/10.1111/j.2041-210X.2010.00021.x},
	langid = {en}
}

@article{open2015,
   author = {{Open Science Collaboration}},
   title = {Estimating the reproducibility of psychological science},
   journal = {Science},
   volume = {349},
   number = {6251},
   pages = {aac4716},
   abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P &lt; .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unor ginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
   DOI = {10.1126/science.aac4716},
   url = {http://science.sciencemag.org/content/sci/349/6251/aac4716.full.pdf},
   year = {2015},
   type = {Journal Article}
}

@article{page2017,
   author = {Page, Matthew J. and Moher, David},
   title = {Evaluations of the uptake and impact of the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) Statement and extensions: a scoping review},
   journal = {Systematic Reviews},
   volume = {6},
   number = {1},
   pages = {263},
   abstract = {The PRISMA Statement is a reporting guideline designed to improve transparency of systematic reviews (SRs) and meta-analyses. Seven extensions to the PRISMA Statement have been published to address the reporting of different types or aspects of SRs, and another eight are in development. We performed a scoping review to map the research that has been conducted to evaluate the uptake and impact of the PRISMA Statement and extensions. We also synthesised studies evaluating how well SRs published after the PRISMA Statement was disseminated adhere to its recommendations.},
   ISSN = {2046-4053},
   DOI = {10.1186/s13643-017-0663-8},
   url = {https://doi.org/10.1186/s13643-017-0663-8},
   year = {2017},
   type = {Journal Article}
}

@article{parker2016,
   author = {Parker, Timothy H. and Forstmeier, Wolfgang and Koricheva, Julia and Fidler, Fiona and Hadfield, Jarrod D. and Chee, Yung En and Kelly, Clint D. and Gurevitch, Jessica and Nakagawa, Shinichi},
   title = {Transparency in ecology and evolution: real problems, real solutions},
   journal = {Trends in Ecology & Evolution},
   volume = {31},
   number = {9},
   pages = {711-719},
   abstract = {To make progress scientists need to know what other researchers have found and how they found it. However, transparency is often insufficient across much of ecology and evolution. Researchers often fail to report results and methods in detail sufficient to permit interpretation and meta-analysis, and many results go entirely unreported. Further, these unreported results are often a biased subset. Thus the conclusions we can draw from the published literature are themselves often biased and sometimes might be entirely incorrect. Fortunately there is a movement across empirical disciplines, and now within ecology and evolution, to shape editorial policies to better promote transparency. This can be done by either requiring more disclosure by scientists or by developing incentives to encourage disclosure.},
   ISSN = {0169-5347},
   DOI = {10.1016/j.tree.2016.07.002},
   url = {http://dx.doi.org/10.1016/j.tree.2016.07.002},
   year = {2016},
   type = {Journal Article}
}


@article{parker2023,
   author = {Parker, Timothy H. and Yang, Yefeng},
   title = {Exaggerated effects in ecology},
   journal = {Nature Ecology & Evolution},
   abstract = {A study of over 18,000 effect sizes from more than 350 published studies in ecology finds clear evidence of selective reporting and exaggeration of effect sizes.},
   ISSN = {2397-334X},
   DOI = {10.1038/s41559-023-02156-z},
   url = {https://doi.org/10.1038/s41559-023-02156-z},
   year = {2023},
   type = {Journal Article}
}



@article{pei2020,
   author = {Pei, Yifan and Forstmeier, Wolfgang and Wang, Daiping and Martin, Katrin and Rutkowska, Joanna and Kempenaers, Bart},
   title = {Proximate Causes of Infertility and Embryo Mortality in Captive Zebra Finches},
   journal = {The American Naturalist},
   volume = {196},
   number = {5},
   pages = {577-596},
   note = {(裴一凡)
(王代平)},
   abstract = {AbstractSome species show high rates of reproductive failure, which is puzzling because natural selection works against such failure in every generation. Hatching failure is common in both captive and wild zebra finches (Taeniopygia guttata), yet little is known about its proximate causes. Here we analyze data on reproductive performance (the fate of >23,000 eggs) based on up to 14 years of breeding of four captive zebra finch populations. We find that virtually all aspects of reproductive performance are negatively affected by inbreeding (mean r=−0.117); by an early-starting, age-related decline (mean r=−0.132); and by poor early-life nutrition (mean r=−0.058). However, these effects together explain only about 3% of the variance in infertility, offspring mortality, fecundity, and fitness. In contrast, individual repeatability of different fitness components varied between 15% and 50%. As expected, we found relatively low heritability in fitness components (median: 7% of phenotypic variation and 29% of individually repeatable variation). Yet some of the heritable variation in fitness appears to be maintained by antagonistic pleiotropy (negative genetic correlations) between male fitness traits and female and offspring fitness traits. The large amount of unexplained variation suggests a potentially important role of local dominance and epistasis, including the possibility of segregating genetic incompatibilities.},
   keywords = {inbreeding,senescence,early nutrition,reproductive failure,quantitative genetics,sexual antagonism},
   DOI = {10.1086/710956},
   url = {https://www.journals.uchicago.edu/doi/abs/10.1086/710956},
   year = {2020},
   type = {Journal Article}
}

@inbook{rosenberg2013,
   author = {Rosenberg, M. S.},
   title = {Moment and least-squares based approaches to metaanalytic inference. },
   booktitle = {Handbook of meta-analysis in ecology and evolution},
   editor = {Koricheva, J. and Gurevitch, J. and Mengersen, K.},
   publisher = {Princeton University Press},
   address = {Princeton, USA},
   pages = {108-124},
   year = {2013},
   type = {Book Section}
}

@article{royle1999,
   author = {Royle, N. J. and Hartley, I. R. and Owens, I. P. F. and Parker, G. A.},
   title = {Sibling competition and the evolution of growth rates in birds},
   journal = {Proceedings of the Royal Society B-Biological Sciences},
   volume = {266},
   number = {1422},
   pages = {923-932},
   keywords = {sibling competition
growth rates
parental investment
brood reduction
kin selection
multiple paternity
extra-pair paternity
intraspecific brood parasitism
parent-offspring
conflict
red-winged blackbirds
genetic-evidence
low-frequency
reproductive success
tree swallows
microsatellite markers
phenotypic
plasticity
Life Sciences & Biomedicine - Other Topics
Environmental Sciences &
Ecology
Evolutionary Biology},
   ISSN = {0962-8452},
   DOI = {10.1098/rspb.1999.0725},
   url = {<Go to ISI>://WOS:000080290100009},
   year = {1999},
   type = {Journal Article}
}


@article{schweinsberg2021,
   author = {Schweinsberg, M. and Feldman, M. and Staub, N. and van den Akker, O. R. and van Aert, R. C. M. and van Assen, Malm and Liu, Y. and Althoff, T. and Heer, J. and Kale, A. and Mohamed, Z. and Amireh, H. and Prasad, V. V. and Bernstein, A. and Robinson, E. and Snellman, K. and Sommer, S. A. and Otner, S. M. G. and Robinson, D. and Madan, N. and Silberzahn, R. and Goldstein, P. and Tierney, W. and Murase, T. and Mandl, B. and Viganola, D. and Strobl, C. and Schaumans, C. B. C. and Kelchtermans, S. and Naseeb, C. and Garrison, S. M. and Yarkoni, T. and Chan, C. S. R. and Adie, P. and Alaburda, P. and Albers, C. and Alspaugh, S. and Alstott, J. and Nelson, A. A. and de la Rubia, E. A. and Arzi, A. and Bahnik, S. and Baik, J. and Balling, L. W. and Banker, S. and Baranger, D. A. and Barr, D. J. and Barros-Rivera, B. and Bauer, M. and Blaise, E. and Boelen, L. and Carbonell, K. B. and Briers, R. A. and Burkhard, O. and Canela, M. A. and Castrillo, L. and Catlett, T. and Chen, O. L. and Clark, M. and Cohn, B. and Coppock, A. and Cuguero-Escofet, N. and Curran, P. G. and Cyrus-Lai, W. and Dai, D. and Dalla Riva, G. V. and Danielsson, H. and Russo, Rdsm and de Silva, N. and Derungs, C. and Dondelinger, F. and de Souza, C. D. and Dube, B. T. and Dubova, M. and Dunn, B. and Edelsbrunner, P. A. and Finley, S. and Fox, N. and Gnambs, T. and Gong, Y. Y. and Grand, E. and Greenawalt, B. and Han, D. and Hanel, P. H. P. and Hong, A. B. and Hood, D. and Hsueh, J. and Huang, L. L. and Hui, K. N. and Hultman, K. A. and Javaid, A. and Jiang, L. J. and Jong, J. and Kamdar, J. and Kane, D. and Kappler, G. and Kaszubowski, E. and Kavanagh, C. M. and Khabsa, M. and Kleinberg, B. and others },
   title = {Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis},
   journal = {Organizational Behavior and Human Decision Processes},
   volume = {165},
   pages = {228-249},
   note = {Schweinsberg, Martin Feldman, Michael Staub, Nicola van den Akker, Olmo R. van Aert, Robbie C. M. van Assen, Marcel A. L. M. Liu, Yang Althoff, Tim Heer, Jeffrey Kale, Alex Mohamed, Zainab Amireh, Hashem Prasad, Vaishali Venkatesh Bernstein, Abraham Robinson, Emily Snellman, Kaisa Sommer, S. Amy Otner, Sarah M. G. Robinson, David Madan, Nikhil Silberzahn, Raphael Goldstein, Pavel Tierney, Warren Murase, Toshio Mandl, Benjamin Viganola, Domenico Strobl, Carolin Schaumans, Catherine B. C. Kelchtermans, Stijn Naseeb, Chan Garrison, S. Mason Yarkoni, Tal Chan, C. S. Richard Adie, Prestone Alaburda, Paulius Albers, Casper Alspaugh, Sara Alstott, Jeff Nelson, Andrew A. de la Rubia, Eduardo Arinno Arzi, Adbi Bahnik, Stepan Baik, Jason Balling, Laura Winther Banker, Sachin Baranger, David A. A. Barr, Dale J. Barros-Rivera, Brenda Bauer, Matt Blaise, Enuh Boelen, Lisa Carbonell, Katerina Bohle Briers, Robert A. Burkhard, Oliver Canela, Miguel-Angel Castrillo, Laura Catlett, Timothy Chen, Olivia Clark, Michael Cohn, Brent Coppock, Alex Cuguero-Escofet, Natalia Curran, Paul G. Cyrus-Lai, Wilson Dai, David Dalla Riva, Giulio Valentino Danielsson, Henrik Russo, Rosaria de F. S. M. de Silva, Niko Derungs, Curdin Dondelinger, Frank de Souza, Carolina Duarte Dube, B. Tyson Dubova, Marina Dunn, Ben Mark Edelsbrunner, Peter Adriaan Finley, Sara Fox, Nick Gnambs, Timo Gong, Yuanyuan Grand, Erin Greenawalt, Brandon Han, Dan Hanel, Paul H. P. Hong, Antony B. Hood, David Hsueh, Justin Huang, Lilian Hui, Kent N. Hultman, Keith A. Javaid, Azka Jiang, Lily Ji Jong, Jonathan Kamdar, Jash Kane, David Kappler, Gregor Kaszubowski, Erikson Kavanagh, Christopher M. Khabsa, Madian Kleinberg, Bennett Kouros, Jens Krause, Heather Krypotos, Angelos-Miltiadis Lavbic, Dejan Lee, Rui Ling Leffel, Timothy Lim, Wei Yang Liverani, Silvia Loh, Bianca Lonsmann, Dorte Low, Jia Wei Lu, Alton MacDonald, Kyle Madan, Christopher R. Madsen, Lasse Hjorth Maimone, Christina Mangold, Alexandra Marshall, Adrienne Matskewich, Helena Ester Mavon, Kimia McLain, Katherine L. McNamara, Amelia A. McNeill, Mhairi Mertens, Ulf Miller, David Moore, Ben Moore, Andrew Nantz, Eric Nasrullah, Ziauddin Nejkovic, Valentina Nell, Colleen S. Nelson, Andrew Arthur Nilsonne, Gustav Nolan, Rory O'Brien, Christopher E. O'Neil, Patrick O'Shea, Kieran Olita, Toto Otterbacher, Jahna Palsetia, Diana Pereira, Bianca Pozdniakov, Ivan Protzko, John Reyt, Jean-Nicolas Riddle, Travis Ali, Amal (Akmal) Ridhwan Omar Ropovik, Ivan Rosenberg, Joshua M. Rothen, Stephane Schulte-Mecklenbeck, Michael Sharma, Nirek Shotwell, Gordon Skarzynski, Martin Stedden, William Stodden, Victoria Stoffel, Martin A. Stoltzman, Scott Subbaiah, Subashini Tatman, Rachael Thibodeau, Paul H. Tomkins, Sabina Valdivia, Ana van de Woestijne, Gerrieke B. Viana, Laura Villeseche, Florence Wadsworth, W. Duncan Wanders, Florian Watts, Krista Wells, Jason D. Whelpley, Christopher E. Won, Andy Wu, Lawrence Yip, Arthur Youngflesh, Casey Yu, Ju-Chi Zandian, Arash Zhang, Leilei Zibman, Chava Uhlmann, Eric Luis
Ropovik, Ivan/J-7404-2015; Hanel, Paul/AAB-3287-2020; Madan, Christopher R./C-6214-2008; S, Martin/AAH-9592-2021; Adbi, Arzi/U-8529-2019; Thibodeau, Paul/AAY-7259-2021; Otner, Sarah/A-9647-2012; Olita, Toto/F-2190-2015; Lavbič, Dejan/G-1405-2010; Blaise, Enuh/AAD-9668-2021; Gnambs, Timo/I-8353-2014; Briers, Robert/B-5810-2014; Valdivia, Ana/M-8266-2015
Ropovik, Ivan/0000-0001-5222-1233; Hanel, Paul/0000-0002-3225-1395; Madan, Christopher R./0000-0003-3228-6501; S, Martin/0000-0003-3529-9463; Adbi, Arzi/0000-0003-2099-2484; Otner, Sarah/0000-0002-2743-8462; Olita, Toto/0000-0002-3247-3756; Lavbič, Dejan/0000-0003-2390-4160; Blaise, Enuh/0000-0002-2081-6029; Gnambs, Timo/0000-0002-6984-1276; van den Akker, Olmo/0000-0002-0712-3746; McNamara, Amelia/0000-0003-4916-2433; Finley, Sara/0000-0002-7090-8108; Briers, Robert/0000-0003-0341-1203; Yip, Arthur Hong Chun/0000-0001-5610-6507; Albers, Casper/0000-0002-9213-6743; Valdivia, Ana/0000-0001-8214-8380
1095-9920},
   abstract = {In this crowdsourced initiative, independent analysts used the same dataset to test two hypotheses regarding the effects of scientists' gender and professional status on verbosity during group meetings. Not only the analytic approach but also the operationalizations of key variables were left unconstrained and up to individual analysts. For instance, analysts could choose to operationalize status as job title, institutional ranking, citation counts, or some combination. To maximize transparency regarding the process by which analytic choices are made, the analysts used a platform we developed called DataExplained to justify both preferred and rejected analytic paths in real time. Analyses lacking sufficient detail, reproducible code, or with statistical errors were excluded, resulting in 29 analyses in the final sample. Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. A Boba multiverse analysis demonstrates that decisions about how to operationalize variables explain variability in outcomes above and beyond statistical choices (e.g., covariates). Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. Implications for orga-nizations and leaders, whose decision making relies in part on scientific findings, consulting reports, and internal analyses by data scientists, are discussed.},
   ISSN = {0749-5978},
   DOI = {10.1016/j.obhdp.2021.02.003},
   url = {<Go to ISI>://WOS:000674429500016},
   year = {2021},
   type = {Journal Article}
}


@article{senior2016,
   author = {Senior, Alistair M. and Grueber, Catherine E. and Kamiya, Tsukushi and Lagisz, Malgorzata and O'Dwyer, Katie and Santos, Eduardo S. A. and Nakagawa, Shinichi},
   title = {Heterogeneity in ecological and evolutionary meta-analyses: its magnitude and implications},
   journal = {Ecology},
   volume = {97},
   number = {12},
   pages = {3293-3299},
   abstract = {Abstract Meta-analysis is the gold standard for synthesis in ecology and evolution. Together with estimating overall effect magnitudes, meta-analyses estimate differences between effect sizes via heterogeneity statistics. It is widely hypothesized that heterogeneity will be present in ecological/evolutionary meta-analyses due to the system-specific nature of biological phenomena. Despite driving recommended best practices, the generality of heterogeneity in ecological data has never been systematically reviewed. We reviewed 700 studies, finding 325 that used formal meta-analysis, of which total heterogeneity was reported in fewer than 40%. We used second-order meta-analysis to collate heterogeneity statistics from 86 studies. Our analysis revealed that the median and mean heterogeneity, expressed as I2, are 84.67% and 91.69%, respectively. These estimates are well above “high” heterogeneity (i.e., 75%), based on widely adopted benchmarks. We encourage reporting heterogeneity in the forms of I2 and the estimated variance components (e.g., τ2) as standard practice. These statistics provide vital insights in to the degree to which effect sizes vary, and provide the statistical support for the exploration of predictors of effect-size magnitude. Along with standard meta-regression techniques that fit moderator variables, multi-level models now allow partitioning of heterogeneity among correlated (e.g., phylogenetic) structures that exist within data.},
   ISSN = {0012-9658},
   DOI = {10.1002/ecy.1591},
   url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.1591},
   year = {2016},
   type = {Journal Article}
}

@book{shavit2017,
   author = {Shavit, A. and Ellison, Aaron M.},
   title = {Stepping in the same river twice: replication in biological research},
   publisher = {Yale University Press},
   address = {New Haven, Connecticut, USA},
   year = {2017},
   type = {Edited Book}
}

@article{siegel2022,
   author = {Siegel, Kyle R. and Kaur, Muskanjot and Grigal, A. Calvin and Metzler, Rebecca A. and Dickinson, Gary H.},
   title = {Meta-analysis suggests negative, but pCO2-specific, effects of ocean acidification on the structural and functional properties of crustacean biomaterials},
   journal = {Ecology and Evolution},
   volume = {12},
   number = {6},
   pages = {e8922},
   abstract = {Abstract Crustaceans comprise an ecologically and morphologically diverse taxonomic group. They are typically considered resilient to many environmental perturbations found in marine and coastal environments, due to effective physiological regulation of ions and hemolymph pH, and a robust exoskeleton. Ocean acidification can affect the ability of marine calcifying organisms to build and maintain mineralized tissue and poses a threat for all marine calcifying taxa. Currently, there is no consensus on how ocean acidification will alter the ecologically relevant exoskeletal properties of crustaceans. Here, we present a systematic review and meta-analysis on the effects of ocean acidification on the crustacean exoskeleton, assessing both exoskeletal ion content (calcium and magnesium) and functional properties (biomechanical resistance and cuticle thickness). Our results suggest that the effect of ocean acidification on crustacean exoskeletal properties varies based upon seawater pCO2 and species identity, with significant levels of heterogeneity for all analyses. Calcium and magnesium content was significantly lower in animals held at pCO2 levels of 1500–1999 µatm as compared with those under ambient pCO2. At lower pCO2 levels, however, statistically significant relationships between changes in calcium and magnesium content within the same experiment were observed as follows: a negative relationship between calcium and magnesium content at pCO2 of 500–999 µatm and a positive relationship at 1000–1499 µatm. Exoskeleton biomechanics, such as resistance to deformation (microhardness) and shell strength, also significantly decreased under pCO2 regimes of 500–999 µatm and 1500–1999 µatm, indicating functional exoskeletal change coincident with decreases in calcification. Overall, these results suggest that the crustacean exoskeleton can be susceptible to ocean acidification at the biomechanical level, potentially predicated by changes in ion content, when exposed to high influxes of CO2. Future studies need to accommodate the high variability of crustacean responses to ocean acidification, and ecologically relevant ranges of pCO2 conditions, when designing experiments with conservation-level endpoints.},
   ISSN = {2045-7758},
   DOI = {https://doi.org/10.1002/ece3.8922},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.8922},
   year = {2022},
   type = {Journal Article}
}


@article{silberzahn2018,
   author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
   title = {Many analysts, one data set: making transparent how variations in analytic choices affect results},
   journal = {Advances in Methods and Practices in Psychological Science},
   volume = {1},
   number = {3},
   pages = {337-356},
   abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
   keywords = {crowdsourcing science,data analysis,scientific transparency,open data,open materials},
   DOI = {10.1177/2515245917747646},
   url = {http://dx.doi.org/10.1177/2515245917747646},
   year = {2018},
   type = {Journal Article}
}

@article{simmons2011,
title={False-Positive Psychology},
volume={22},
DOI={10.1177/0956797611417632},
abstractNote={In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
number={11},
journal={Psychological Science},
author={Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
year={2011},
month={Oct},
pages={1359–1366} 
}


@article{simons2017,
   author = {Simons, Daniel J. and Shoda, Yuichi and Lindsay, D. Stephen},
   title = {Constraints on Generality (COG): A proposed addition to all empirical papers},
   journal = {Perspectives on Psychological Science},
   abstract = {Psychological scientists draw inferences about populations based on samples—of people, situations, and stimuli—from
those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested
samples are representative of broader populations. A cumulative science depends on accurately characterizing the
generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving
readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research
articles specify Constraints on Generality (i.e., a “COG” statement) that identify and justify target populations for the
reported findings. Explicitly defining the target populations will help other researchers to sample from the same
populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary
conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor
a more cumulative science.},
   DOI = {10.1177/174569161770863},
   year = {2017},
   type = {Journal Article}
}



@article{simonsohn2015,
   author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
   title = {Specification curve: descriptive and inferential statistics on all reasonable specifications},
   DOI = {10.2139/ssrn.2694998 },
   year = {2015},
date = {2015},
	journal = {SSRN Electronic Journal},
   type = {Manuscript},
url = {http://dx.doi.org/10.2139/ssrn.2694998},
	langid = {en}
}

@article{simonsohn2020,
	title = {Specification curve analysis},
	author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
	year = {2020},
	month = {07},
	date = {2020-07-27},
	journal = {Nature Human Behaviour},
	pages = {1208--1214},
	volume = {4},
	number = {11},
	doi = {10.1038/s41562-020-0912-z},
	url = {http://dx.doi.org/10.1038/s41562-020-0912-z},
	langid = {en}
}

@article{stanton-geddes2014,
   author = {Stanton-Geddes, John and de Freitas, Cintia Gomes and de Sales Dambros, Cristian},
   title = {In defense of P values: comment on the statistical methods actually used by ecologists},
   journal = {Ecology},
   volume = {95},
   number = {3},
   pages = {637-642},
   ISSN = {0012-9658},
   DOI = {https://doi.org/10.1890/13-1156.1},
   url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/13-1156.1},
   year = {2014},
   type = {Journal Article}
}


@article{steegen2016,
   author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
   title = {Increasing transparency through a multiverse analysis},
   journal = {Perspectives on Psychological Science},
   volume = {11},
   number = {5},
   pages = {702-712},
   abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
   keywords = {multiverse analysis,arbitrary choices,data processing,good research practices,transparency,selective reporting},
   DOI = {10.1177/1745691616658637},
   url = {https://journals.sagepub.com/doi/abs/10.1177/1745691616658637},
   year = {2016},
   type = {Journal Article}
}


@Article{stefan2022,
author = {Stefan, Angelika and Schönbrodt, Felix}, 
title = {Big little lies: A compendium and simulation of p-hacking strategies}, 
journal = {Royal Society Open Science}, 
volume = {10}, 
number = {2}, 
pages = {220346}, 
year = {2023}, 
abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of …}, 
location = {},
DOI={10.1098/rsos.220346}, 
   url = {https://doi.org/10.1098/rsos.220346},
keywords = {}
}


@article{taylor2023,
   author = {Taylor, James W. and Taylor, Kathryn S.},
   title = {Combining probabilistic forecasts of COVID-19 mortality in the United States},
   journal = {European Journal of Operational Research},
   volume = {304},
   number = {1},
   pages = {25-41},
   abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality, cases and hospitalisations help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID-19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we combine the forecasts to extract the wisdom of the crowd. We use a dataset that has been made publicly available from the COVID-19 Forecast Hub. A notable feature of the dataset is that the availability of forecasts from participating teams varies greatly across the 40 weeks in our study. We evaluate the accuracy of combining methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods. In addition, we propose several new weighted combining methods. Our results show that, although the median was very useful for the early weeks of the pandemic, the simple average was preferable thereafter, and that, as a history of forecast accuracy accumulates, the best results can be produced by a weighted combining method that uses weights that are inversely proportional to the historical accuracy of the individual forecasting teams.},
   keywords = {OR in health services
COVID-19
Forecast combining
Distributional forecasts
Interval forecasts},
   ISSN = {0377-2217},
   DOI = {https://doi.org/10.1016/j.ejor.2021.06.044},
   url = {https://www.sciencedirect.com/science/article/pii/S0377221721005609},
   year = {2023},
   type = {Journal Article}
}

@article{tedersoo2021,
	title = {Data sharing practices and data availability upon request differ across scientific disciplines},
	author = {Tedersoo, Leho and {Küngas}, Rainer and Oras, Ester and {Köster}, Kajar and Eenmaa, Helen and Leijen, {Äli} and Pedaste, Margus and Raju, Marju and Astapova, Anastasiya and Lukner, Heli and Kogermann, Karin and Sepp, Tuul},
	year = {2021},
	month = {07},
	date = {2021-07-27},
	journal = {Scientific Data},
	volume = {8},
	number = {1},
	doi = {10.1038/s41597-021-00981-0},
	url = {http://dx.doi.org/10.1038/s41597-021-00981-0},
	langid = {en}
}

@Manual{timetk,
    title = {timetk: A Tool Kit for Working with Time Series},
    author = {Matt Dancho and Davis Vaughan},
    year = {2023},
    note = {R package version 2.8.3},
    url = {https://CRAN.R-project.org/package=timetk},
  }

@article{touchon2016,
   author = {Touchon, Justin C. and McCoy, Michael W.},
   title = {The mismatch between current statistical practice and doctoral training in ecology},
   journal = {Ecosphere},
   volume = {7},
   number = {8},
   pages = {e01394},
   abstract = {Abstract Ecologists are studying increasingly complex and important issues such as climate change and ecosystem services. These topics often involve large data sets and the application of complicated quantitative models. We evaluated changes in statistics used by ecologists by searching nearly 20,000 published articles in ecology from 1990 to 2013. We found that there has been a rise in sophisticated and computationally intensive statistical techniques such as mixed effects models and Bayesian statistics and a decline in reliance on approaches such as ANOVA or t tests. Similarly, ecologists have shifted away from software such as SAS and SPSS to the open source program R. We also searched the published curricula and syllabi of 154 doctoral programs in the United States and found that despite obvious changes in the statistical practices of ecologists, more than one-third of doctoral programs showed no record of required or optional statistics classes. Approximately one-quarter of programs did require a statistics course, but most of those did not cover contemporary statistical philosophy or advanced techniques. Only one-third of doctoral programs surveyed even listed an optional course that teaches some aspect of contemporary statistics. We call for graduate programs to lead the charge in improving training of future ecologists with skills needed to address and understand the ecological challenges facing humanity.},
   ISSN = {2150-8925},
   DOI = {https://doi.org/10.1002/ecs2.1394},
   url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecs2.1394},
   year = {2016},
   type = {Journal Article}
}

@article{vanderwerf1992,
   author = {Vander Werf, Eric},
   title = {Lack's clutch size hypothesis: an examination of the evidence using meta-analysis},
   journal = {Ecology},
   volume = {73},
   number = {5},
   pages = {1699-1705},
   keywords = {brood enlargement
clutch size
factors affecting results of conflicting
studies
food limitation
lack hypothesis
metaanalysis
quantitative
review
flycatcher ficedula-hypoleuca
tit parus-major
brood-size
breeding
success
experimental manipulation
nestling survival
parental
survival
house sparrows
birds
reproduction
Environmental Sciences & Ecology},
   ISSN = {0012-9658},
   DOI = {10.2307/1940021},
   url = {<Go to ISI>://WOS:A1992JP74000018},
   year = {1992},
   type = {Journal Article}
}

@article{verhoef2012,
   author = {Ver Hoef, Jay M.},
   title = {Who invented the delta method?},
   journal = {The American Statistician},
   volume = {66},
   number = {2},
   pages = {124-127},
   abstract = {Many statisticians and other scientists use what is commonly called the ?delta method.? However, few people know who proposed it. The earliest article was found in an obscure journal, and the author is rarely cited for his contribution. This article briefly reviews three modern versions of the delta method and how they are used. Then, some history on the author and the journal of the first known article on the delta method is given. The original author?s specific contribution is reproduced, along with a discussion on possible reasons that it has been overlooked.},
   ISSN = {0003-1305},
   DOI = {10.1080/00031305.2012.687494},
   url = {https://doi.org/10.1080/00031305.2012.687494},
   year = {2012},
   type = {Journal Article}
}

@article{verhulst2006,
   author = {Verhulst, S. and Holveck, M. J. and Riebel, K.},
   title = {Long-term effects of manipulated natal brood size on metabolic rate in zebra finches},
   journal = {Biology Letters},
   volume = {2},
   number = {3},
   pages = {478-480},
   keywords = {metabolic programming
metabolic syndrome
brood size manipulation
developmental stress
Taeniopygia guttata
taeniopygia-guttata
sex-ratio
reproduction
nestlings
growth
health
costs
fetal
Life Sciences & Biomedicine - Other Topics
Environmental Sciences &
Ecology
Evolutionary Biology},
   ISSN = {1744-9561},
   DOI = {10.1098/rsbl.2006.0496},
   url = {<Go to ISI>://WOS:000241863400043},
   year = {2006},
   type = {Journal Article}
}

@article{vesk2016,
   author = {Vesk, P. A. and Morris, W. K. and McCallum, W. and Apted, R. and Miles, C. },
   title = {Processes of woodland eucalypt regeneration: lessons from the bush returns trial},
   journal = {Proceedings of the Royal Society of Victoria},
   volume = {128},
   pages = {54-63},
   year = {2016},
   type = {Journal Article}
}

@article{voracek2019,
	title = {Which Data to Meta-Analyze, and How?},
	author = {Voracek, Martin and Kossmeier, Michael and Tran, Ulrich S.},
	year = {2019},
	month = {01},
	date = {2019-01},
	journal = {Zeitschrift für Psychologie},
	pages = {64--82},
	volume = {227},
	number = {1},
	doi = {10.1027/2151-2604/a000357},
	url = {http://dx.doi.org/10.1027/2151-2604/a000357},
	langid = {en}
}


@Article{wicherts2016,
author = {Wicherts, JM and Veldkamp, C. L. S. and Augusteijn, H. E. M. and Bakker, M. and van Aert, R. C. M., and van Assen, M. A. L. M.}, 
title = {Degrees of freedom in planning, running, analyzing, and reporting psychological studies: a checklist to avoid p-hacking.}, 
journal = {Frontiers in Psychology}, 
volume = {7}, 
number = {}, 
pages = {1832}, 
year = {2016}, 
abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.}, 
location = {Methodology and Statistics, Tilburg University Tilburg, Netherlands.}, 
keywords = {questionable research practice; experimental design; significance testing; p-hacking; bias; significance chasing; research methods education}
}

@Article{Wicherts2016b,
author = {}, 
title = {}, 
journal = {}, 
volume = {}, 
number = {}, 
pages = {}, 
year = {}, 
abstract = {}, 
location = {}, 
keywords = {}
}


@article{yang2023,
   author = {Yang, Yefeng and Sánchez-Tójar, Alfredo and O’Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
   title = {Publication bias impacts on effect size, statistical power, and magnitude (Type M) and sign (Type S) errors in ecology and evolutionary biology},
   journal = {BMC Biology},
   volume = {21},
   number = {1},
   pages = {71},
   abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the ‘replication crisis’. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23% to 15% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5% to 8% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
   ISSN = {1741-7007},
   DOI = {10.1186/s12915-022-01485-y},
   url = {https://doi.org/10.1186/s12915-022-01485-y},
   year = {2023},
   type = {Journal Article}
}



@Article{young2021,
author = {Young, Cristobal and Stewart, Sheridan A}, 
title = {Multiverse analysis: advancements in functional form robustness}, 
journal = {Cornell University, Ithaca, NY Unpublished working paper}, 
volume = {}, 
number = {}, 
pages = {}, 
year = {2021}, 
abstract = {Social scientists face a dual problem of model uncertainty and methodological abundance. There are many different ways to conduct an analysis, but the true model is unknown. This uncertainty among abundance offers spiraling opportunities to discover a significant result. Multiverse analysis addresses this by recognizing `many worlds' of modeling assumptions, using computational tools to show a large set of plausible estimates. We consider cases where plausible methods include OLS, logit, Poisson, inverse probability weighting, and …}, 
location = {}, 
keywords = {}
}

 

